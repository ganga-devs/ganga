#
# command line submission interface for Athena
#

import optparse
import sys
import os
import re
from Ganga.Core.exceptions import GangaException

usage = """
------------------
Analysis examples:
------------------

Job to LSF

ganga athena --inDS trig1_misal1_csc11.005009.J0_pythia_jetjet.recon.AOD.v12000604 --outputdata AnalysisSkeleton.aan.root --lsf --maxevt 100 AnalysisSkeleton_jobOptions.py

Job to LCG

ganga athena --inDS trig1_misal1_csc11.005009.J0_pythia_jetjet.recon.AOD.v12000604 --outputdata AnalysisSkeleton.aan.root --split 2 --lcg --site FZK AnalysisSkeleton_jobOptions.py

Job to Nordugrid

ganga athena --inDS trig1_misal1_csc11.005009.J0_pythia_jetjet.recon.AOD.v12000604 --outputdata AnalysisSkeleton.aan.root --split 2 --ng AnalysisSkeleton_jobOptions.py

Job to Panda

ganga athena --inDS trig1_misal1_csc11.005009.J0_pythia_jetjet.recon.AOD.v12000604 --outDS user.FirstLast.result.AANT.v12000604 --split 2 --panda AnalysisSkeleton_jobOptions.py




"""

p = optparse.OptionParser(usage=usage)

p.add_option('--verbose', '-v', action='store_true')

# Job object
p.add_option('--name', '-n', action='store', type='string', dest='name', help='Job name')

# Input dataset
p.add_option('--inDS', '-i', action='store', type='string', dest='input_dataset', metavar='DATASETNAME', help='Name of the DQ2 datasetname to process')
p.add_option('--inTagDS', action='store', type='string', dest='tag_input_dataset', metavar='TAGDATASETNAME', help='Name of the Tag DQ2 datasetname to process')
p.add_option('--inputtype', action='store', type='choice', dest='inputtype', choices = ['DQ2', 'ATLAS', 'Local', 'Castor'],  default='DQ2', help='Type of the input dataset: DQ2, ATLAS, Local, Castor, [default: DQ2]' )
p.add_option('--inputaccessmode', action='store', type='choice', dest='inputaccessmode', choices = ['DQ2_LOCAL', 'LFC', 'TAG', 'TAG_REC', 'FILE_STAGER' ], default='DQ2_LOCAL',  help='Access mode of the DQ2/TAG input dataset: DQ2_LOCAL, FILE_STAGER, LFC, TAG, TAG_REC [default: DQ2_LOCAL]'  )
p.add_option('--inputfailover', action='store_true', help='Use DQ2_COPY automatically if DQ2_LOCAL fails' )
p.add_option('--inputaccessprotocol', action='store', type='string', dest='inputaccessprotocol', help='Access protocol on worker node, like Xrootd'  )
p.add_option('--locallocation', action='store', type='string', dest='locallocation', nargs=2, help='Directory and file pattern for ATLASLocalDataset: e.g. /path/to/dir/ *AOD*.root')
p.add_option('--inputlfn', action='store', type='string', dest='inputlfn', help='ATLASDataset lfn parameter, e.g. --inputlfn=rome.004100.recov10.T1_McAtNLO_top._[00001-00010].AOD.pool.root or --inputlfn=lfn:file1.root,lfn:file2.root' )
p.add_option('--inputlfc', action='store', type='string', dest='inputlfc', help='ATLASDataset lfc host parameter, e.g. --inputlfc=lfc-fzk.gridka.de' )
p.add_option('--match_ce_all', action='store_const', const='match_ce_all', dest='match_ce_all', help='Job are sent to incomplete and complete inputdataset sources' )
p.add_option('--inputnames', action='store', type='string', dest='inputnames', help='Logical files names to process with DQ2Dataset seperated by : , e.g. --inputnames=file1.root:file2.root' )
p.add_option('--excludeinputnames', action='store', type='string', dest='excludeinputnames', help='Logical files names to exclude from processing with DQ2Dataset seperated by : , e.g. --excludeinputnames=file1.root:file2.root' )
p.add_option('--inputnumfiles', action='store', type='int', dest='inputnumfiles', help='Number of files to proccess in a DQ2Dataset, e.g. --inputnumfiles=5' )
p.add_option('--input_minnumfiles', action='store', type='int', dest='input_minnumfiles', help='Minimum number of files that should exist in a incomplete dataset source location of a DQ2Dataset, e.g. --input_minnumfiles=1' )
p.add_option('--use_aodesd_backnav', action='store_const', const='use_aodesd_backnav', dest='use_aodesd_backnav', help='Use AOD to ESD Backnavigation' )
# Event picking
p.add_option('--pickevent', action='store_true', help='Use Event picking as inputdata' )
p.add_option('--pick_data_type', action='store', type='string', dest='pick_data_type', help='Type of data for event picking. One of AOD, ESD, RAW.' )
p.add_option('--pick_stream_name', action='store', type='string', dest='pick_stream_name', help='Stream name for event picking, e.g. physics_L1Calo' )
p.add_option('--pick_dataset_pattern', action='store', type='string', dest='pick_dataset_pattern', help='Dataset pattern which matches the selection ' )
p.add_option('--pick_event_list', action='store', type='string', dest='pick_event_list', help='A filename which contains list of runs/events for event picking.' )
p.add_option('--pick_filter_policy', action='store', type='string', dest='pick_filter_policy', help='accept/reject the pick event.' )

# Output dataset
p.add_option('--outputdata', '-o', action='store', type='string', dest='outputdata', help='List of job output roottuples to be stored, separated by ":", e.g. AnalysisSekeleton.aan.root:Numbers.txt')
p.add_option('--outputtype', action='store', type='choice', dest='outputtype', choices = ['DQ2', 'ATLAS'], default='DQ2', help='Type of the output dataset: DQ2, ATLAS, [default: DQ2]')
p.add_option('--outDS', action='store', type='string', dest='output_dataset', metavar='DATASETNAME', help='Name of the DQ2 datasetname for the job output, will be of the type: user.username.ganga.DATSETNAME')
p.add_option('--outputlocation', action='store', type='string', dest='output_location', help='Location and path of the job output on the remote storage element for grid jobs or local path for local or batch jobs, e.g. sfn://castorgrid.cern.ch/castor/cern.ch/atlas/scratch/$USER/ganga or /path/to/storefiles/')
p.add_option('--outputlocallocation', action='store', type='string', dest='output_local_location', help='Local path to store output that is transfered with retrieve() method')
#p.add_option('--use_datasetname', action='store_true', help='Use datasetname as it is and do not prepend users.myname.ganga' )
p.add_option('--use_shortfilename', action='store_true', help='Use shorter version of filenames and do not prepend users.myname.ganga' )


# Job splitting
p.add_option('--split', '-s', action='store', type='int', dest='numsubjobs', help='Number of subjobs, if a job should be splitted. The splitting is done via the list of inputfile')
p.add_option('--splitfiles', action='store', type='int', dest='numfiles_subjob', help='Number of files per subjob, if a job should be splitted.')
p.add_option('--splitfilesize', action='store', type='int', dest='filesize', help='Maximum filesize sum per subjob im MB, if a job should be splitted.')
p.add_option('--match_subjobs_files', action='store_const', const='match_subjobs_files', dest='match_subjobs_files', help='Match the number of subjobs to the number of inputfiles"')
p.add_option('--noblacklist', action='store_true', dest='noblacklist', help='Do not use black list of sites create by GangaRobot functional tests')

# Backends
p.add_option('--lcg', action='store_const', const='lcg', dest='backend', help='Submit job(s) to LCG Grid')
p.add_option('--cream', action='store_const', const='cream', dest='backend', help='Submit job(s) to a CREAM CE')
p.add_option('--ng', action='store_const', const='ng', dest='backend', help='Submit job(s) to NorduGrid')
p.add_option('--panda', action='store_const', const='panda', dest='backend', help='Submit job(s) to Panda')
p.add_option('--lsf', action='store_const', const='lsf', dest='backend', help='Submit job(s) to the local LSF batch system, e.g. on lxplus')
p.add_option('--pbs', action='store_const', const='pbs', dest='backend', help='Submit job(s) to the local PBS batch system')
p.add_option('--sge', action='store_const', const='sge', dest='backend', help='Submit job(s) to the local SGE batch system')
p.add_option('--local', action='store_const', const='local', dest='backend', help='Execute job on the local desktop computer')
p.add_option('--queue', action='store', type='string', dest='queue', help='Specify queue for LSF or PBS submission')
p.add_option('--ce' , action='store', type='string', dest='ce', help='Specific computing element and queue for LCG or NG submission')
p.add_option('--rejectce' , action='store', type='string', dest='rejectce', help='Specific computing element and queue to exclude from NG submission')
p.add_option('--site' , action='store', type='string', dest='site', help='Specific computing element and queue for LCG submission following the DQ2 naming schema, separated by ":", e.g. --site FZK:LRZ')
p.add_option('--cloud' , action='store', type='string', dest='cloud', help='Cloudname for LCG or Panda submission, e.g. --cloud DE or --cloud US')
p.add_option('--glite', action='store_true', help='Use gLite resource broker')
p.add_option('--edg', action='store_true', help='Use the EDG resource broker')
p.add_option('--walltime' , action='store', type='int', dest='walltime', help='Specify walltime requirement')
p.add_option('--cputime' , action='store', type='int', dest='cputime', help='Specify cputime requirement')
p.add_option('--memory' , action='store', type='int', dest='memory', help='Specify memory requirement')
p.add_option('--long', action='store_true', help='Select only long queues')
p.add_option('--batchextraopts' , action='store', type='string', dest='batchextraopts', help='extra options for batchsystems')
p.add_option('--check_availability', action='store_true', help='Check for availability of input files on NG' )
p.add_option('--runtimeenvironment', action='store', type='string', dest='runtimeenvironment', help='Specify the required runtime environment on an NG site' )
p.add_option('--bexec', action='store', type='string', dest='bexec', help='Specify the build command for --athena_exe EXE on Panda')
p.add_option('--libDS', action='store', type='string', dest='libDS', help='Existing Library dataset to use (disables buildjob)')
p.add_option('--nobuild', action='store_true', help='Panda build job is switched off.' )
p.add_option('--extOutFile', action='store', type='string', dest='extOutFile', help='define extra output files on Panda, e.g. --extOutFile=output1.txt:output2.dat')
p.add_option('--excludedSite', action='store', type='string', dest='excludedSite', default='', help="list of Panda sites which are not used for site section, e.g., ANALY_ABC,ANALY_XYZ")
p.add_option('--useChirpServer', action='store', type='string', dest='useChirpServer', default='', help="Chirp Server to be used to stage out the output e.g., voatlas92.cern.ch")

# Application options
p.add_option('-c', action='store', type='string', dest='options', help='One-liner, runs before any job Options')
p.add_option('--maxevt' , action='store', type='int', dest='maxevt', help='Maximal number of events to process')
p.add_option('--compile', action='store_true', help='Determine if job should be recompiled on Grid worker node' )
p.add_option('--nocompile', action='store_true', help='Determine if job should *not* be recompiled on Grid worker node' )
p.add_option('--excludefiles', action='store', type='string', dest='excludefiles', help='List of files to be excluded from user_area tar file, separated by ":", e.g. --excludefiles=*.root:Numbers.txt')
p.add_option('--athena_release', action='store', type='string', dest='athena_release', help='Athena release version to be used' )
p.add_option('--athena_production', action='store', type='string', dest='athena_production', help='AtlasProduction transformation cache release version to be used' )
p.add_option('--athena_project', action='store', type='string', dest='athena_project', help='AtlasProject name of release to be used, e.g. AtlasProduction' )
p.add_option('--athena_exe', action='store', type='choice', dest='athena_exe', choices = ['ATHENA', 'PYARA', 'ROOT','ARES','EXE' ], default='ATHENA', help='Executable type: ATHENA, PYARA, ROOT [default: ATHENA]')
p.add_option('--athena_dbrelease', action='store', type='string', dest='athena_dbrelease', help='ATLAS DBRelease DQ2 dataset and DQ2Release tar file (DatasetName:FileName). e.g., ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-5.1.1.tar.gz')
p.add_option('--user_area', action='store', type='string', dest='user_area', help='Tar file of user athena code area produced e.g. by a previous Ganga job' )
p.add_option('--group_area', action='store', type='string', dest='group_area', help='Tar file of GroupArea produced e.g. by a previous Ganga job' )
p.add_option('--user_setupfile', action='store', type='string', dest='user_setupfile', help='User setup script for special setup on the grid worker node' )
p.add_option('--excludepackage', action='store', type='string', dest='excludepackage', help='Pattern of files to exclude from user area requirements files, separated by : , e.g. --excludepackage=MyPackage:YourPackage' )
p.add_option('--prepare_old', action='store_true', help='Use prepare_old instead of new prepare method' )
p.add_option('--collectstats', action='store_true', help='Switch to collect application statistics info and store in stats field' )
p.add_option('--atlas_supp_stream', action='store', type='string', dest='atlas_supp_stream', help='suppress some output streams, e.g. --atlas_supp_stream=GLOBAL:D3PD' )
p.add_option('--atlas_run_dir', action='store', type='string', dest='atlas_run_dir', help='Athena run directory, e.g. --atlas_run_dir=./' )
p.add_option('--individualOutDS', action='store_true', dest='individualOutDS', help='Create seperate output datasets/containers for each output type (e.g. AANT, HIST).' )
p.add_option('--express', action='store_true', dest='express', help='Panda option to send the job using express quota to have higher priority. The number of express subjobs in the queue and the total execution time used by express subjobs are limited (a few subjobs and several hours per day, respectively). This option is intended to be used for quick tests before bulk submission. Note that buildXYZ is not included in quota calculation. If this option is used when quota has already exceeded, the panda server will ignore the option so that subjobs have normal priorities. Also, if you submit 1 buildXYZ and N runXYZ subjobs when you only have quota of M (M < N),  only the first M runXYZ subjobs will have higher priorities' )

# Inputsandbox
p.add_option('--inputsandbox', action='store', type='string', dest='inputsandbox', help='List of files to be transfered to the grid worker node with the input sandbox, separated by ":", e.g. --inputsandbox=file1.txt:file2.txt')

# Outputsandbox
p.add_option('--outputsandbox', action='store', type='string', dest='outputsandbox', help='List of files to be retrieved from the grid worker node with the output sandbox, separated by ":", e.g. --outputsandbox=file1.txt:file2.txt')


# Job query
p.add_option('--query', '-q', action='store', type='int', dest='jobid', help='Query job status, e.q. ganga athena -q jobid, Output: job id, CE, job status, job output' )



opt, args = p.parse_args()


# Job query
if opt.jobid:
    print '-----------'
    print 'Job Status:'
    print '-----------'
    if jobs[opt.jobid].subjobs:
        for subjob in jobs[opt.jobid].subjobs:
                print jobs[opt.jobid].id, subjob.id, subjob.backend.actualCE, subjob.status,
                if subjob.status=='completed':
                    try:
                        print subjob.outputdata.datasetname
                    except AttributeError:
                        print subjob.outputdata.output
    else:
        print jobs[opt.jobid].id, jobs[opt.jobid].backend.actualCE, jobs[opt.jobid].status, 
        if jobs[opt.jobid].status=='completed':
            try:
                print jobs[opt.jobid].outputdata.datasetname
            except AttributeError:
                print jobs[opt.jobid].outputdata.output
        
    print '\n-----------'
    sys.exit(0)        

# Parse additional arguments

if args:
    option_files=[]
    if args:  
        for option_file in args:
            if not opt.athena_exe in ['EXE']:	
                if not os.access(option_file,os.R_OK):
                    print >>sys.stderr, 'ERROR: Cannot read athena job option file: %s' % option_file
                    sys.exit(4)
            	else:
		    option_files.append(option_file)
            else:
	        option_files.append(option_file)
else:
    print >>sys.stderr, 'ERROR: No athena job option file given'
    sys.exit(4)


# Start job configuration

j = Job()

# job object

if opt.name:
    j.name = opt.name

# application

j.application = Athena()
if opt.excludefiles:
    j.application.exclude_from_user_area=opt.excludefiles.split(':')
if opt.excludepackage:
    j.application.exclude_package=opt.excludepackage.split(':')
if opt.athena_release:
    match=re.search(r'^([\w+]+.[\w+]+.[\w+]+)',opt.athena_release)
    if match:
        j.application.atlas_release=match.group(1)
        match2=re.search(r'^([\w+]+.[\w+]+.[\w+]+.[\w+]+)',opt.athena_release)	
	if match2:
            j.application.atlas_production=opt.athena_release
    else:
        j.application.atlas_release=opt.athena_release
if opt.athena_production:
    j.application.atlas_production=opt.athena_production
if opt.athena_project:
    j.application.atlas_project=opt.athena_project
if opt.athena_exe:
    j.application.atlas_exetype=opt.athena_exe
if opt.athena_dbrelease:
    j.application.atlas_dbrelease=opt.athena_dbrelease
if opt.atlas_run_dir:
    j.application.atlas_run_dir=opt.atlas_run_dir


if opt.user_area:
    j.application.user_area=opt.user_area
if opt.group_area:
    j.application.group_area=opt.group_area

j.application.option_file = option_files
if opt.options:
    aoptions = opt.options.replace('"',"'''")        
    aoptions = opt.options.replace('\'',"'''")        
    j.application.options = "-c '''%s''' " %aoptions
if opt.user_setupfile:
    j.application.user_setupfile = opt.user_setupfile

# max_events
if opt.maxevt:
    j.application.max_events = int(opt.maxevt)

if opt.atlas_supp_stream:
    j.application.atlas_supp_stream=opt.atlas_supp_stream.split(':')

if opt.compile:
    j.application.athena_compile=True
    if opt.prepare_old:	
        j.application.prepare_old(athena_compile=True)
    else:
        j.application.prepare()
elif opt.nocompile:
    j.application.athena_compile=False
    if opt.prepare_old:	
        j.application.prepare_old(athena_compile=False)
    else:
        j.application.prepare()
else:
    if not (opt.athena_release or opt.user_area) or opt.bexec:
        if opt.prepare_old:	
            j.application.prepare_old()
        else:
	    j.application.prepare()
if opt.collectstats:
    j.application.collect_stats=True

# inputdata

if opt.input_dataset:
    #print opt.inputtype
    if opt.inputtype:
        if opt.inputtype=='DQ2':
            j.inputdata = DQ2Dataset()
        if opt.inputtype=='ATLAS':
            j.inputdata = ATLASDataset()
            if opt.inputlfn:
                 j.inputdata.lfn=opt.inputlfn.split(',')
            else:
                print >>sys.stderr, 'ERROR: No parameter --inputlfn for ATLASDataset given'
                sys.exit(4)
            if opt.inputlfc:
                 j.inputdata.lfc=opt.inputlfc

        if opt.inputtype=='Local':
            j.inputdata = ATLASLocalDataset()
            print opt.locallocation
            if opt.locallocation:
                j.inputdata.get_dataset(opt.locallocation[0],opt.locallocation[1])
            else:
                print >>sys.stderr, 'ERROR: No parameter --locallocation for AthenaLocalDataset given'
                sys.exit(4)
        if opt.inputtype=='Castor':
            j.inputdata = ATLASCastorDataset()
    else:        
        j.inputdata = DQ2Dataset()
        opt.inputtype = 'DQ2'

    if opt.inputtype == 'DQ2':
        j.inputdata.dataset = opt.input_dataset.split(',')
        if opt.inputaccessmode:
            j.inputdata.type = opt.inputaccessmode        
        else:    
            j.inputdata.type = 'DQ2_LOCAL'

	if opt.inputfailover:
	    j.inputdata.failover=True
        if opt.inputaccessprotocol:
            j.inputdata.accessprotocol = opt.inputaccessprotocol        
        if opt.backend == 'lcg' and not opt.ce and opt.match_ce_all=='match_ce_all':
            j.inputdata.match_ce_all = True
        if opt.input_minnumfiles>0:
            j.inputdata.min_num_files = opt.input_minnumfiles
        if opt.inputnames:
            j.inputdata.names = opt.inputnames.split(':')
        if opt.excludeinputnames:
            j.inputdata.exclude_names = opt.inputnames.split(':')
        if opt.inputnumfiles:
            j.inputdata.number_of_files = opt.inputnumfiles
        if opt.tag_input_dataset:
            j.inputdata.tagdataset = opt.tag_input_dataset
        if opt.backend == 'lcg' and opt.use_aodesd_backnav:
            j.inputdata.use_aodesd_backnav = True

if opt.pickevent:
    j.inputdata=EventPicking()
    if opt.pick_data_type:
        j.inputdata.pick_data_type=opt.pick_data_type
    if opt.pick_stream_name:
        j.inputdata.pick_stream_name=opt.pick_stream_name
    if opt.pick_dataset_pattern:
        j.inputdata.pick_dataset_pattern=opt.pick_dataset_pattern
    if opt.pick_event_list:
        j.inputdata.pick_event_list=opt.pick_event_list
    if opt.pick_filter_policy:
        j.inputdata.pick_filter_policy=opt.pick_filter_policy

    if not (opt.numsubjobs>0 and opt.numfiles_subjob>0 and opt.filesize>0):
        if (opt.backend in [ 'lcg', 'cream', 'panda', 'ng' ] or opt.glite):
            j.splitter = DQ2JobSplitter()
	    j.splitter.numsubjobs = 1

# outputdata

if opt.outputdata or opt.backend == 'panda':
    if opt.outputtype:         
        if opt.outputtype == 'ATLAS':
            j.outputdata = ATLASOutputDataset()
        elif opt.outputtype == 'DQ2':
            j.outputdata = DQ2OutputDataset()
            if opt.output_dataset:
                j.outputdata.datasetname = opt.output_dataset
    else:        
        j.outputdata = DQ2OutputDataset()
        if opt.output_dataset:
             j.outputdata.datasetname = opt.output_dataset

    if opt.outputdata:
        if opt.outputdata.find(':')>0:
            j.outputdata.outputdata = opt.outputdata.split(':')
        elif opt.outputdata.find(',')>0:
            j.outputdata.outputdata = opt.outputdata.split(',')
        else:
            j.outputdata.outputdata = opt.outputdata.split('\n')
    if opt.output_location:
        j.outputdata.location = opt.output_location
    if opt.output_local_location:
        j.outputdata.local_location = opt.output_local_location
#    if opt.use_datasetname:
#        j.outputdata.use_datasetname = True
    if opt.use_shortfilename:
        j.outputdata.use_shortfilename = True

# splitter

if opt.numsubjobs > 0:
    if (opt.backend in [ 'lcg' ] or opt.glite or (opt.backend in ['sge'] and config.Athena.ENABLE_SGE_DQ2JOBSPLITTER)):
        j.splitter = DQ2JobSplitter()
    elif (opt.backend in [ 'cream', 'panda', 'ng' ]):
        j.splitter = DQ2JobSplitter()
    else:
        j.splitter = AthenaSplitterJob()
    j.splitter.numsubjobs = opt.numsubjobs

    if opt.outputdata:
        j.merger=AthenaOutputMerger()
elif opt.numfiles_subjob>0:
    if (opt.backend in [ 'lcg' ] or opt.glite or (opt.backend in ['sge'] and config.Athena.ENABLE_SGE_DQ2JOBSPLITTER)):
        j.splitter = DQ2JobSplitter()
        j.splitter.numfiles = opt.numfiles_subjob
    elif (opt.backend in [ 'cream', 'panda', 'ng' ]):
        j.splitter = DQ2JobSplitter()
        j.splitter.numfiles = opt.numfiles_subjob
    else:
        j.splitter = AthenaSplitterJob()
        j.splitter.numfiles_subjob = opt.numfiles_subjob

    if opt.outputdata:
        j.merger=AthenaOutputMerger()

elif opt.filesize>0:
    if (opt.backend in [ 'lcg' ] or opt.glite or (opt.backend in ['sge'] and config.Athena.ENABLE_SGE_DQ2JOBSPLITTER)):
        j.splitter = DQ2JobSplitter()
        j.splitter.filesize = opt.filesize
    elif (opt.backend in [ 'cream', 'panda', 'ng' ]):
        j.splitter = DQ2JobSplitter()
        j.splitter.filesize = opt.filesize

    if opt.outputdata:
        j.merger=AthenaOutputMerger()

if opt.match_subjobs_files:
    j.splitter = AthenaSplitterJob()
    j.splitter.match_subjobs_files = True

    if opt.outputdata:
        j.merger=AthenaOutputMerger()

if opt.noblacklist:
    if j.splitter and j.splitter.__class__.__name__=='DQ2JobSplitter':
        j.splitter.use_blacklist=False

# backend

if opt.backend == 'lsf':
    j.backend = LSF()
    if opt.queue: j.backend.queue = opt.queue
elif opt.backend == 'lcg' or opt.glite:
    j.backend = LCG()
    if opt.ce: j.backend.CE = opt.ce 
    if opt.site: 
        j.backend.requirements=AtlasLCGRequirements()
        j.backend.requirements.sites=opt.site.split(':') 
    if opt.cloud: 
        j.backend.requirements=AtlasLCGRequirements()
        j.backend.requirements.cloud=opt.cloud 
    if opt.glite: 
        j.backend.middleware = 'GLITE'
    elif opt.edg: 
        j.backend.middleware = 'EDG' 
elif opt.backend == 'cream':
    j.backend = CREAM()
    if opt.ce: 
        j.backend.CE = opt.ce 
    elif opt.site: 
        j.backend.requirements=AtlasCREAMRequirements()
        j.backend.requirements.sites=opt.site.split(':') 
    else:
        print >>sys.stderr, 'ERROR: CREAM backend requires "--ce" or "--site" to be configured'
        sys.exit(100)
        
elif opt.backend == 'pbs':
    j.backend = PBS()
    if opt.queue: j.backend.queue = opt.queue
elif opt.backend == 'sge':
    j.backend = SGE()
    if opt.queue: j.backend.queue = opt.queue
elif opt.backend=='ng':
    try:
        j.backend = NG()
    except NameError:
        print >>sys.stderr, 'ERROR: Nordugrid backend has not been enabled in the gangarc file'
        sys.exit(100)
    if opt.ce: j.backend.CE = opt.ce
    if opt.rejectce: j.backend.RejectCE = opt.rejectce
    if opt.walltime: j.backend.requirements.walltime = opt.walltime
    if opt.cputime: j.backend.requirements.cputime = opt.cputime
    if opt.memory: j.backend.requirements.memory = opt.memory
    if opt.check_availability: j.backend.check_availability = True
    if opt.runtimeenvironment: j.backend.requirements.runtimeenvironment = [opt.runtimeenvironment]

elif opt.backend=='panda':
    try:
        j.backend = Panda()
    except NameError:
        print >>sys.stderr, 'ERROR: Panda backend has not been enabled in the gangarc file'
        sys.exit(100)
    if opt.site: j.backend.site = opt.site
    if opt.cloud: 
        j.backend.requirements.cloud = opt.cloud
	j.backend.requirements.anyCloud = False
    if opt.long: j.backend.requirements.long = True 
    if opt.express: j.backend.requirements.express = True 
    if opt.cputime: j.backend.requirements.cputime = opt.cputime
    if opt.memory: j.backend.requirements.memory = opt.memory
    if opt.extOutFile: j.backend.extOutFile = opt.extOutFile.split(':')

    if opt.bexec:
        j.backend.bexec=opt.bexec

    if opt.nobuild:
        j.backend.nobuild=opt.nobuild

    if opt.libDS:
        j.backend.libDS=opt.libDS

    if opt.excludedSite: j.backend.requirements.excluded_sites = opt.excludedSite.split(',')

    if opt.useChirpServer: config.Panda.chirpserver = opt.useChirpServer

    if opt.individualOutDS: j.backend.individualOutDS = True
else:
    j.backend = Local()

# Extra queue parameters
if opt.backend in [ 'lcg', 'cream' ] or opt.glite:
    if opt.long: j.backend.requirements.cputime = 8 * 60;
    if opt.cputime: j.backend.requirements.cputime = opt.cputime
    if opt.walltime: j.backend.requirements.walltime = opt.walltime
    if opt.memory: j.backend.requirements.memory = opt.memory

if opt.backend in ['sge', 'pbs', 'lsf']:
   if opt.batchextraopts: j.backend.extraopts = opt.batchextraopts

# Inputsandbox
if opt.inputsandbox:
   if opt.inputsandbox.find(':')>0:
      j.inputsandbox = opt.inputsandbox.split(':')        
   elif opt.inputsandbox.find(',')>0:
      j.inputsandbox = opt.inputsandbox.split(',')        
   else:
      j.inputsandbox = opt.inputsandbox.split('\n')        

# Outputsandbox
if opt.outputsandbox:
   if opt.outputsandbox.find(':')>0:
      j.outputsandbox = opt.outputsandbox.split(':')        
   elif opt.outputsandbox.find(',')>0:
      j.outputsandbox = opt.outputsandbox.split(',')        
   else:
      j.outputsandbox = opt.outputsandbox.split('\n')        

# Remove default DBRelease='LATEST' if no splitter or AthenaSplitterJob is used
if not j.splitter:
    j.application.atlas_dbrelease=''
if j.splitter and j.splitter.__class__.__name__=='AthenaSplitterJob':
    j.application.atlas_dbrelease=''

#print j

try:
    j.submit()
except GangaException, e:
    print >>sys.stderr, 'ERROR: %s - %s' % (type(e),e)


