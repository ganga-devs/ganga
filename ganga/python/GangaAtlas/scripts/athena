#
# command line submission interface for Athena
#

import optparse
import sys
from Ganga.Core.exceptions import GangaException

usage = """
------------------
Analysis examples:
------------------

Job to LSF

ganga athena --inDS trig1_misal1_csc11.005009.J0_pythia_jetjet.recon.AOD.v12000604 --outputdata AnalysisSkeleton.aan.root --lsf --maxevt 100 AnalysisSkeleton_jobOptions.py

Job to LCG

ganga athena --inDS trig1_misal1_csc11.005009.J0_pythia_jetjet.recon.AOD.v12000604 --outputdata AnalysisSkeleton.aan.root --split 2 --lcg --site FZK AnalysisSkeleton_jobOptions.py

Job to Nordugrid

ganga athena --inDS trig1_misal1_csc11.005009.J0_pythia_jetjet.recon.AOD.v12000604 --outputdata AnalysisSkeleton.aan.root --split 2 --ng AnalysisSkeleton_jobOptions.py

Job to Panda

ganga athena --inDS trig1_misal1_csc11.005009.J0_pythia_jetjet.recon.AOD.v12000604 --outDS user.FirstLast.result.AANT.v12000604 --split 2 --panda AnalysisSkeleton_jobOptions.py




"""

p = optparse.OptionParser(usage=usage)

p.add_option('--verbose', '-v', action='store_true')

# Job object
p.add_option('--name', '-n', action='store', type='string', dest='name', help='Job name')

# Input dataset
p.add_option('--inDS', '-i', action='store', type='string', dest='input_dataset', metavar='DATASETNAME', help='Name of the DQ2 datasetname to process')
p.add_option('--inTagDS', action='store', type='string', dest='tag_input_dataset', metavar='TAGDATASETNAME', help='Name of the Tag DQ2 datasetname to process')
p.add_option('--inputtype', action='store', type='choice', dest='inputtype', choices = ['DQ2', 'ATLAS', 'Local', 'Castor'],  default='DQ2', help='Type of the input dataset: DQ2, ATLAS, Local, Castor, [default: DQ2]' )
p.add_option('--inputaccessmode', action='store', type='choice', dest='inputaccessmode', choices = ['DQ2_LOCAL', 'DQ2_DOWNLOAD', 'LFC', 'TAG', 'TAG_REC', 'DQ2_COPY' ], default='DQ2_LOCAL',  help='Access mode of the DQ2/TAG input dataset: DQ2_LOCAL, DQ2_DOWNLOAD, LFC, TAG, TAG_REC, DQ2_COPY [default: DQ2_LOCAL]'  )
p.add_option('--inputfailover', action='store_true', help='Use DQ2_COPY automatically if DQ2_LOCAL fails' )
p.add_option('--inputaccessprotocol', action='store', type='string', dest='inputaccessprotocol', help='Access protocol on worker node, like Xrootd'  )
p.add_option('--locallocation', action='store', type='string', dest='locallocation', nargs=2, help='Directory and file pattern for ATLASLocalDataset: e.g. /path/to/dir/ *AOD*.root')
p.add_option('--inputlfn', action='store', type='string', dest='inputlfn', help='ATLASDataset lfn parameter, e.g. --inputlfn=rome.004100.recov10.T1_McAtNLO_top._[00001-00010].AOD.pool.root or --inputlfn=lfn:file1.root,lfn:file2.root' )
p.add_option('--inputlfc', action='store', type='string', dest='inputlfc', help='ATLASDataset lfc host parameter, e.g. --inputlfc=lfc-fzk.gridka.de' )
p.add_option('--match_ce_all', action='store_const', const='match_ce_all', dest='match_ce_all', help='Job are sent to incomplete and complete inputdataset sources' )
p.add_option('--inputnames', action='store', type='string', dest='inputnames', help='Logical files names to process with DQ2Dataset seperated by : , e.g. --inputnames=file1.root:file2.root' )
p.add_option('--excludeinputnames', action='store', type='string', dest='excludeinputnames', help='Logical files names to exclude from processing with DQ2Dataset seperated by : , e.g. --excludeinputnames=file1.root:file2.root' )
p.add_option('--inputnumfiles', action='store', type='int', dest='inputnumfiles', help='Number of files to proccess in a DQ2Dataset, e.g. --inputnumfiles=5' )
p.add_option('--input_minnumfiles', action='store', type='int', dest='input_minnumfiles', help='Minimum number of files that should exist in a incomplete dataset source location of a DQ2Dataset, e.g. --input_minnumfiles=1' )
p.add_option('--use_aodesd_backnav', action='store_const', const='use_aodesd_backnav', dest='use_aodesd_backnav', help='Use AOD to ESD Backnavigation' )

# Output dataset
p.add_option('--outputdata', '-o', action='store', type='string', dest='outputdata', help='List of job output roottuples to be stored, separated by ":", e.g. AnalysisSekeleton.aan.root:Numbers.txt')
p.add_option('--outputtype', action='store', type='choice', dest='outputtype', choices = ['DQ2', 'ATLAS'], default='DQ2', help='Type of the output dataset: DQ2, ATLAS, [default: DQ2]')
p.add_option('--outDS', action='store', type='string', dest='output_dataset', metavar='DATASETNAME', help='Name of the DQ2 datasetname for the job output, will be of the type: user.username.ganga.DATSETNAME')
p.add_option('--outputlocation', action='store', type='string', dest='output_location', help='Location and path of the job output on the remote storage element for grid jobs or local path for local or batch jobs, e.g. sfn://castorgrid.cern.ch/castor/cern.ch/atlas/scratch/$USER/ganga or /path/to/storefiles/')
p.add_option('--outputlocallocation', action='store', type='string', dest='output_local_location', help='Local path to store output that is transfered with retrieve() method')
#p.add_option('--use_datasetname', action='store_true', help='Use datasetname as it is and do not prepend users.myname.ganga' )
p.add_option('--use_shortfilename', action='store_true', help='Use shorter version of filenames and do not prepend users.myname.ganga' )


# Job splitting
p.add_option('--split', '-s', action='store', type='int', dest='numsubjobs', help='Number of subjobs, if a job should be splitted. The splitting is done via the list of inputfile')
p.add_option('--splitfiles', action='store', type='int', dest='numfiles_subjob', help='Number of files per subjob, if a job should be splitted.')
p.add_option('--splitfilesize', action='store', type='int', dest='filesize', help='Maximum filesize sum per subjob im MB, if a job should be splitted.')
p.add_option('--match_subjobs_files', action='store_const', const='match_subjobs_files', dest='match_subjobs_files', help='Match the number of subjobs to the number of inputfiles"')

# Backends
p.add_option('--lcg', action='store_const', const='lcg', dest='backend', help='Submit job(s) to LCG Grid')
p.add_option('--ng', action='store_const', const='ng', dest='backend', help='Submit job(s) to NorduGrid')
p.add_option('--panda', action='store_const', const='panda', dest='backend', help='Submit job(s) to Panda')
p.add_option('--lsf', action='store_const', const='lsf', dest='backend', help='Submit job(s) to the local LSF batch system, e.g. on lxplus')
p.add_option('--pbs', action='store_const', const='pbs', dest='backend', help='Submit job(s) to the local PBS batch system')
p.add_option('--sge', action='store_const', const='sge', dest='backend', help='Submit job(s) to the local SGE batch system')
p.add_option('--local', action='store_const', const='local', dest='backend', help='Execute job on the local desktop computer')
p.add_option('--queue', action='store', type='string', dest='queue', help='Specify queue for LSF or PBS submission')
p.add_option('--ce' , action='store', type='string', dest='ce', help='Specific computing element and queue for LCG or NG submission')
p.add_option('--rejectce' , action='store', type='string', dest='rejectce', help='Specific computing element and queue to exclude from NG submission')
p.add_option('--site' , action='store', type='string', dest='site', help='Specific computing element and queue for LCG submission following the DQ2 naming schema, separated by ":", e.g. --site FZK:LRZ')
p.add_option('--cloud' , action='store', type='string', dest='cloud', help='Cloudname for LCG or Panda submission, e.g. --cloud DE or --cloud US')
p.add_option('--glite', action='store_true', help='Use gLite resource broker')
p.add_option('--edg', action='store_true', help='Use the EDG resource broker')
p.add_option('--walltime' , action='store', type='int', dest='walltime', help='Specify walltime requirement')
p.add_option('--cputime' , action='store', type='int', dest='cputime', help='Specify cputime requirement')
p.add_option('--memory' , action='store', type='int', dest='memory', help='Specify memory requirement')
p.add_option('--long', action='store_true', help='Select only long queues')
p.add_option('--check_availability', action='store_true', help='Check for availability of input files on NG' )
p.add_option('--runtimeenvironment', action='store', type='string', dest='runtimeenvironment', help='Specify the required runtime environment on an NG site' )

# Application options
p.add_option('-c', action='store', type='string', dest='options', help='One-liner, runs before any job Options')
p.add_option('--maxevt' , action='store', type='int', dest='maxevt', help='Maximal number of events to process')
p.add_option('--compile', action='store_true', help='Determine if job should be recompiled on Grid worker node' )
p.add_option('--nocompile', action='store_true', help='Determine if job should *not* be recompiled on Grid worker node' )
p.add_option('--excludefiles', action='store', type='string', dest='excludefiles', help='List of files to be excluded from user_area tar file, separated by ":", e.g. --excludefiles=*.root:Numbers.txt')
p.add_option('--athena_release', action='store', type='string', dest='athena_release', help='Athena release version to be used' )
p.add_option('--athena_production', action='store', type='string', dest='athena_production', help='AtlasProduction transformation cache release version to be used' )
p.add_option('--athena_exe', action='store', type='choice', dest='athena_exe', choices = ['ATHENA', 'PYARA', 'ROOT' ], default='ATHENA', help='Executable type: ATHENA, PYARA, ROOT [default: ATHENA]')
p.add_option('--user_area', action='store', type='string', dest='user_area', help='Tar file of user athena code area produced e.g. by a previous Ganga job' )
p.add_option('--group_area', action='store', type='string', dest='group_area', help='Tar file of GroupArea produced e.g. by a previous Ganga job' )
p.add_option('--user_setupfile', action='store', type='string', dest='user_setupfile', help='User setup script for special setup on the grid worker node' )
p.add_option('--excludepackage', action='store', type='string', dest='excludepackage', help='Pattern of files to exclude from user area requirements files, separated by : , e.g. --excludepackage=MyPackage:YourPackage' )

# Inputsandbox
p.add_option('--inputsandbox', action='store', type='string', dest='inputsandbox', help='List of files to be transfered to the grid worker node with the input sandbox, separated by ":", e.g. --inputsandbox=file1.txt:file2.txt')

# Outputsandbox
p.add_option('--outputsandbox', action='store', type='string', dest='outputsandbox', help='List of files to be retrieved from the grid worker node with the output sandbox, separated by ":", e.g. --outputsandbox=file1.txt:file2.txt')


# Job query
p.add_option('--query', '-q', action='store', type='int', dest='jobid', help='Query job status, e.q. ganga athena -q jobid, Output: job id, CE, job status, job output' )

opt, args = p.parse_args()


# Job query
if opt.jobid:
    print '-----------'
    print 'Job Status:'
    print '-----------'
    if jobs[opt.jobid].subjobs:
        for subjob in jobs[opt.jobid].subjobs:
                print jobs[opt.jobid].id, subjob.id, subjob.backend.actualCE, subjob.status,
                if subjob.status=='completed':
                    try:
                        print subjob.outputdata.datasetname
                    except AttributeError:
                        print subjob.outputdata.output
    else:
        print jobs[opt.jobid].id, jobs[opt.jobid].backend.actualCE, jobs[opt.jobid].status, 
        if jobs[opt.jobid].status=='completed':
            try:
                print jobs[opt.jobid].outputdata.datasetname
            except AttributeError:
                print jobs[opt.jobid].outputdata.output
        
    print '\n-----------'
    sys.exit(0)        

# Parse additional arguments

if args:
    option_files=[]
    if args:
        for option_file in args:
            if not os.access(option_file,os.R_OK):
                print >>sys.stderr, 'ERROR: Cannot read athena job option file: %s' % option_file
                sys.exit(4)
            else:
                option_files.append(option_file)
else:
    print >>sys.stderr, 'ERROR: No athena job option file given'
    sys.exit(4)


# Start job configuration

j = Job()

# job object

if opt.name:
   j.name = opt.name

# application

j.application = Athena()
if opt.excludefiles:
    j.application.exclude_from_user_area=opt.excludefiles.split(':')
if opt.excludepackage:
    j.application.exclude_package=opt.excludepackage.split(':')
if opt.athena_release:
    j.application.atlas_release=opt.athena_release
if opt.athena_production:
    j.application.atlas_production=opt.athena_production
if opt.athena_exe:
    j.application.atlas_exetype=opt.athena_exe

if opt.user_area:
    j.application.user_area=opt.user_area
if opt.group_area:
    j.application.group_area=opt.group_area

if opt.compile:
   j.application.prepare(athena_compile=True)
elif opt.nocompile:
   j.application.prepare(athena_compile=False)
else:
   if not (opt.athena_release or opt.user_area):
       j.application.prepare()

j.application.option_file = option_files
if opt.options:
   aoptions = opt.options.replace('"',"'''")        
   aoptions = opt.options.replace('\'',"'''")        
   j.application.options = "-c '''%s''' " %aoptions
if opt.user_setupfile:
    j.application.user_setupfile = opt.user_setupfile

# max_events
if opt.maxevt:
   j.application.max_events = int(opt.maxevt)

# inputdata

if opt.input_dataset:
    print opt.inputtype
    if opt.inputtype:
        if opt.inputtype=='DQ2':
            j.inputdata = DQ2Dataset()
        if opt.inputtype=='ATLAS':
            j.inputdata = ATLASDataset()
            if opt.inputlfn:
                 j.inputdata.lfn=opt.inputlfn.split(',')
            else:
                print >>sys.stderr, 'ERROR: No parameter --inputlfn for ATLASDataset given'
                sys.exit(4)
            if opt.inputlfc:
                 j.inputdata.lfc=opt.inputlfc

        if opt.inputtype=='Local':
            j.inputdata = ATLASLocalDataset()
            print opt.locallocation
            if opt.locallocation:
                j.inputdata.get_dataset(opt.locallocation[0],opt.locallocation[1])
            else:
                print >>sys.stderr, 'ERROR: No parameter --locallocation for AthenaLocalDataset given'
                sys.exit(4)
        if opt.inputtype=='Castor':
            j.inputdata = ATLASCastorDataset()
    else:        
        j.inputdata = DQ2Dataset()
        opt.inputtype = 'DQ2'

    if opt.inputtype == 'DQ2':
        j.inputdata.dataset = opt.input_dataset.split(',')
        if opt.inputaccessmode:
            j.inputdata.type = opt.inputaccessmode        
        else:    
            j.inputdata.type = 'DQ2_LOCAL'

	if opt.inputfailover:
	    j.inputdata.failover=True
        if opt.inputaccessprotocol:
            j.inputdata.accessprotocol = opt.inputaccessprotocol        
        if opt.backend == 'lcg' and not opt.ce and opt.match_ce_all=='match_ce_all':
            j.inputdata.match_ce_all = True
        if opt.input_minnumfiles>0:
            j.inputdata.min_num_files = opt.input_minnumfiles
        if opt.inputnames:
            j.inputdata.names = opt.inputnames.split(':')
        if opt.excludeinputnames:
            j.inputdata.exclude_names = opt.inputnames.split(':')
        if opt.inputnumfiles:
            j.inputdata.number_of_files = opt.inputnumfiles
        if opt.tag_input_dataset:
            j.inputdata.tagdataset = opt.tag_input_dataset
        if opt.backend == 'lcg' and opt.use_aodesd_backnav:
            j.inputdata.use_aodesd_backnav = True


# outputdata

if opt.outputdata:
    if opt.outputtype:         
        if opt.outputtype == 'ATLAS':
            j.outputdata = ATLASOutputDataset()
        elif opt.outputtype == 'DQ2':
            j.outputdata = DQ2OutputDataset()
            if opt.output_dataset:
                j.outputdata.datasetname = opt.output_dataset
    else:        
        j.outputdata = DQ2OutputDataset()
        if opt.output_dataset:
             j.outputdata.datasetname = opt.output_dataset

    j.outputdata.outputdata = opt.outputdata.split(':')
    if opt.output_location:
        j.outputdata.location = opt.output_location
    if opt.output_local_location:
        j.outputdata.local_location = opt.output_local_location
#    if opt.use_datasetname:
#        j.outputdata.use_datasetname = True
    if opt.use_shortfilename:
        j.outputdata.use_shortfilename = True

if opt.backend=='panda':
    j.outputdata = DQ2OutputDataset()
    if opt.output_dataset: j.outputdata.datasetname = opt.output_dataset

# splitter

if opt.numsubjobs > 1:
    if (opt.backend == 'lcg' or opt.glite) and not opt.site:
        j.splitter = DQ2JobSplitter()
    elif (opt.backend in [ 'panda', 'ng' ]):
        j.splitter = DQ2JobSplitter()
    else:
        j.splitter = AthenaSplitterJob()
    j.splitter.numsubjobs = opt.numsubjobs

    if opt.outputdata:
        j.merger=AthenaOutputMerger()
elif opt.numfiles_subjob>1:
    if (opt.backend == 'lcg' or opt.glite) and not opt.site:
        j.splitter = DQ2JobSplitter()
        j.splitter.numfiles = opt.numfiles_subjob
    elif (opt.backend in [ 'panda', 'ng' ]):
        j.splitter = DQ2JobSplitter()
        j.splitter.numfiles = opt.numfiles_subjob
    else:
        j.splitter = AthenaSplitterJob()
        j.splitter.numfiles_subjob = opt.numfiles_subjob

    if opt.outputdata:
        j.merger=AthenaOutputMerger()

elif opt.filesize>0:
    if (opt.backend == 'lcg' or opt.glite) and not opt.site:
        j.splitter = DQ2JobSplitter()
        j.splitter.filesize = opt.filesize
    elif (opt.backend in [ 'panda', 'ng' ]):
        j.splitter = DQ2JobSplitter()
        j.splitter.filesize = opt.filesize

    if opt.outputdata:
        j.merger=AthenaOutputMerger()

if opt.match_subjobs_files:
    j.splitter = AthenaSplitterJob()
    j.splitter.match_subjobs_files = True

    if opt.outputdata:
        j.merger=AthenaOutputMerger()

# backend

if opt.backend == 'lsf':
    j.backend = LSF()
    if opt.queue: j.backend.queue = opt.queue
elif opt.backend == 'lcg' or opt.glite:
    j.backend = LCG()
    if opt.ce: j.backend.CE = opt.ce 
    if opt.site: 
        j.backend.requirements=AtlasLCGRequirements()
        j.backend.requirements.sites=opt.site.split(':') 
    if opt.cloud: 
        j.backend.requirements=AtlasLCGRequirements()
        j.backend.requirements.cloud=opt.cloud 
    if opt.glite: 
        j.backend.middleware = 'GLITE'
    elif opt.edg: 
        j.backend.middleware = 'EDG' 
    if opt.long: j.backend.requirements.cputime = 8 * 60;
    if opt.cputime: j.backend.requirements.cputime = opt.cputime
    if opt.walltime: j.backend.requirements.walltime = opt.walltime
    if opt.memory: j.backend.requirements.memory = opt.memory
elif opt.backend == 'pbs':
    j.backend = PBS()
    if opt.queue: j.backend.queue = opt.queue
elif opt.backend == 'sge':
    j.backend = SGE()
    if opt.queue: j.backend.queue = opt.queue
elif opt.backend=='ng':
    try:
        j.backend = NG()
    except NameError:
        print >>sys.stderr, 'ERROR: Nordugrid backend has not been enabled in the gangarc file'
        sys.exit(100)
    if opt.ce: j.backend.CE = opt.ce
    if opt.rejectce: j.backend.RejectCE = opt.rejectce
    if opt.walltime: j.backend.requirements.walltime = opt.walltime
    if opt.cputime: j.backend.requirements.cputime = opt.cputime
    if opt.memory: j.backend.requirements.memory = opt.memory
    if opt.check_availability: j.backend.check_availability = True
    if opt.runtimeenvironment: j.backend.requirements.runtimeenvironment = [opt.runtimeenvironment]

elif opt.backend=='panda':
    try:
        j.backend = Panda()
        if opt.cloud: j.backend.cloud = opt.cloud
    except NameError:
        print >>sys.stderr, 'ERROR: Panda backend has not been enabled in the gangarc file'
        sys.exit(100)
    if opt.site: j.backend.site = opt.site
    if opt.long: j.backend.long = True 
else:
    j.backend = Local()

# Inputsandbox
if opt.inputsandbox:
    j.inputsandbox = opt.inputsandbox.split(':')        

# Outputsandbox
if opt.outputsandbox:
    j.outputsandbox = opt.outputsandbox.split(':')        

print j

try:
    j.submit()
except GangaException, e:
    print >>sys.stderr, 'ERROR: %s - %s' % (type(e),e)


