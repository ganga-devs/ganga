# Ganga configuration file ($Name: not supported by cvs2svn $). DO NOT remove this line.
#
#=======================================================================
# All  settings are  commented out,  so ganga  will apply  the default
# values automatically. Wherever possible these default values are indicated. 
# To see which configuration settings are used, type at ganga prompt:
#  print config
#
# Support of Atlas and LHCb-specific functionality:
#
# GANGA_CONFIG_PATH environment variable or --config-path option at the command
# line is used to enable LHCb or Atlas extensions. 
# For example:
#    $ export GANGA_CONFIG_PATH=GangaLHCb/LHCb.ini
#    $ ganga
# or:
#    $ ganga --config-path=GangaAtlas/Atlas.ini
#
# In LHCb environment you do not need to worry about it because it is done
# by GangaEnv command automatically.
#


#=======================================================================
#  NG configuration parameters
[ARC]

#  Turn ON/OFF the ARC middleware support
#ARC_ENABLE = True

#  FIXME Environment setup script for ARC middleware
#ARC_SETUP = /afs/cern.ch/sw/ganga/external/nordugrid-arc-standalone/0.6.3/slc4_amd64_gcc34//setup.sh

#  sets the size limitation of the input sandbox, oversized input sandbox will
#  be pre-uploaded to rls
#BoundSandboxLimit = 1048576

#  FIXME under testing sets the full qualified class name forother specific NG
#  job requirements
#Requirements = GangaNG.Lib.NG.NGRequirements


#=======================================================================
#  Athena configuration parameters
[Athena]

#  FIXME
#ATLASOutputDatasetLFC = prod-lfc-atlas-local.cern.ch

#  FIXME
#ATLAS_SOFTWARE = /afs/cern.ch/project/gd/apps/atlas/slc3/software

#  The path in which the cmtsetup magic function will look up the setup.sh for
#  CMT environment setup
#CMTHOME = /afs/cern.ch/user/g/gangaat/cmthome

#  FIXME
#ExcludedSites = 

#  FIXME
#LCGOutputLocation = srm://srm-atlas.cern.ch/castor/cern.ch/grid/atlas/scratch/gangaat/ganga

#  FIXME
#LocalOutputLocation = /castor/cern.ch/atlas/scratch/gangaat/ganga

#  Number of maximum jobs allowed for job splitting with the AthenaSplitterJob
#  and the LCG backend
#MaxJobsAthenaSplitterJobLCG = 100

#  Maximum number of allowed subjobs of DQ2JobSplitter
#MaxJobsDQ2JobSplitter = 100

#  FIXME
#PRODUCTION_ARCHIVE_BASEURL = http://atlas-computing.web.cern.ch/atlas-computing/links/kitsDirectory/Production/kits/


#=======================================================================
#  AthenaMC configuration options
[AthenaMC]


#=======================================================================
#  AthenaMCDatasets configuration options
[AthenaMCDatasets]


#=======================================================================
#  global configuration parameters. this is a catch all section.
[Configuration]

#  runtime warnings issued by the interpreter may be suppresed
#IgnoreRuntimeWarnings = False

#  the search path for the load() function
#LOAD_PATH = 

#  path to runtime plugin packages where custom handlers may be added. Normally
#  you should not worry about it. If an element of the path is just a name (like
#  in the example below) then the plugins will be loaded using current python
#  path. This means that some packages such as GangaTest may be taken from the
#  release area.
#  Examples:
#    RUNTIME_PATH = GangaGUI
#    RUNTIME_PATH = /my/SpecialExtensions:GangaTest
#RUNTIME_PATH = 

#  the search path to scripts directory. When running a script from the system
#  shell (e.g. ganga script) this path is used to search for script
#SCRIPTS_PATH = Ganga/scripts

#  block of GPI commands executed at startup
#StartupGPI = 

#  The type of the interactive shell: IPython (cooler) or Console (limited)
#TextShell = IPython

#  MonALISA configuration file used to setup the destination of usage messages
#UsageMonitoringURL = http://gangamon.cern.ch:8080/apmon/ganga.conf

#  Location of local job repositories and workspaces. Default is ~/gangadir but
#  in somecases (such as LSF CNAF) this needs to be modified to point to the
#  shared file system directory.
#gangadir = /afs/cern.ch/user/g/gangaat/gangadir

#  Type of the repository.
#  Examples:
#    LocalAMGA,RemoteAMGA,LocalXML
#repositorytype = LocalAMGA

#  User name. The same person may have different roles (user names) and still
#  use the same gangadir. Unless explicitly set this option defaults to the real
#  user name.
#user = gangaat

#  Type of workspace. Workspace is a place where input and output sandbox of
#  jobs are stored. Currently the only supported type is LocalFilesystem.
#workspacetype = LocalFilesystem


#=======================================================================
#  DQ2 configuration options
[DQ2]

#  Default backup locations of DQ2OutputDataset output
#DQ2_BACKUP_OUTPUT_LOCATIONS = ['CERN-PROD_USERDISK', 'CERN-PROD_USERTAPE', 'FZK-LCG2_USERDISK', 'IN2P3-CC_USERDISK', 'TRIUMF-LCG2_USERDISK', 'IFAE_USERDISK', 'NIKHEF-ELPROD_USERDISK']

#  Allowed space tokens names of DQ2OutputDataset output
#DQ2_OUTPUT_SPACE_TOKENS = ['ATLASUSERDISK', 'ATLASUSERTAPE', 'ATLASLOCALGROUPDISK']

#  FIXME
#DQ2_URL_SERVER = http://atlddmcat.cern.ch/dq2/

#  FIXME
#DQ2_URL_SERVER_SSL = https://atlddmcat.cern.ch:443/dq2/

#  Allow DQ2 subscription to aggregate DQ2OutputDataset output on a storage
#  element instead of using remote lcg-cr
#USE_STAGEOUT_SUBSCRIPTION = False


#=======================================================================
#  control the printing style of the job registry ("print jobs")
[Display]

#  colour print of the docstrings and examples
#config_docstring_colour = fg.green

#  colour print of the names of configuration sections and options
#config_name_colour = fx.bold

#  colour print of the configuration values
#config_value_colour = fx.bold

#  list of job attributes to be printed in separate columns
#registry_columns = ('fqid', 'status', 'name', 'subjobs', 'application', 'backend', 'backend.actualCE')

#  optional converter functions
#registry_columns_converter = {'subjobs': 'lambda s: len(s)', 'backend': 'lambda b:b._name', 'application': 'lambda a: a._name'}

#  with exception of columns mentioned here, hide all values which evaluate to
#  logical false (so 0,"",[],...)
#registry_columns_show_empty = ['fqid']

#  width of each column
#registry_columns_width = {'fqid': 5, 'status': 10, 'name': 10, 'subjobs': 8, 'backend.actualCE': 45, 'application': 15, 'backend': 15}


#=======================================================================
#  default associations between file types and file-viewing commands.
[File_Associations]

#  FIXME
#fallback_command = less &&

#  FIXME
#html = mozilla &

#  FIXME
#listing_command = ls -lhtr

#  FIXME
#newterm_command = xterm

#  FIXME
#newterm_exeopt = -e

#  FIXME
#root = root.exe &&


#=======================================================================
#  Customization of GPI component object assignment for each category there may
#  be multiple filters registered, the one used being defined  in the
#  configuration file in [GPIComponentFilters] e.g:
#  {'datasets':{'lhcbdatasets':lhcbFilter, 'testdatasets':testFilter}...}
[GPIComponentFilters]


#files = string_file_shortcut


#=======================================================================
#  LCG/gLite/EGEE configuration parameters
[LCG]

#  sets allowed computing elements by a regular expression
#AllowedCEs = 

#  sets the size limitation of the input sandbox, oversized input sandbox will
#  be pre-uploaded to the storage element specified by 'DefaultSE' in the area
#  specified by 'DefaultSRMToken'
#BoundSandboxLimit = 10485760

#  sets the generic LCG-UI configuration script for the GLITE workload
#  management system
#Config = 

#  sets the VO-specific LCG-UI configuration script for the EDG resource broker
#ConfigVO = 

#  sets the file catalogue server
#DefaultLFC = prod-lfc-shared-central.cern.ch

#  sets the default storage element
#DefaultSE = srm.cern.ch

#  sets the space token for storing temporary files (e.g. oversized input
#  sandbox)
#DefaultSRMToken = 

#  enables/disables the support of the EDG middleware
#EDG_ENABLE = True

#  sets the LCG-UI environment setup script for the EDG middleware
#EDG_SETUP = /afs/cern.ch/project/gd/LCG-share/current/etc/profile.d/grid_env.sh

#  sets excluded computing elements by a regular expression
#ExcludedCEs = 

#  Enables/disables the support of the GLITE middleware
#GLITE_ENABLE = False

#  sets the LCG-UI environment setup script for the GLITE middleware
#GLITE_SETUP = /afs/cern.ch/project/gd/LCG-share/current/etc/profile.d/grid_env.sh

#  sets the myproxy server
#MyProxyServer = myproxy.cern.ch

#  sets the ranking rule for picking up computing element
#Rank = 

#  sets the replica catalogue server
#ReplicaCatalog = 

#  sets the full qualified class name for other specific LCG job requirements
#Requirements = Ganga.Lib.LCG.LCGRequirements

#  sets maximum number of job retry
#RetryCount = 3

#  sets the transfer timeout of the oversized input sandbox
#SandboxTransferTimeout = 60

#  sets maximum number of job shallow retry
#ShallowRetryCount = 10

#  sets the storage index
#StorageIndex = 

#  sets the name of the grid virtual organisation
#VirtualOrganisation = dteam


#=======================================================================
#  internal LSF command line interface
[LSF]

#  Name of environment with ID of the job
#jobid_name = LSB_BATCH_JID

#  String contains option name for name of job in batch system
#jobnameopt = J

#  String pattern for replay from the kill command
#kill_res_pattern = (^Job <\d+> is being terminated)|(Job <\d+>: Job has already finished)|(Job <\d+>: No matching job found)

#  String used to kill job
#kill_str = bkill %s

#  String contains commands executing before submiting job to queue
#postexecute = 
# def filefilter(fn):
#   # FILTER OUT Batch INTERNAL INPUT/OUTPUT FILES: 
#   # 10 digits . any number of digits . err or out
#   import re
#   internals = re.compile(r'\d{10}\.\d+.(out|err)')
#   return internals.match(fn) or fn == '.Batch.start'

#  String contains commands executing before submiting job to queue
#preexecute = 


#  Name of environment with queue name of the job
#queue_name = LSB_QUEUE

#  Shared PYTHON
#shared_python_executable = False

#  String pattern for replay from the submit command
#submit_res_pattern = ^Job <(?P<id>\d*)> is submitted to .*queue <(?P<queue>\S*)>

#  String used to submit job to queue
#submit_str = cd %s; bsub %s %s %s %s


#=======================================================================
#  parameters of the local backend (jobs in the background on localhost)
[Local]

#  remove automatically the local working directory when the job completed
#remove_workdir = True


#=======================================================================
#  Settings for the local AMGA job repository
[LocalAMGA_Repository]

#  maximum number of jobs stored in a block of local repository
#blocklength = 1000

#  maximum size of memory (in blocks) that local repository can use for job
#  caching
#cache_size = 3

#  maximum time in seconds that limits lock validity for local repository
#lock_timeout = 60

#  maximum number of attempts to write/move file or to acquire the table lock in
#  local repository
#tries_limit = 200


#=======================================================================
#  Parameters of the local XML-based job repository
[LocalXML_Repository]

#  (ADVANCED DEBUGGING) enable/disable profiling of job repository at startup
#DEBUG_startup_profile = False


#=======================================================================
#  control the messages printed by Ganga The settings are applied hierarchically
#  to the loggers. Ganga is the name of the top-level logger which applies by
#  default to all Ganga.* packages unless overriden in sub-packages. You may
#  define new loggers in this section. The log level may be one of: CRITICAL
#  ERROR WARNING INFO DEBUG
[Logging]

#  top-level logger
#Ganga = WARNING

#  logger of Ganga.GPIDev.* packages
#Ganga.GPIDev = INFO

#  FIXME
#Ganga.Runtime.bootstrap = INFO

#  logger of the Ganga logging package itself (use with care!)
#Ganga.Utility.logging = WARNING


#GangaAtlas = INFO

#  enable ASCII colour formatting of messages e.g. errors in red
#_colour = True

#  format of logging messages: TERSE,NORMAL,VERBOSE,DEBUG
#_format = NORMAL

#  if True then the cache used for interactive sessions, False disables caching
#_interactive_cache = True

#  location of the logfile
#_logfile = ~/.ganga.log

#  the size of the logfile (in bytes), the rotating log will never exceed this
#  file size
#_logfile_size = 100000


#=======================================================================
#  parameters for mergers
[Mergers]

#  Dictionary of file associations
#associate = {'log':'TextMerger','root':'RootMerger','text':'TextMerger','txt':'TextMerger'}

#  location of the merger's outputdir
#merge_output_dir = ~/gangadir/merge_results

#  Standard (default) merger
#std_merge = TextMerger


#=======================================================================
#  migration of different job versions in the peristent repository
[MigrationControl]

#  display limited number of choices for the interactive migration
#display = compact

#  no plugin migration
#migration = deny


#=======================================================================
#  External monitoring systems are used to follow the submission and execution
#  of jobs. Each entry in this section defines a monitoring plugin used for a
#  particular combination of application and backend. Asterisks may be used to
#  specify any application or any backend. The configuration entry syntax:
#  ApplicationName/BackendName = dot.path.to.monitoring.plugin.class.  Example:
#  DummyMS plugin will be used to track executables run on all backends:
#  Executable/* = Ganga.Lib.MonitoringServices.DummyMS.DummyMS
[MonitoringServices]

#  FIXME
#Athena = None

#  FIXME
#Athena/LCG = None

#  FIXME
#Athena/NG = None

#  FIXME
#AthenaMC = None

#  FIXME
#AthenaMC/LCG = None


#=======================================================================
#  internal PBS command line interface
[PBS]

#  Name of environment with ID of the job
#jobid_name = PBS_JOBID

#  String contains option name for name of job in batch system
#jobnameopt = N

#  String pattern for replay from the kill command
#kill_res_pattern = (^$)|(qdel: Unknown Job Id)

#  String used to kill job
#kill_str = qdel %s

#  String contains commands executing before submiting job to queue
#postexecute = 


#  String contains commands executing before submiting job to queue
#preexecute = 
# env = os.environ
# jobnumid = env["PBS_JOBID"]
# os.system("mkdir /tmp/%s/" %jobnumid)
# os.chdir("/tmp/%s/" %jobnumid)
# os.environ["PATH"]+=":."

#  Name of environment with queue name of the job
#queue_name = PBS_QUEUE

#  Shared PYTHON
#shared_python_executable = False

#  String pattern for replay from the submit command
#submit_res_pattern = ^(?P<id>\d*)\.pbs\s*

#  String used to submit job to queue
#submit_str = cd %s; qsub %s %s %s %s


#=======================================================================
#  Panda backend configuration parameters
[Panda]

#  FIXME
#assignedPriority = 1000

#  FIXME
#prodSourceLabel = user


#=======================================================================
#  General control of plugin mechanism. Set the default plugin in a given
#  category. For example: default_applications = DaVinci default_backends = LCG
[Plugins]


#=======================================================================
#  background job status monitoring and output retrieval
[PollThread]

#  Poll rate for Condor backend.
#Condor = 30

#  Poll rate for Dirac backend.
#Dirac = 50

#  disk space checking callback. This function should return False when there is
#  no disk space available, True otherwise
#DiskSpaceChecker = 

#  Poll rate for LCG backend.
#LCG = 30

#  Poll rate for LSF backend.
#LSF = 20

#  Poll rate for Local backend.
#Local = 10

#  Poll rate for PBS backend.
#PBS = 20

#  Poll rate for Panda backend.
#Panda = 50

#  enable monitoring automatically at startup, in script mode monitoring is
#  disabled by default, in interactive mode it is enabled
#autostart = False

#  internal supervising thread
#base_poll_rate = 2

#  The frequency in seconds for credentials checker
#creds_poll_rate = 30

#  Default rate for polling job status in the thread pool. This is the default
#  value for all backends.
#default_backend_poll_rate = 30

#  The frequency in seconds for free disk checker
#diskspace_poll_rate = 30

#  Poll rate for gLite backend.
#gLite = 30

#  number of retries that should be made for a clean shutdown
#max_shutdown_retries = 5

#  if 0 then log only once the errors for a given backend and do not repeat them
#  anymore
#repeat_messages = False

#  Size of the thread pool. Each threads monitors a specific backaend at a given
#  time. Minimum value is one, preferably set to the number_of_backends + 1
#update_thread_pool_size = 5


#=======================================================================
#  Options for Root backend
[ROOT]

#  Architecture of ROOT
#arch = slc4_ia32_gcc34

#  Location of ROOT
#location = /afs/cern.ch/sw/lcg/external/root

#  Set to a specific ROOT version. Will override other options.
#path = 

#  Location of the python used for execution of PyROOT script
#pythonhome = ${location}/../Python/${pythonversion}/${arch}/

#  Version number of python used for execution python ROOT script
#pythonversion = 

#  Version of ROOT
#version = 5.18.00


#=======================================================================
#  Settings for the local AMGA job repository
[RemoteAMGA_Repository]

#  location of the AMGA metadata server used by the remote repository
#host = gangamd.cern.ch

#  login name to connect to the remote repository
#login = 

#  port for secure connection to the remote repository
#port = 8822

#  flag for secure connection to the remote repository
#reqSSL = 1


#=======================================================================
#  internal SGE command line interface
[SGE]

#  Name of environment with ID of the job
#jobid_name = JOB_ID

#  String contains option name for name of job in batch system
#jobnameopt = N

#  String pattern for replay from the kill command
#kill_res_pattern = (has registered the job +\d+ +for deletion)|(denied: job +"\d+" +does not exist)

#  String used to kill job
#kill_str = qdel %s

#  String contains commands executing before submiting job to queue
#postexecute = 

#  String contains commands executing before submiting job to queue
#preexecute = os.chdir(os.environ["TMPDIR"])
# os.environ["PATH"]+=":."

#  Name of environment with queue name of the job
#queue_name = QUEUE

#  Shared PYTHON
#shared_python_executable = False

#  String pattern for replay from the submit command
#submit_res_pattern = Your job (?P<id>\d+) (.+)

#  String used to submit job to queue
#submit_str = cd %s; qsub -cwd -V %s %s %s %s


#=======================================================================
#  configuration parameters for internal Shell utility.
[Shell]

#  list of env variables not inherited in Shell environment
#IgnoredVars = ['_', 'SHVL', 'PWD']


#=======================================================================
#  IPython shell configuration See IPython manual for more details:
#  http://ipython.scipy.org/doc/manual
[TextShell_IPython]

#  FIXME
#args = ['-colors','LightBG', '-noautocall']


#=======================================================================
#  default attribute values for ATLASCastorDataset objects
[defaults_ATLASCastorDataset]

#  The name of the selected dataset
#dataset = 

#  A directory on castor that contains the datasets in directories
#location = 

#  The selected file names
#names = []

#  A regexp filter to select the correct files
#pattern = 


#=======================================================================
#  default attribute values for ATLASDataset objects
[defaults_ATLASDataset]

#  LFC Catalog address
#lfc = 

#  List of input file lfns
#lfn = []


#=======================================================================
#  default attribute values for ATLASLocalDataset objects
[defaults_ATLASLocalDataset]

#  List of input files with full path
#names = []


#=======================================================================
#  default attribute values for ATLASOutputDataset objects
[defaults_ATLASOutputDataset]

#  Local output path location
#local_location = 

#  SE output path location
#location = 

#  Output files to be returned via SE
#outputdata = []


#=======================================================================
#  default attribute values for AbstractJob objects
[defaults_AbstractJob]

#  This is set to true if this task is done. Unset if this was set to done by
#  mistake
#done = False

#  exclude CEs for this job
#excluded_CEs = []

#  if this specific job makes problems ignore it without pausing the task
#ignore_this = False

#  Name of this job
#name = analysis:0

#  Number of attempts that should be made before an error is triggered
#run_limit = 4

#  Sites where the job could run
#sites = None

#  duration of the GANGA-job status
#status_duration = {}

#  Name of the task this job is part of
#task = 


#=======================================================================
#  default attribute values for AfsCommand objects
[defaults_AfsCommand]

#  Command for destroying credential
#destroy = unlog

#  Dictionary of parameter-value pairs to pass to destroy command
#destroy_parameters = {'cell': '-cell'}

#  Command for obtaining information about credential
#info = tokens

#  Dictionary mapping from Ganga credential properties to command-line options
#info_parameters = {}

#  Command for creating/initialising credential
#init = klog

#  Dictionary of parameter-value pairs to pass to init command
#init_parameters = {'pipe': '-pipe', 'username': '-principal', 'valid': '-lifetime', 'cell': '-cell'}


#=======================================================================
#  default attribute values for AfsToken objects
[defaults_AfsToken]

#  AFS cell with which token is used [empty string implies local cell]
#cell = 

#  Set of commands to be used for credential-related operations
#command = ICommandSet

#  Number of password attempts allowed when creating credential
#maxTry = 1

#  Default minimum validity
#minValidity = 00:15

#  AFS username with which token is used [defaults to login id]
#username = 

#  Default credential validity at creation
#validityAtCreation = 24:00


#=======================================================================
#  default attribute values for AnaJob objects
[defaults_AnaJob]

#  This is set to true if this task is done. Unset if this was set to done by
#  mistake
#done = False

#  exclude CEs for this job
#excluded_CEs = []

#  Files which will be analysed by the job
#files = []

#  if this specific job makes problems ignore it without pausing the task
#ignore_this = False

#  Name of this job
#name = analysis:0

#  Sequential task number in the dataset
#number = 0

#  Number of attempts that should be made before an error is triggered
#run_limit = 4

#  Sites where the job could run
#sites = None

#  duration of the GANGA-job status
#status_duration = {}

#  Name of the task this job is part of
#task = 


#=======================================================================
#  default attribute values for AnaTask objects
[defaults_AnaTask]

#  Name of the CE queue where the computation should take place
#CE = 

#  contains job name, files to be analysed and sites where to run
#abstract_jobs = {}


#allow_change = False

#  sites where the job is allowed to run
#allowed_sites = []

#  content of jobOption file
#app_opt_file_content = []

#  Pattern of files to exclude from user area
#application_exclude_from_user_area = ['*.o', '*.root*', '*.exe']


#application_group_area = 

#  Total number of events to analyze
#application_max_events = -1

#  Set this to the full path to your analysis jobOption file
#application_option_file = 

#  Athena versions to use
#athena_version = 12.0.6

#  output dataset: 1=GPI.ATLASOutputDataset() or 0=GPI.DQ2OutputDataset()
#atlas_outputdata = False

#  exclude CEs
#excluded_CEs = []

#  files in the dataset which should be excluded
#excluded_files = []

#  sites which you want to exclude for this task
#excluded_sites = []

#  number of files per job
#files_per_job = -1

#  How many jobs should be run at the same time
#float = 0

#  input dataset
#inputdata_dataset = 

#  minimum number of files reqired in a dataset
#inputdata_min_num_files = -1

#  Set this if you only want to analyze some files from the input dataset
#inputdata_names = []

#  Name of the Task
#name = New Analysis Task

#  output dataset
#outputdata_datasetname = 

#  where to put the output ROOT-file
#outputdata_location = 

#  output ROOT-file
#outputdata_outputdata = []

#  create a file with name 'Task_'+ taskname+'_report' and write all incidents
#  to it
#report_file = 

#  create a file with name 'Task_'+ taskname+'_report' and write all incidents
#  to it
#report_output = False

#  sites where the job should run
#requirements_sites = []

#  List of spjobs
#spjobs = []

#  Status - new, running, paused or completed
#status = new


#stress_test = False


#stress_test_status = {}

#  Type of the Task
#type = 

#  set the float to this value if the first job runs successfully
#working_float = 0


#=======================================================================
#  default attribute values for ArgSplitter objects
[defaults_ArgSplitter]

#  A list of lists of arguments to pass to script
#args = []


#=======================================================================
#  default attribute values for Athena objects
[defaults_Athena]

#  ATLAS CMTCONFIG environment variable
#atlas_cmtconfig = 

#  Extra environment variable to be set
#atlas_environment = []

#  Athena Executable type, e.g. ATHENA, PYARA, ROOT
#atlas_exetype = ATHENA

#  ATLAS Production Software Release
#atlas_production = 

#  ATLAS Project Name
#atlas_project = 

#  ATLAS Software Release
#atlas_release = 

#  Pattern of files to exclude from user area
#exclude_from_user_area = []

#  Packages to exclude from user area requirements file
#exclude_package = []

#  A tar file of the group area
#group_area = None

#  Maximum number of events
#max_events = 

#  list of job options files
#option_file = []

#  Additional Athena options
#options = 

#  A tar file of the user area
#user_area = None

#  User setup script for special setup
#user_setupfile = None


#=======================================================================
#  default attribute values for AthenaMC objects
[defaults_AthenaMC]

#  ATLAS Software Release
#atlas_release = 

#  flag to use kit or cern AFS installation. Set to CERN for the latter, leave
#  unset otherwise.
#cmtsite = 

#  JobOption filename, or path is modified locally
#evgen_job_option = 

#  Extra arguments for the transformation, fixed value (experts only)
#extraArgs = 

#  Extra integer arguments for the transformation, with value increasing with
#  the subjob number. Please set like this: extraIncArgs="arg1=val1_0
#  arg2=val2_0" with valX_0 the value taken by the argument at the first subjob.
#  On the second subjob, the arguments will have the value valX_0 + 1 and so
#  on...  (experts only)
#extraIncArgs = 

#  evgen: sets first event number to be generated (in first job. The first event
#  number in second job will be firstevent+number_events_job and so on...).
#  simul, recon: decides how many events to be skipped in input files (= skip
#  +1). This is propagated to all subjobs.
#firstevent = 1

#  Geometry tag for simulation and reconstruction
#geometryTag = ATLAS-DC3-05

#  Step in the generation chain (evgen, simul (is simul+digit), recon,
#  template). template is to use any transformation not coverd by any of the
#  three previous steps.
#mode = 

#  Number of events per job
#number_events_job = 1

#  output partition number
#partition_number = 

#  Name of the generated physics process
#process_name = 

#  Name of the MC production
#production_name = 

#  Random Seed for MC Generator
#random_seed = 1

#  Run number
#run_number = 

#  Name of prefered SE or DQ2 site (from TierOfAtlas.py) for output
#se_name = none

#  location of experiment software area for non-grid backends.
#siteroot = 

#  Name or Web location of a modified ATLAS transform archive.
#transform_archive = 

#  File name of the transformation script to use
#transform_script = 

#  recon, 12.0.5 and beyond: trigger configuration
#triggerConfig = NONE

#  Verbosity of transformation for log files
#verbosity = ERROR

#  version tag to insert in the output dataset and file names
#version = 


#=======================================================================
#  default attribute values for AthenaMCInputDatasets objects
[defaults_AthenaMCInputDatasets]

#  DQ2 Dataset Name
#DQ2dataset = 

#  LFC path of directory to find inputfiles on the grid, or local directory path
#  for input datasets (datasetType=local). For all non-DQ2 datasets.
#LFCpath = 

#  Name of the dataset to be used for cavern noise (pileup jobs) or extra input
#  dataset (other transforms). This dataset must be a DQ2 dataset
#cavern = 

#  Type of dataset(DQ2,private,unknown or local). DQ2 means the requested
#  dataset is registered in DQ2 catalogs, private is for input datasets
#  registered in a non-DQ2 storage (Tier3) and known to CERN local LFC. local is
#  for local datasets on Local backend only
#datasetType = unknown

#  Logical File Names of subset of files to be processed. Must be used in
#  conjunction of either DQ2dataset or LFCpath.
#inputfiles = []

#  String of input file numbers to be used (each block separated by a coma).A
#  block can be a single number or a closed subrange (x-y). Subranges are
#  defined with a dash. Must be used in conjunction of either DQ2dataset or
#  LFCpath. Alternative to inputfiles.
#inputpartitions = 

#  Name of the dataset to be used for minimum bias (pileup jobs) or extra input
#  dataset (other transforms). This dataset must be a DQ2 dataset
#minbias = 

#  Number of input cavern files processed by one job or subjob. Minimum 1
#n_cavern_files_job = 1

#  Number of input files processed by one job or subjob. Minimum 1
#n_infiles_job = 1

#  Number of input cavern files processed by one job or subjob. Minimum 1
#n_minbias_files_job = 1

#  Number of inputfiles to process.
#number_inputfiles = 


#=======================================================================
#  default attribute values for AthenaMCOutputDatasets objects
[defaults_AthenaMCOutputDatasets]

#  file prefix and dataset suffix for logfiles.
#logfile = 

#  file prefix and dataset suffix for AOD files. Placeholder for any type of
#  output file in template mode.
#outaodfile = 

#  path of output directory tree for storage. Used for both LFC and physical
#  file locations.
#outdirectory = 

#  file prefix and dataset suffix for ESD files. Placeholder for any type of
#  output file in template mode.
#outesdfile = 

#  file prefix and dataset suffix for histogram files. Placeholder for any type
#  of output file in template mode.
#outhistfile = 

#  file prefix and dataset suffix for ntuple files. Placeholder for any type of
#  output file in template mode.
#outntuplefile = 

#  dataset suffix for combined output dataset. If set, it will collect all
#  expected output files for the job. If not set, every output type (histo,
#  HITS, EVGEN...) will have its own output dataset.
#output_dataset = 

#  offset for output file partition numbers. First job will generate the
#  partition number output_firstfile, second will generate output_firstfile+1,
#  and so on...
#output_firstfile = 1

#  file prefix and dataset suffix for RDO files. Placeholder for any type of
#  output file in template mode.
#outrdofile = 

#  file prefix and dataset suffix for primary output root file (EVGEN for evgen
#  jobs, HITS for simul jobs). Placeholder for any type of output file in
#  template mode.
#outrootfile = 


#=======================================================================
#  default attribute values for AthenaMCSplitterJob objects
[defaults_AthenaMCSplitterJob]

#  Number of subjobs processing one inputfile (N to M splitting)
#nsubjobs_inputfile = 1

#  Number of subjobs
#numsubjobs = 1


#=======================================================================
#  default attribute values for AthenaOutputDataset objects
[defaults_AthenaOutputDataset]

#  Files to be returned
#files = []

#  Output location
#location = 


#=======================================================================
#  default attribute values for AthenaOutputMerger objects
[defaults_AthenaOutputMerger]

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  Subjob numbers to be merged
#subjobs = []

#  Output directory of merged files
#sum_outputdir = 


#=======================================================================
#  default attribute values for AthenaSplitterJob objects
[defaults_AthenaSplitterJob]

#  Match the number of subjobs to the number of inputfiles
#match_subjobs_files = False

#  Number of files per subjob
#numfiles_subjob = 0

#  Number of subjobs
#numsubjobs = 0


#=======================================================================
#  default attribute values for AtlasLCGRequirements objects
[defaults_AtlasLCGRequirements]

#  ATLAS cloud name: CERN, IT, ES, FR, UK, DE, NL, TW, CA, US, NG
#cloud = 

#  Minimum available CPU time (min)
#cputime = None

#  ATLAS site names to be excluded
#excluded_sites = []

#  External connectivity
#ipconnectivity = False

#  Mininum available memory (MB)
#memory = None

#  Number of Nodes for MPICH jobs
#nodenumber = 1

#  Operation Systems
#os = 

#  Other Requirements
#other = []

#  ATLAS site names
#sites = []

#  Software Installations
#software = []

#  Mimimum available total time (min)
#walltime = None


#=======================================================================
#  default attribute values for Condor objects
[defaults_Condor]

#  Environment settings for execution host
#env = {}

#  Globus RSL settings (for Condor-G submission)
#globus_rsl = 

#  Globus scheduler to be used (required for Condor-G submission)
#globusscheduler = 

#  Ranking scheme to be used when selecting execution host
#rank = Memory

#  Requirements for selecting execution host
#requirements = CondorRequirements

#  Flag indicating if Condor nodes have shared filesystem
#shared_filesystem = True

#  Options passed to Condor at submission time
#submit_options = []

#  Type of execution environment to be used by Condor
#universe = vanilla


#=======================================================================
#  default attribute values for CondorRequirements objects
[defaults_CondorRequirements]

#  System architecture
#arch = INTEL

#  Excluded execution hosts, given as a string of space-separated names:
#  'machine1 machine2 machine3'; or as a list of names: [ 'machine1',
#  'machine2', 'machine3' ]
#excluded_machine = 

#  Requested execution hosts, given as a string of space-separated names:
#  'machine1 machine2 machine3'; or as a list of names: [ 'machine1',
#  'machine2', 'machine3' ]
#machine = 

#  Mininum physical memory
#memory = 400

#  Operating system
#opsys = LINUX

#  Other requirements, given as a list of strings, for example: [ 'OSTYPE ==
#  "SLC4"', '(POOL == "GENERAL" || POOL == "GEN_FARM")' ]; the final requirement
#  is the AND of all elements in the list
#other = []

#  Minimum virtual memory
#virtual_memory = 400


#=======================================================================
#  default attribute values for CustomMerger objects
[defaults_CustomMerger]

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  Path to a python module to perform the merge.
#module = None

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for DQ2Dataset objects
[defaults_DQ2Dataset]

#  Accessprotocol to use on worker node, e.g. Xrootd
#accessprotocol = 

#  Check md5sum of input files on storage elemenet - very time consuming !
#check_md5sum = False

#  Dataset Name(s)
#dataset = []

#  Data type: DATA, MC or MuonCalibStream
#datatype = 

#  Logical File Names to exclude from processing
#exclude_names = []

#  GUID of Logical File Names
#guids = []

#  Match complete and incomplete sources of dataset to CE during job submission
#match_ce_all = False

#  Number of minimum files at incomplete dataset location
#min_num_files = 0

#  Logical File Names to use for processing
#names = []

#  Number of files.
#number_of_files = 0

#  Tag Dataset Name
#tagdataset = []

#  Dataset type, DQ2 or LFN
#type = 

#  Use AOD to ESD Backnavigation
#use_aodesd_backnav = False


#=======================================================================
#  default attribute values for DQ2JobSplitter objects
[defaults_DQ2JobSplitter]

#  Number of files per subjob
#numfiles = 0

#  Number of subjobs
#numsubjobs = 0

#  Use LFC catalog instead of default site catalog/tracker service
#use_lfc = False


#=======================================================================
#  default attribute values for DQ2Output objects
[defaults_DQ2Output]

#  Dataset Name
#dataset = 

#  Output to SE
#destinationSE = 

#  Logical File Names
#names = []

#  Output Type
#type = 


#=======================================================================
#  default attribute values for DQ2OutputDataset objects
[defaults_DQ2OutputDataset]

#  Name of the DQ2 output dataset automatically filled by the job
#datasetname = 

#  Local output path location
#local_location = 

#  SE output path location
#location = 

#  Output files to be returned via SE
#outputdata = []

#  Use datasetname as it is and do not prepend users.myname.ganga
#use_datasetname = False

#  Use shorter version of filenames and do not prepend users.myname.ganga
#use_shortfilename = False


#=======================================================================
#  default attribute values for EmptyDataset objects
[defaults_EmptyDataset]


#=======================================================================
#  default attribute values for Executable objects
[defaults_Executable]

#  List of arguments for the executable. Arguments may be strings or File
#  objects.
#args = ['Hello World']

#  Environment
#env = {}

#  A path (string) or a File object specifying an executable.
#exe = echo


#=======================================================================
#  default attribute values for File objects
[defaults_File]

#  path to the file source
#name = 

#  destination subdirectory (a relative path)
#subdir = .


#=======================================================================
#  default attribute values for GangaList objects
[defaults_GangaList]


#=======================================================================
#  default attribute values for GenericSplitter objects
[defaults_GenericSplitter]

#  The attribute on which the job is splitted
#attribute = None

#  A list of the values corresponding to the attribute of the subjobs
#values = []


#=======================================================================
#  default attribute values for GridCommand objects
[defaults_GridCommand]

#  Command for destroying credential
#destroy = grid-proxy-destroy

#  Dictionary of parameter-value pairs to pass to destroy command
#destroy_parameters = {}

#  Command for obtaining information about credential
#info = grid-proxy-info

#  Dictionary mapping from Ganga credential properties to command-line options
#info_parameters = {}

#  Command for creating/initialising credential
#init = grid-proxy-init

#  Dictionary of parameter-value pairs to pass to init command
#init_parameters = {'pipe': '-pwstdin', 'valid': '-valid'}


#=======================================================================
#  default attribute values for GridProxy objects
[defaults_GridProxy]

#  Set of commands to be used for credential-related operations
#command = ICommandSet

#  String of options to be passed to command for proxy creation
#init_opts = 

#  Number of password attempts allowed when creating credential
#maxTry = 1

#  Default minimum validity
#minValidity = 00:15

#  Default credential validity at creation
#validityAtCreation = 24:00

#  Virtual organisation managment system information
#voms = 


#=======================================================================
#  default attribute values for ICommandSet objects
[defaults_ICommandSet]

#  Command for destroying credential
#destroy = 

#  Dictionary of parameter-value pairs to pass to destroy command
#destroy_parameters = {}

#  Command for obtaining information about credential
#info = 

#  Dictionary mapping from Ganga credential properties to command-line options
#info_parameters = {}

#  Command for creating/initialising credential
#init = 

#  Dictionary of parameter-value pairs to pass to init command
#init_parameters = {}


#=======================================================================
#  default attribute values for Interactive objects
[defaults_Interactive]


#=======================================================================
#  default attribute values for Job objects
[defaults_Job]

#  specification of the application to be executed
#application = None

#  specification of the resources to be used (e.g. batch system)
#backend = None

#  JobInfo
#info = None

#  dataset definition (typically this is specific either to an application, a
#  site or the virtual organization
#inputdata = None

#  list of File objects shipped to the worker node
#inputsandbox = []

#  optional output merger
#merger = None

#  optional label which may be any combination of ASCII characters
#name = 

#  dataset definition (typically this is specific either to an application, a
#  site or the virtual organization
#outputdata = None

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  optional splitter
#splitter = None


#=======================================================================
#  default attribute values for JobInfo objects
[defaults_JobInfo]


#=======================================================================
#  default attribute values for JobTemplate objects
[defaults_JobTemplate]

#  specification of the application to be executed
#application = None

#  specification of the resources to be used (e.g. batch system)
#backend = None

#  JobInfo
#info = None

#  dataset definition (typically this is specific either to an application, a
#  site or the virtual organization
#inputdata = None

#  list of File objects shipped to the worker node
#inputsandbox = []

#  optional output merger
#merger = None

#  optional label which may be any combination of ASCII characters
#name = 

#  dataset definition (typically this is specific either to an application, a
#  site or the virtual organization
#outputdata = None

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  optional splitter
#splitter = None


#=======================================================================
#  default attribute values for JobTree objects
[defaults_JobTree]


#name = 


#=======================================================================
#  default attribute values for LCG objects
[defaults_LCG]

#  Request a specific Computing Element
#CE = 

#  Storage element used as a cache for oversized input sandbox
#iocache = 

#  Job type: Normal, MPICH
#jobtype = Normal

#  Middleware type
#middleware = EDG

#  Enable the job perusal feature of GLITE
#perusable = False

#  Requirements for the resource selection
#requirements = None


#=======================================================================
#  default attribute values for LCGRequirements objects
[defaults_LCGRequirements]

#  Minimum available CPU time (min)
#cputime = None

#  External connectivity
#ipconnectivity = False

#  Mininum available memory (MB)
#memory = None

#  Number of Nodes for MPICH jobs
#nodenumber = 1

#  Other Requirements
#other = []

#  Software Installations
#software = []

#  Mimimum available total time (min)
#walltime = None


#=======================================================================
#  default attribute values for LSF objects
[defaults_LSF]

#  extra options for Batch
#extraopts = 

#  queue name as defomed in your local Batch installation
#queue = 


#=======================================================================
#  default attribute values for Local objects
[defaults_Local]


#=======================================================================
#  default attribute values for MCJob objects
[defaults_MCJob]

#  This is set to true if this task is done. Unset if this was set to done by
#  mistake
#done = False

#  number of evgen partition this chunk of events is descended from
#evgen_number = 0

#  exclude CEs for this job
#excluded_CEs = []

#  if this specific job makes problems ignore it without pausing the task
#ignore_this = False

#  Transformation mode of this job
#mode = evgen

#  Name of this job
#name = analysis:0

#  Number of attempts that should be made before an error is triggered
#run_limit = 4

#  number of (first) simul partition this chunk of events is from
#simul_number = 0

#  Sites where the job could run
#sites = None

#  duration of the GANGA-job status
#status_duration = {}

#  Name of the task this job is part of
#task = 


#=======================================================================
#  default attribute values for MCTask objects
[defaults_MCTask]

#  Name of the CE queue where the computation should take place
#CE = 


#allow_change = False

#  Athena versions to use
#athena_version = {'evgen': '12.0.7.2', 'recon': '12.0.7.2', 'simul': '12.0.7.2'}


#datasets_data = {'evgen': '', 'recon': '', 'simul': ''}

#  Events per job for evgen, simul and recon
#events_per_job = {'evgen': 10000, 'recon': 1000, 'simul': 50}

#  Name of the official evgen file, or path to local evgen file
#evgen_job_option = 

#  exclude CEs
#excluded_CEs = []

#  sites which you want to exclude for this task
#excluded_sites = []


#filenames_data = {'evgen': '', 'recon': '', 'simul': ''}

#  How many jobs should be run at the same time
#float = 0

#  Name of the generator if input files are needed, p.e. McAtNlo or alpgen
#generator = 

#  Geometry Tag to use, defaults to ideal Geometry
#geometry_tag = {'recon': 'ATLAS-CSC-01-00-00', 'simul': 'ATLAS-CSC-01-00-00'}

#  Name of the Task
#name = New Analysis Task

#  Name of the process
#process_name = CUSTOM

#  Random seed for Monte Carlo
#random_seed = 1102362401

#  create a file with name 'Task_'+ taskname+'_report' and write all incidents
#  to it
#report_file = 

#  create a file with name 'Task_'+ taskname+'_report' and write all incidents
#  to it
#report_output = False

#  sites where the job should run
#requirements_sites = []

#  Official run number of the monte carlo process
#run_number = 0

#  Name of the SE where the data should be stored
#se_name = FZKDISK

#  Number of evgen partitions to skip
#skip_evgen_partitions = 0

#  List of spjobs
#spjobs = []

#  Status - new, running, paused or completed
#status = new


#stress_test = False


#stress_test_status = {}

#  Total number of events to generate
#total_events = 0

#  TriggerConfig string for later Athena versions
#trigger_config = DEFAULT

#  Type of the Task
#type = 

#  set the float to this value if the first job runs successfully
#working_float = 0


#=======================================================================
#  default attribute values for MultipleMerger objects
[defaults_MultipleMerger]

#  A list of Merge objects to run
#merger_objects = []


#=======================================================================
#  default attribute values for NG objects
[defaults_NG]

#  Request specific cluster(s)
#CE = 

#  RLS dserver
#RLS = rls://atlasrls.nordugrid.org:39281

#  Reject specific cluster(s)
#RejectCE = 

#  Check availability of DQ2 data on NG before submission
#check_availability = False

#  Files to be cleaned after job
#clean = []

#  Middleware type
#middleware = None

#  Requirements for selecting execution host
#requirements = NGRequirements

#  Options passed to Condor at submission time
#submit_options = []


#=======================================================================
#  default attribute values for NGRequirements objects
[defaults_NGRequirements]

#  Requested cpu time
#cputime = 30

#  Minimum memory
#disk = 500

#  Mininum virtual  memory
#memory = 500

#  Other requirements
#other = []

#  Runtimeenvironment
#runtimeenvironment = []

#  Requested wall time
#walltime = 30


#=======================================================================
#  default attribute values for PBS objects
[defaults_PBS]

#  extra options for Batch
#extraopts = 

#  queue name as defomed in your local Batch installation
#queue = 


#=======================================================================
#  default attribute values for Panda objects
[defaults_Panda]

#  use Athena ROOT Access
#ara = False

#  cloud where jobs are submitted (default:US)
#cloud = US

#  Enable a checker to skip corrupted files
#corCheck = False

#  Send job to a long queue
#long = False

#  If input files are not read from SE, they will be skipped by default. This
#  option disables the functionality
#notSkipMissing = False

#  Require the job to run at a specific site
#site = AUTO


#=======================================================================
#  default attribute values for PandaBuildJob objects
[defaults_PandaBuildJob]

#  Panda Job id
#id = None

#  Panda Job status
#status = None


#=======================================================================
#  default attribute values for Remote objects
[defaults_Remote]

#  Overides any environment variables set in the job
#environment = {}

#  Command line to start ganga on the remote host
#ganga_cmd = 

#  The directory to use for the remote workspace, repository, etc.
#ganga_dir = 

#  The remote host to use
#host = 

#  Sequence of commands to execute before running Ganga on the remote site
#pre_script = ['']

#  specification of the resources to be used (e.g. batch system)
#remote_backend = None

#  The username at the remote host
#username = 


#=======================================================================
#  default attribute values for Root objects
[defaults_Root]

#  List of arguments for the script. Accepted types are numerics and strings
#args = []

#  A File object specifying the script to execute when Root starts
#script = File(name='',subdir='.')

#  Execute 'script' using Python. The PyRoot libraries are added to the
#  PYTHONPATH.
#usepython = False

#  The version of Root to run
#version = 5.18.00


#=======================================================================
#  default attribute values for RootMerger objects
[defaults_RootMerger]

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for RootMergerAANT objects
[defaults_RootMergerAANT]

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for SGE objects
[defaults_SGE]

#  extra options for Batch
#extraopts = 

#  queue name as defomed in your local Batch installation
#queue = 


#=======================================================================
#  default attribute values for SmartMerger objects
[defaults_SmartMerger]

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for TNTJobSplitter objects
[defaults_TNTJobSplitter]

#  Username from authentication.xml
#auth_name = 

#  Password for username from authentication.xml
#auth_pass = 

#  Logfile name
#logfile = TNT.log-24963

#  Run jobs on CEs with local dataset
#match_ce = False

#  Minimum number of events per sub collection
#minevents = 0

#  Database query to execute
#query = 

#  Source collection name
#src_collection_name = 

#  Source collection type
#src_collection_type = RelationalCollection

#  Source connection string
#src_connection_string = 


#=======================================================================
#  default attribute values for Task objects
[defaults_Task]

#  Name of the CE queue where the computation should take place
#CE = 


#allow_change = False

#  exclude CEs
#excluded_CEs = []

#  sites which you want to exclude for this task
#excluded_sites = []

#  How many jobs should be run at the same time
#float = 0

#  Name of the Task
#name = New Analysis Task

#  create a file with name 'Task_'+ taskname+'_report' and write all incidents
#  to it
#report_file = 

#  create a file with name 'Task_'+ taskname+'_report' and write all incidents
#  to it
#report_output = False

#  sites where the job should run
#requirements_sites = []

#  List of spjobs
#spjobs = []

#  Status - new, running, paused or completed
#status = new


#stress_test = False


#stress_test_status = {}

#  Type of the Task
#type = 

#  set the float to this value if the first job runs successfully
#working_float = 0


#=======================================================================
#  default attribute values for TaskList objects
[defaults_TaskList]

#  list of tasks
#data = []

#  true if user already used tasks
#user_called_tasks = False


#=======================================================================
#  default attribute values for TextMerger objects
[defaults_TextMerger]

#  Output should be compressed with gzip.
#compress = False

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for VomsCommand objects
[defaults_VomsCommand]

#  Command for destroying credential
#destroy = voms-proxy-destroy

#  Dictionary of parameter-value pairs to pass to destroy command
#destroy_parameters = {}

#  Command for obtaining information about credential
#info = voms-proxy-info

#  Dictionary mapping from Ganga credential properties to command-line options
#info_parameters = {'vo': '-vo'}

#  Command for creating/initialising credential
#init = voms-proxy-init

#  Dictionary of parameter-value pairs to pass to init command
#init_parameters = {'pipe': '-pwstdin', 'voms': '-voms', 'valid': '-valid'}


