# Ganga configuration file ($Name: Ganga-SVN $). DO NOT remove this line.
#
#=======================================================================
# All  settings are  commented out,  so ganga  will apply  the default
# values automatically. Wherever possible these default values are indicated. 
# To see which configuration settings are used, type at ganga prompt:
#  print config
#
# Support of Atlas and LHCb-specific functionality:
#
# GANGA_CONFIG_PATH environment variable or --config-path option at the command
# line is used to enable LHCb or Atlas extensions. 
# For example:
#    $ export GANGA_CONFIG_PATH=GangaLHCb/LHCb.ini
#    $ ganga
# or:
#    $ ganga --config-path=GangaAtlas/Atlas.ini
#
# In LHCb environment you do not need to worry about it because it is done
# by GangaEnv command automatically.
#


#=======================================================================
#  AMI dataset
[AMIDataset]

#  Maximum number of datasets in a given dataset patterns
#MaxNumOfDatasets = 100

#  Maximum number of files in a given dataset patterns
#MaxNumOfFiles = 3000


#=======================================================================
#  Athena configuration parameters
[Athena]

#  FIXME
#ATLASOutputDatasetLFC = prod-lfc-atlas-local.cern.ch

#  FIXME
#ATLAS_SOFTWARE = /afs/cern.ch/atlas/software/releases/

#  Allowed space tokens/sites for DQ2JobSplitter on NG backend
#AllowedSitesNGDQ2JobSplitter = ['NDGF-T1_DATADISK', 'NDGF-T1_MCDISK', 'NDGF-T1_PRODDISK', 'NDGF-T1_SCRATCHDISK']

#  List of clouds that should be preferentially submitted to when using the
#  anyCloud option
#AnyCloudPreferenceList = []

#  Default value to be used as CMTCONFIG environment setup value (LCG/Batch
#  backend)
#CMTCONFIG = i686-slc5-gcc43-opt

#  Allowed values for CMTCONFIG environment setup value (LCG/Batch backend)
#CMTCONFIG_LIST = ['i686-slc4-gcc34-opt', 'i686-slc5-gcc43-opt', 'x86_64-slc5-gcc43-opt']

#  The path in which the cmtsetup magic function will look up the setup.sh for
#  CMT environment setup
#CMTHOME = /afs/cern.ch/user/g/gangaat/cmthome

#  Size of the dCache read ahead buffer used for dcap input file reading
#DCACHE_RA_BUFFER = 32768

#  Default number of input files per subjob used at a direct access site in
#  Panda in DQ2JobSplitter
#DefaultNumFilesPandaDirectDQ2JobSplitter = 50

#  Enable DQ2_COPY input workflow on LCG backend
#ENABLE_DQ2COPY = False

#  Enable DQ2JobSplitter for SGE backend
#ENABLE_SGE_DQ2JOBSPLITTER = False

#  Enable FILE_STAGER input access mode for SGE backend
#ENABLE_SGE_FILESTAGER = False

#  Athena.exetype=EXE jobs: Maximum size of files to be sent to WNs (default
#  1024*1024B)
#EXE_MAXFILESIZE = 1048576

#  FIXME
#ExcludedSites = 

#  When copying local output, should dir structure be jid.sid (False) or jid/sid
#  (True)
#IndividualSubjobDirsForLocalOutput = False

#  FIXME
#LCGOutputLocation = srm://srm-atlas.cern.ch/castor/cern.ch/grid/atlas/scratch/gangaat/ganga

#  FIXME
#LocalOutputLocation = /castor/cern.ch/atlas/scratch/gangaat/ganga

#  Maximum total sum of filesizes per subjob of DQ2JobSplitter at the NG backend
#  (in MB)
#MaxFileSizeNGDQ2JobSplitter = 14336

#  Maximum total sum of filesizes per subjob of DQ2JobSplitter at the Panda
#  backend (in MB)
#MaxFileSizePandaDQ2JobSplitter = 13336

#  Maximum number of allowed subjobs of DQ2JobSplitter
#MaxFilesPandaDQ2JobSplitter = 5000

#  Number of maximum jobs allowed for job splitting with the AthenaSplitterJob
#  and the LCG backend
#MaxJobsAthenaSplitterJobLCG = 1000

#  Maximum number of allowed subjobs of DQ2JobSplitter
#MaxJobsDQ2JobSplitter = 5000

#  Maximum number of allowed subjobs of DQ2JobSplitter on LCG/Cream with compile
#  option switched on
#MaxJobsDQ2JobSplitterLCGCompile = 500

#  When copying local output, all output is copied to the given output location
#  with no subdirs created
#NoSubDirsAtAllForLocalOutput = False

#  FIXME
#PRODUCTION_ARCHIVE_BASEURL = http://atlas-computing.web.cern.ch/atlas-computing/links/kitsDirectory/Production/kits/

#  Path to the EOS binary for output copying/checking
#PathToEOSBinary = /afs/cern.ch/project/eos/installation/pro/bin/eos.select

#  If True, user areas created in /tmp are removed after prepare has been called
#  (the version in the gangadir shared area will be used instead)
#RemoveTempUserAreaAfterPrepare = False

#  When copying local output, only a single dirs used for output and output
#  filenames are changed with jid.sid
#SingleDirForLocalOutput = False

#  Set to True to dereference symlinks in the sources area. E.g. if src is a
#  symlink, the target will be copied into the sources archive.
#dereferenceSymLinks = False


#=======================================================================
#  AthenaMC configuration options
[AthenaMC]


#=======================================================================
#  Settings for Condor Batch system
[Condor]

#  Query global condor queues, i.e. use '-global' flag
#query_global_queues = True


#=======================================================================
#  global configuration parameters. this is a catch all section.
[Configuration]

#  default batch system
#Batch = LSF

#  Time in seconds before a ganga session (lock file) is treated as a zombie and
#  removed
#DiskIOTimeout = 45

#  runtime warnings issued by the interpreter may be suppresed
#IgnoreRuntimeWarnings = False

#  the search path for the load() function
#LOAD_PATH = 

#  path to runtime plugin packages where custom handlers may be added. Normally
#  you should not worry about it. If an element of the path is just a name (like
#  in the example below) then the plugins will be loaded using current python
#  path. This means that some packages such as GangaTest may be taken from the
#  release area.
#  Examples:
#    RUNTIME_PATH = GangaGUIRUNTIME_PATH = /my/SpecialExtensions:GangaTest
#RUNTIME_PATH = GangaAtlas:GangaPanda:

#  Flag to print out the relevent subsection of release notes for each
#  experiment at start up
#ReleaseNotes = True

#  the search path to scripts directory. When running a script from the system
#  shell (e.g. ganga script) this path is used to search for script
#SCRIPTS_PATH = GangaAtlas/scripts:Ganga/scripts:

#  The SMTP server for notification emails to be sent, default is localhost
#SMTPHost = localhost

#  Port for the Ganga server to listen on
#ServerPort = 434343

#  Timeout in minutes for auto-server shutdown
#ServerTimeout = 60

#  Full path to user script to call periodically. The script will be executed as
#  if called within Ganga by 'execfile'.
#ServerUserScript = 

#  Time in seconds between executions of the user script
#ServerUserScriptWaitTime = 300

#  block of GPI commands executed at startup
#StartupGPI = execfile('/afs/cern.ch/sw/ganga/install/config/ATLAS.py')

#  The type of the interactive shell: IPython (cooler) or Console (limited)
#TextShell = IPython

#  enable usage monitoring through MSG server defined in MSGMS configuration
#UsageMonitoringMSG = True

#  Autogenerate workspace dirs for new jobs
#autoGenerateJobWorkspace = False

#  Ask the user on exit if we should exit, (this is passed along to IPython)
#confirm_exit = 1

#  If set to ask the user is presented with a prompt asking whether Shared
#  directories not associated with a persisted Ganga object should be deleted
#  upon Ganga exit. If set to never, shared directories will not be deleted upon
#  exit, even if they are not associated with a persisted Ganga object. If set
#  to always (the default), then shared directories will always be deleted if
#  not associated with a persisted Ganga object.
#deleteUnusedShareDir = always

#  Location of local job repositories and workspaces. Default is ~/gangadir but
#  in somecases (such as LSF CNAF) this needs to be modified to point to the
#  shared file system directory.
#gangadir = /afs/cern.ch/user/g/gangaat/gangadir

#  The default file extension for the named template system. If a package sets
#  up their own by calling "establishNamedTemplates" from
#  python/Ganga/GPIDev/Lib/Job/NamedJobTemplate.py in their ini file then they
#  can override this without needing the config option
#namedTemplates_ext = tpl

#  Determines if named template system stores templates in pickle file format
#  (True) or in the Ganga streamed object format (False). By default streamed
#  object format which is human readable is used. If a package sets up their own
#  by calling "establishNamedTemplates" from
#  python/Ganga/GPIDev/Lib/Job/NamedJobTemplate.py in their ini file then they
#  can override this without needing the config option
#namedTemplates_pickle = False

#  Type of the repository.
#  Examples:
#    LocalXML
#repositorytype = LocalXML

#  If TRUE (default), calling job.resubmit() will only resubmit FAILED subjobs.
#  Note that the auto_resubmit mechanism will only ever resubmit FAILED subjobs.
#resubmitOnlyFailedSubjobs = True

#  User name. The same person may have different roles (user names) and still
#  use the same gangadir. Unless explicitly set this option defaults to the real
#  user name.
#user = gangaat

#  Type of workspace. Workspace is a place where input and output sandbox of
#  jobs are stored. Currently the only supported type is LocalFilesystem.
#workspacetype = LocalFilesystem


#=======================================================================
#  DQ2 configuration options
[DQ2]

#  Allow that voms nickname is empty for DQ2OutputDataset name creating.
#ALLOW_MISSING_NICKNAME_DQ2OUTPUTDATASET = False

#  Check for duplicate files in DQ2OutputDataset in LCG backend - this could
#  possibly happen by ShallowRetry of glite WMS. A duplicates dataset is created
#CHECK_OUTPUT_DUPLICATES = False

#  If CHECK_OUTPUT_DUPLICATES=True is used, duplicates dataset can be
#  automatically deleted by setting this flag to True.
#DELETE_DUPLICATES_DATASET = False

#  Default backup locations of DQ2OutputDataset output
#DQ2_BACKUP_OUTPUT_LOCATIONS = ['CERN-PROD_SCRATCHDISK', 'FZK-LCG2_SCRATCHDISK', 'IN2P3-CC_SCRATCHDISK', 'TRIUMF-LCG2_SCRATCHDISK', 'IFAE_SCRATCHDISK', 'NIKHEF-ELPROD_SCRATCHDISK']

#  Sets the DQ2 local site id
#DQ2_LOCAL_SITE_ID = CERN-PROD_DATADISK

#  Allowed space tokens names of DQ2OutputDataset output
#DQ2_OUTPUT_SPACE_TOKENS = ['ATLASSCRATCHDISK', 'ATLASLOCALGROUPDISK', 'T2ATLASSCRATCHDISK', 'T2ATLASLOCALGROUPDISK']

#  FIXME
#DQ2_URL_SERVER = http://atlddmcat.cern.ch/dq2/

#  FIXME
#DQ2_URL_SERVER_SSL = https://atlddmcat.cern.ch:443/dq2/

#  Number of simultaneous DQ2 downloads when calling "retrieve"
#NumberOfDQ2DownloadThreads = 5

#  Maximum lifetime of a DQ2OutputDataset.
#OUTPUTDATASET_LIFETIME = 

#  Maximum characters of a DQ2OutputDataset.
#OUTPUTDATASET_NAMELENGTH = 131

#  Maximum characters of a filename in DQ2OutputDataset.
#OUTPUTFILE_NAMELENGTH = 150

#  Use automatic best choice of input dataset access mode provided by
#  AtlasLCGRequirements.
#USE_ACCESS_INFO = False

#  Use voms nicknames for DQ2OutputDataset.
#USE_NICKNAME_DQ2OUTPUTDATASET = True

#  Allow DQ2 subscription to aggregate DQ2OutputDataset output on a storage
#  element instead of using remote lcg-cr
#USE_STAGEOUT_SUBSCRIPTION = False

#  Script to setup DQ2Clients software
#setupScript = /cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/user/gangaDDMSetup.sh

#  user tag for a given data taking period
#usertag = user


#=======================================================================
#  Settings for Dashboard Messaging Service.
[DashboardMS]

#  The MSG destination (topic or queue) for job meta messages.
#destination_job_meta = /topic/dashboard.atlas.jobMeta

#  The MSG destination (topic or queue) for job processing attributes messages.
#destination_job_processing_attributes = /topic/dashboard.atlas.jobProcessingAttributes

#  The MSG destination (topic or queue) for job status messages.
#destination_job_status = /topic/dashboard.atlas.jobStatus

#  The MSG destination (topic or queue) for task meta messages.
#destination_task_meta = /topic/dashboard.atlas.taskMeta


#password = analysis

#  The MSG server port.
#port = 61113

#  The MSG server name.
#server = dashb-mb.cern.ch

#  The type of task. e.g. analysis, production, hammercloud,...
#task_type = analysis


#user = ganga-atlas


#=======================================================================
#  control the printing style of the different registries
#  ("jobs","box","tasks"...)
[Display]

#  list of job attributes to be printed in separate columns
#box_columns = ('id', 'type', 'name', 'application')

#  optional converter functions
#box_columns_functions = {'application': 'lambda obj: obj.application._name'}

#  with exception of columns mentioned here, hide all values which evaluate to
#  logical false (so 0,"",[],...)
#box_columns_show_empty = ['id']

#  width of each column
#box_columns_width = {'application': 15, 'type': 20, 'id': 5, 'name': 40}

#  colour print of the docstrings and examples
#config_docstring_colour = fg.green

#  colour print of the names of configuration sections and options
#config_name_colour = fx.bold

#  colour print of the configuration values
#config_value_colour = fx.bold

#  list of job attributes to be printed in separate columns
#jobs_columns = ('fqid', 'status', 'name', 'subjobs', 'application', 'backend', 'backend.actualCE', 'comment')

#  optional converter functions
#jobs_columns_functions = {'comment': 'lambda j: j.comment', 'subjobs': 'lambda j: len(j.subjobs)', 'backend': 'lambda j:j.backend._name', 'application': 'lambda j: j.application._name'}

#  with exception of columns mentioned here, hide all values which evaluate to
#  logical false (so 0,"",[],...)
#jobs_columns_show_empty = ['fqid']

#  width of each column
#jobs_columns_width = {'fqid': 8, 'status': 10, 'application': 15, 'name': 10, 'subjobs': 8, 'backend.actualCE': 45, 'comment': 30, 'backend': 15}

#  colours for jobs status
#jobs_status_colours = {'new': 'fx.normal', 'running': 'fg.green', 'completed': 'fg.blue', 'submitted': 'fg.orange', 'failed': 'fg.red'}

#  list of job attributes to be printed in separate columns
#tasks_columns = ('id', 'Type', 'Name', 'State', 'Comment', 'Jobs', '\x1b[44;97mdone\x1b[0;0m')

#  optional converter functions
#tasks_columns_functions = {'State ': 'lambda task : task.status', 'Jobs': 'lambda task : task.n_all()', 'Name': 'lambda t : t.name', '\x1b[44;97mdone\x1b[0;0m': "lambda task : task.n_status('completed')", 'Type': 'lambda task : task._name', 'Comment ': 'lambda task : task.comment'}

#  with exception of columns mentioned here, hide all values which evaluate to
#  logical false (so 0,"",[],...)
#tasks_columns_show_empty = ['id', 'Jobs', '\x1b[44;97mdone\x1b[0;0m']

#  width of each column
#tasks_columns_width = {'Comment': 30, 'Jobs': 33, 'Name': 22, 'State': 9, '\x1b[44;97mdone\x1b[0;0m': 5, 'Type': 13, 'id': 5}

#  change this to False if you do not want to see the help screen if you first
#  type "tasks" in a session
#tasks_show_help = True


#=======================================================================
#  Settings for the Feedback plugin. Cannot be changed ruding the interactive
#  Ganga session.
[Feedback]

#  The server to connect to
#uploadServer = http://gangamon.cern.ch/django/errorreports


#=======================================================================
#  Default associations between file types and file-viewing commands. The name
#  identifies the extension and the value the commans. New extensions can be
#  added. A single & after the command indicates that the process will be
#  started in the background. A && after the command indicates that a new
#  terminal will be opened and the command executed in that terminal.
[File_Associations]

#  Default command to use if there is no association with the file type
#fallback_command = less

#  Command for viewing html files.
#htm = firefox &

#  Command for viewing html files.
#html = firefox &

#  Command for listing the content of a directory
#listing_command = ls -ltr

#  Command for opening a new terminal (xterm, gnome-terminal, ...
#newterm_command = xterm

#  Option to give to a new terminal to tell it to execute a command.
#newterm_exeopt = -e

#  Command for opening ROOT files.
#root = root.exe &&

#  Command for opening tar files.
#tar = file-roller &

#  Command for opening tar files.
#tgz = file-roller &


#=======================================================================
#  Customization of GPI component object assignment for each category there may
#  be multiple filters registered, the one used being defined  in the
#  configuration file in [GPIComponentFilters] e.g:
#  {'datasets':{'lhcbdatasets':lhcbFilter, 'testdatasets':testFilter}...}
[GPIComponentFilters]


#files = string_file_shortcut_file


#gangafiles = string_file_shortcut


#postprocessor = postprocessor_filter


#shareddirs = string_sharedfile_shortcut


#=======================================================================
#  Customization of GPI behaviour. These options may affect the semantics of the
#  Ganga GPI interface (what may result in a different behaviour of scripts and
#  commands).
[GPI_Semantics]

#  Keep on submitting as many subjobs as possible. Option to j.submit(), see Job
#  class for details
#job_submit_keep_going = False

#  Do not revert job to new status even if submission failed. Option to
#  j.submit(), see Job class for details
#job_submit_keep_on_fail = False


#=======================================================================
#  Jedi backend configuration parameters
[Jedi]

#  assignedPriorityBuild
#assignedPriorityBuild = 2000

#  assignedPriorityRun
#assignedPriorityRun = 1000

#  Configuration string for chirp data output, e.g.
#  "chirp^etpgrid01.garching.physik.uni-muenchen.de^/tanyasandoval^-d chirp"
#chirpconfig = 

#  Configuration string for the chirp server, e.g. "voatlas92.cern.ch". If this
#  variable is set config.Panda.chirpconfig is filled and chirp output will be
#  enabled.
#chirpserver = 

#  enableDownloadLogs
#enableDownloadLogs = False

#  processingType
#processingType = ganga

#  prodSourceLabelBuild
#prodSourceLabelBuild = panda

#  prodSourceLabelRun
#prodSourceLabelRun = user

#  Maximum number of subjobs to send to the Panda server
#serverMaxJobs = 5000

#  Expert only.
#siteType = analysis

#  Trust the Information System
#trustIS = True


#=======================================================================
#  LCG/gLite/EGEE configuration parameters
[LCG]

#  sets allowed computing elements by a regular expression
#AllowedCEs = 

#  Config file for ARC submission. Use to specify CEs, etc. Default is blank
#  which will mean no config file is specified and the default
#  (~/arc/client.conf) is used
#ArcConfigFile = 

#  File to store ARC job info in when submitting and monitoring, i.e. argument
#  to "-j" option in arcsub. Ganga default is different to ARC default
#  (~/.arc/jobs.xml) to keep them separate.
#ArcJobListFile = ~/.arc/gangajobs.xml

#  Time in seconds to wait after submission before starting to monitor ARC jobs
#  to ensure they are in the system
#ArcWaitTimeBeforeStartingMonitoring = 240

#  sets the size limitation of the input sandbox, oversized input sandbox will
#  be pre-uploaded to the storage element specified by 'DefaultSE' in the area
#  specified by 'DefaultSRMToken'
#BoundSandboxLimit = 10485760

#  sets the generic LCG-UI configuration script for the GLITE workload
#  management system
#Config = /afs/cern.ch/sw/ganga/install/config/atlas_glite_wms.conf.cern.emi

#  sets the baseURI for getting the input sandboxes for the job
#CreamInputSandboxBaseURI = 

#  sets the baseURI for putting the output sandboxes for the job
#CreamOutputSandboxBaseURI = 

#  sets the file catalogue server
#DefaultLFC = prod-lfc-atlas-local.cern.ch

#  sets the default storage element
#DefaultSE = srm-atlas.cern.ch

#  sets the space token for storing temporary files (e.g. oversized input
#  sandbox)
#DefaultSRMToken =

#  sets excluded computing elements by a regular expression
#ExcludedCEs = 


#GLITE_ALLOWED_WMS_LIST = []

#  Enables/disables the support of the GLITE middleware
#GLITE_ENABLE = True

#  sets the LCG-UI environment setup script for the GLITE middleware
#GLITE_SETUP = /afs/cern.ch/sw/ganga/install/config/grid_env_atlas.sh

#  sets the WMProxy service to be contacted
#GLITE_WMS_WMPROXY_ENDPOINT = 

#  sets the maximum number of nodes (i.e. subjobs) in a gLite bulk job
#GliteBulkJobSize = 50

#  sets to True will load script-based glite-wms-* commands forcely with current
#  python, a trick for 32/64 bit compatibility issues.
#IgnoreGliteScriptHeader = False

#  sets the way the job's stdout/err are being handled.
#JobLogHandler = WMS

#  sets to True will do resource matching before submitting jobs, jobs without
#  any matched resources will fail the submission
#MatchBeforeSubmit = True

#  sets the myproxy server
#MyProxyServer = myproxy.cern.ch

#  sets the number of concurrent threads for downloading job's output sandbox
#  from gLite WMS
#OutputDownloaderThread = 10

#  sets the ranking rule for picking up computing element
#Rank = 

#  sets the replica catalogue server
#ReplicaCatalog = 

#  sets the full qualified class name for other specific LCG job requirements
#Requirements = GangaAtlas.Lib.AtlasLCGRequirements.AtlasLCGRequirements

#  sets maximum number of job retry
#RetryCount = 0

#  sets the full qualified class name for handling the oversized input sandbox
#SandboxCache = GangaAtlas.Lib.ATLASDataset.DQ2SandboxCache

#  sets the transfer timeout of the oversized input sandbox
#SandboxTransferTimeout = 60

#  sets maximum number of job shallow retry
#ShallowRetryCount = 10

#  sets the gLite job status polling timeout in seconds
#StatusPollingTimeout = 300

#  sets the storage index
#StorageIndex = 

#  sets the number of concurrent threads for job submission to gLite WMS
#SubmissionThread = 10

#  sets the gLite job submission timeout in seconds
#SubmissionTimeout = 300

#  sets the name of the grid virtual organisation
#VirtualOrganisation = atlas


#=======================================================================
#  internal LSF command line interface
[LSF]

#  Heartbeat frequency config variable
#heartbeat_frequency = 30

#  Name of environment with ID of the job
#jobid_name = LSB_BATCH_JID

#  String contains option name for name of job in batch system
#jobnameopt = J

#  String pattern for replay from the kill command
#kill_res_pattern = (^Job <\d+> is being terminated)|(Job <\d+>: Job has already finished)|(Job <\d+>: No matching job found)

#  String used to kill job
#kill_str = bkill %s

#  String contains commands executing before submiting job to queue
#postexecute = 
# def filefilter(fn):
#   # FILTER OUT Batch INTERNAL INPUT/OUTPUT FILES: 
#   # 10 digits . any number of digits . err or out
#   import re
#   internals = re.compile(r'\d{10}\.\d+.(out|err)')
#   return internals.match(fn) or fn == '.Batch.start'

#  String contains commands executing before submiting job to queue
#preexecute = 


#  Name of environment with queue name of the job
#queue_name = LSB_QUEUE

#  Shared PYTHON
#shared_python_executable = False

#  String pattern for defining the stderr
#stderrConfig = -e %s/stderr

#  String pattern for defining the stdout
#stdoutConfig = -o %s/stdout

#  String pattern for replay from the submit command
#submit_res_pattern = ^Job <(?P<id>\d*)> is submitted to .*queue <(?P<queue>\S*)>

#  String used to submit job to queue
#submit_str = cd %s; bsub %s %s %s %s

#  Timeout in seconds after which a job is declared killed if it has not touched
#  its heartbeat file. Heartbeat is touched every 30s so do not set this below
#  120 or so.
#timeout = 600


#=======================================================================
#  parameters of the local backend (jobs in the background on localhost)
[Local]

#  The location where the workdir will be created. If None it defaults to the
#  value of $TMPDIR
#location = None

#  remove automatically the local working directory when the job completed
#remove_workdir = True


#=======================================================================
#  control the messages printed by Ganga The settings are applied hierarchically
#  to the loggers. Ganga is the name of the top-level logger which applies by
#  default to all Ganga.* packages unless overriden in sub-packages. You may
#  define new loggers in this section. The log level may be one of: CRITICAL
#  ERROR WARNING INFO DEBUG
[Logging]

#  top-level logger
#Ganga = INFO

#  logger of Ganga.GPIDev.* packages
#Ganga.GPIDev = INFO

#  FIXME
#Ganga.Runtime.bootstrap = INFO

#  logger of the Ganga logging package itself (use with care!)
#Ganga.Utility.logging = WARNING


#GangaAtlas = INFO


#GangaPanda = INFO

#  enable ASCII colour formatting of messages e.g. errors in red
#_colour = True

#  custom formatting string for Ganga logging  e.g. '%(name)-35s:
#  %(levelname)-8s %(message)s'
#_customFormat = 

#  format of logging messages: TERSE,NORMAL,VERBOSE,DEBUG
#_format = NORMAL

#  if True then the cache used for interactive sessions, False disables caching
#_interactive_cache = True

#  location of the logfile
#_logfile = ~/.ganga.log

#  the size of the logfile (in bytes), the rotating log will never exceed this
#  file size
#_logfile_size = 100000

#  logger for stomp.py external package
#stomp.py = CRITICAL


#=======================================================================
#  Settings for the MSGMS monitoring plugin. Cannot be changed ruding the
#  interactive Ganga session.
[MSGMS]


#job_submission_message_destination = /queue/ganga.jobsubmission


#message_destination = /queue/ganga.status


#password = analysis

#  The port to connect to
#port = 61113

#  The server to connect to
#server = dashb-mb.cern.ch


#usage_message_destination = /queue/ganga.usage


#username = ganga


#=======================================================================
#  parameters for mergers
[Mergers]

#  Dictionary of file associations
#associate = {'text': 'TextMerger', 'txt': 'TextMerger', 'root': 'RootMerger', 'log': 'TextMerger'}

#  location of the merger's outputdir
#merge_output_dir = /afs/cern.ch/user/g/gangaat/gangadir/merge_results

#  Standard (default) merger
#std_merge = TextMerger


#=======================================================================
#  External monitoring systems are used to follow the submission and execution
#  of jobs. Each entry in this section defines a monitoring plugin used for a
#  particular combination of application and backend. Asterisks may be used to
#  specify any application or any backend. The configuration entry syntax:
#  ApplicationName/BackendName = dot.path.to.monitoring.plugin.class.  Example:
#  DummyMS plugin will be used to track executables run on all backends:
#  Executable/* = Ganga.Lib.MonitoringServices.DummyMS.DummyMS
[MonitoringServices]


#AMAAthena/CREAM = Ganga.Lib.MonitoringServices.ARDADashboard.LCG.ARDADashboardLCGAthena.ARDADashboardLCGAthena,Ganga.Lib.MonitoringServices.MSGMS.MSGMS,Ganga.Lib.MonitoringServices.Dashboard.LCGAthenaMS.LCGAthenaMS


#AMAAthena/LCG = Ganga.Lib.MonitoringServices.ARDADashboard.LCG.ARDADashboardLCGAthena.ARDADashboardLCGAthena,Ganga.Lib.MonitoringServices.MSGMS.MSGMS,Ganga.Lib.MonitoringServices.Dashboard.LCGAthenaMS.LCGAthenaMS


#AMAAthenaTask/CREAM = Ganga.Lib.MonitoringServices.ARDADashboard.LCG.ARDADashboardLCGAthena.ARDADashboardLCGAthena,Ganga.Lib.MonitoringServices.MSGMS.MSGMS,Ganga.Lib.MonitoringServices.Dashboard.LCGAthenaMS.LCGAthenaMS


#AMAAthenaTask/LCG = Ganga.Lib.MonitoringServices.ARDADashboard.LCG.ARDADashboardLCGAthena.ARDADashboardLCGAthena,Ganga.Lib.MonitoringServices.MSGMS.MSGMS,Ganga.Lib.MonitoringServices.Dashboard.LCGAthenaMS.LCGAthenaMS

#  FIXME
#Athena = None

#  sets job monitoring service for Athena/CREAM jobs
#Athena/CREAM = Ganga.Lib.MonitoringServices.ARDADashboard.LCG.ARDADashboardLCGAthena.ARDADashboardLCGAthena,Ganga.Lib.MonitoringServices.MSGMS.MSGMS,Ganga.Lib.MonitoringServices.Dashboard.LCGAthenaMS.LCGAthenaMS

#  sets job monitoring service for Athena/LCG jobs
#Athena/LCG = Ganga.Lib.MonitoringServices.ARDADashboard.LCG.ARDADashboardLCGAthena.ARDADashboardLCGAthena,Ganga.Lib.MonitoringServices.MSGMS.MSGMS,Ganga.Lib.MonitoringServices.Dashboard.LCGAthenaMS.LCGAthenaMS

#  FIXME
#AthenaMC = None

#  FIXME
#AthenaMC/LCG = Ganga.Lib.MonitoringServices.ARDADashboard.LCG.ARDADashboardLCGAthena.ARDADashboardLCGAthena,Ganga.Lib.MonitoringServices.MSGMS.MSGMS

#  FIXME
#AthenaMC/Local = None


#AthenaTask/CREAM = Ganga.Lib.MonitoringServices.ARDADashboard.LCG.ARDADashboardLCGAthena.ARDADashboardLCGAthena,Ganga.Lib.MonitoringServices.MSGMS.MSGMS,Ganga.Lib.MonitoringServices.Dashboard.LCGAthenaMS.LCGAthenaMS


#AthenaTask/LCG = Ganga.Lib.MonitoringServices.ARDADashboard.LCG.ARDADashboardLCGAthena.ARDADashboardLCGAthena,Ganga.Lib.MonitoringServices.MSGMS.MSGMS,Ganga.Lib.MonitoringServices.Dashboard.LCGAthenaMS.LCGAthenaMS


#=======================================================================
#  configuration section for postprocessing the output
[Output]

#  List of outputfile types that will be auto removed when job is removed if
#  AutoRemoveFilesWithJob is True
#AutoRemoveFileTypes = ['DiracFile']

#  if True, each outputfile of type in list AutoRemoveFileTypes will be removed
#  when the job is
#AutoRemoveFilesWithJob = False

#  fileExtensions:list of output files that will be written to
#  DIRAC,backendPostprocess:defines where postprocessing should be done
#  (WN/client) on different backends,uploadOptions:config values needed for the
#  actual DIRAC upload
#DiracFile = {'defaultSite': {'download': 'CERN-USER', 'upload': 'CERN-USER'}, 'fileExtensions': ['*.dst'], 'uploadOptions': {}, 'backendPostprocess': {'LSF': 'WN', 'Dirac': 'WN', 'LCG': 'WN', 'ARC': 'WN', 'Interactive': 'WN', 'Localhost': 'WN', 'CREAM': 'WN'}}

#  if True, a job will be marked failed if output is asked for but not found.
#FailJobIfNoOutputMatched = True

#  if True, writing to the job inputsandbox field will be forbidden
#ForbidLegacyInput = False

#  if True, writing to the job outputdata and outputsandbox fields will be
#  forbidden
#ForbidLegacyOutput = False

#  fileExtensions:list of output files that will be written to
#  GoogleDrive,backendPostprocess:defines where postprocessing should be done
#  (WN/client) on different backends,uploadOptions:config values needed for the
#  actual Google upload
#GoogleFile = {'fileExtensions': [], 'uploadOptions': {}, 'backendPostprocess': {'LSF': 'client', 'Dirac': 'client', 'LCG': 'client', 'ARC': 'client', 'Interactive': 'client', 'Localhost': 'client', 'CREAM': 'client'}}

#  fileExtensions:list of output files that will be written to LCG
#  SE,backendPostprocess:defines where postprocessing should be done (WN/client)
#  on different backends,uploadOptions:config values needed for the actual LCG
#  upload
#LCGSEFile = {'fileExtensions': ['*.root', '*.asd'], 'uploadOptions': {'dest_SRM': 'srm-public.cern.ch', 'LFC_HOST': 'lfc-dteam.cern.ch'}, 'backendPostprocess': {'LSF': 'client', 'LCG': 'WN', 'ARC': 'WN', 'CREAM': 'WN', 'Localhost': 'WN', 'Interactive': 'WN'}}

#  fileExtensions:list of output files that will be written to Mass
#  Storage,backendPostprocess:defines where postprocessing should be done
#  (WN/client) on different backends,uploadOptions:config values needed for the
#  actual EOS upload
#MassStorageFile = {'defaultProtocol': 'root://eosatlas.cern.ch', 'fileExtensions': [''], 'uploadOptions': {'path': '/eos/atlas/user/g/gangaat/ganga', 'cp_cmd': '/afs/cern.ch/project/eos/installation/atlas/bin/eos.select cp', 'ls_cmd': '/afs/cern.ch/project/eos/installation/atlas/bin/eos.select ls', 'mkdir_cmd': '/afs/cern.ch/project/eos/installation/atlas/bin/eos.select mkdir'}, 'backendPostprocess': {'LSF': 'WN', 'Dirac': 'client', 'LCG': 'client', 'ARC': 'client', 'CREAM': 'client', 'Localhost': 'WN', 'Interactive': 'client'}}

#  name of the file that will contain the locations of the uploaded from the WN
#  files
#PostProcessLocationsFileName = __postprocesslocations__


#=======================================================================
#  internal PBS command line interface
[PBS]

#  Heartbeat frequency config variable
#heartbeat_frequency = 30

#  Name of environment with ID of the job
#jobid_name = PBS_JOBID

#  String contains option name for name of job in batch system
#jobnameopt = N

#  String pattern for replay from the kill command
#kill_res_pattern = (^$)|(qdel: Unknown Job Id)

#  String used to kill job
#kill_str = qdel %s

#  String contains commands executing before submiting job to queue
#postexecute = 
# env = os.environ
# jobnumid = env["PBS_JOBID"]
# os.chdir("/tmp/")
# os.system("rm -rf /tmp/%s/" %jobnumid) 

#  String contains commands executing before submiting job to queue
#preexecute = 
# env = os.environ
# jobnumid = env["PBS_JOBID"]
# os.system("mkdir /tmp/%s/" %jobnumid)
# os.chdir("/tmp/%s/" %jobnumid)
# os.environ["PATH"]+=":."

#  Name of environment with queue name of the job
#queue_name = PBS_QUEUE

#  Shared PYTHON
#shared_python_executable = False

#  String pattern for defining the stderr
#stderrConfig = -e %s/stderr

#  String pattern for defining the stdout
#stdoutConfig = -o %s/stdout

#  String pattern for replay from the submit command
#submit_res_pattern = ^(?P<id>\d*)\.pbs\s*

#  String used to submit job to queue
#submit_str = cd %s; qsub %s %s %s %s

#  Timeout in seconds after which a job is declared killed if it has not touched
#  its heartbeat file. Heartbeat is touched every 30s so do not set this below
#  120 or so.
#timeout = 600


#=======================================================================
#  Panda backend configuration parameters
[Panda]

#  Deprecated. Please use the Jedi backend
#AllowDirectSubmission = False

#  assignedPriorityBuild
#assignedPriorityBuild = 2000

#  assignedPriorityRun
#assignedPriorityRun = 1000

#  Expert only.
#baseURL = 

#  Expert only.
#baseURLSSL = 

#  Configuration string for chirp data output, e.g.
#  "chirp^etpgrid01.garching.physik.uni-muenchen.de^/tanyasandoval^-d chirp"
#chirpconfig = 

#  Configuration string for the chirp server, e.g. "voatlas92.cern.ch". If this
#  variable is set config.Panda.chirpconfig is filled and chirp output will be
#  enabled.
#chirpserver = 

#  enableDownloadLogs
#enableDownloadLogs = False

#  processingType
#processingType = ganga

#  prodSourceLabelBuild
#prodSourceLabelBuild = panda

#  prodSourceLabelRun
#prodSourceLabelRun = user

#  Maximum number of subjobs to send to the Panda server
#serverMaxJobs = 5000

#  Expert only.
#siteType = analysis

#  specialHandling - Expert only.
#specialHandling = 

#  Trust the Information System
#trustIS = True


#=======================================================================
#  General control of plugin mechanism. Set the default plugin in a given
#  category. For example: default_applications = DaVinci default_backends = LCG
[Plugins]


#=======================================================================
#  background job status monitoring and output retrieval
[PollThread]

#  Poll rate for Condor backend.
#Condor = 30

#  Poll rate for Dirac backend.
#Dirac = 50

#  disk space checking callback. This function should return False when there is
#  no disk space available, True otherwise
#DiskSpaceChecker = 

#  Poll rate for LCG backend.
#LCG = 180

#  Poll rate for LSF backend.
#LSF = 20

#  Poll rate for Local backend.
#Local = 10

#  Maximum fraction of failed jobs before stopping automatic resubmission
#MaxFracForResubmit = 0.25

#  Maximum number of automatic job resubmits to do before giving
#MaxNumResubmits = 5

#  Poll rate for PBS backend.
#PBS = 20

#  Poll rate for Panda backend.
#Panda = 300

#  enable monitoring automatically at startup, in script mode monitoring is
#  disabled by default, in interactive mode it is enabled
#autostart = False

#  internal supervising thread
#base_poll_rate = 2

#  The frequency in seconds for credentials checker
#creds_poll_rate = 30

#  Default rate for polling job status in the thread pool. This is the default
#  value for all backends.
#default_backend_poll_rate = 30

#  The frequency in seconds for free disk checker
#diskspace_poll_rate = 30

#  User will get the FIRST prompt after N seconds, as specified by this
#  parameter. This parameter also defines the time that Ganga will wait before
#  shutting down, if there are only non-critical threads alive, in both
#  interactive and batch mode.
#forced_shutdown_first_prompt_time = 5

#  If there are remaining background activities at exit such as monitoring,
#  output download Ganga will attempt to wait for the activities to complete.
#  You may select if a user is prompted to answer if he wants to force shutdown
#  ("interactive") or if the system waits on a timeout without questions
#  ("timeout"). The default is "session_type" which will do interactive shutdown
#  for CLI and timeout for scripts.
#forced_shutdown_policy = batch

#  User will get the prompt every N seconds, as specified by this parameter.
#forced_shutdown_prompt_time = 10

#  Timeout in seconds for forced Ganga shutdown in batch mode.
#forced_shutdown_timeout = 60

#  Poll rate for gLite backend.
#gLite = 30

#  OBSOLETE: this option has no effect anymore
#max_shutdown_retries = 5

#  if 0 then log only once the errors for a given backend and do not repeat them
#  anymore
#repeat_messages = False

#  Size of the thread pool. Each threads monitors a specific backaend at a given
#  time. Minimum value is one, preferably set to the number_of_backends + 1
#update_thread_pool_size = 5


#=======================================================================
#  Parameters for preparable applications
[Preparable]

#  Unprepare a prepared application when it is copied
#unprepare_on_copy = False


#=======================================================================
#  configuration section for the queues
[Queues]

#  default number of worker threads in the queues system
#NumWorkerThreads = 3

#  default timeout for queue generated processes
#Timeout = None


#=======================================================================
#  Options for Root backend
[ROOT]

#  Architecture of ROOT
#arch = i686-slc5-gcc43-opt

#  Location of ROOT
#location = /afs/cern.ch/sw/lcg/app/releases/ROOT

#  Set to a specific ROOT version. Will override other options.
#path = $$ROOTSYS$$

#  Location of the python used for execution of PyROOT script
#pythonhome = /afs/cern.ch/sw/lcg/external/Python/${pythonversion}/i686-slc5-gcc43-opt/

#  Version number of python used for execution python ROOT script
#pythonversion = 2.7.9.p1

#  Version of ROOT
#version = 5.22.00j


#=======================================================================
#  internal SGE command line interface
[SGE]

#  Heartbeat frequency config variable
#heartbeat_frequency = 30

#  Name of environment with ID of the job
#jobid_name = JOB_ID

#  String contains option name for name of job in batch system
#jobnameopt = N

#  String pattern for replay from the kill command
#kill_res_pattern = (has registered the job +\d+ +for deletion)|(denied: job +"\d+" +does not exist)

#  String used to kill job
#kill_str = qdel %s

#  String contains commands executing before submiting job to queue
#postexecute = 

#  String contains commands executing before submiting job to queue
#preexecute = os.chdir(os.environ["TMPDIR"])
# os.environ["PATH"]+=":."

#  Name of environment with queue name of the job
#queue_name = QUEUE

#  Shared PYTHON
#shared_python_executable = False

#  String pattern for defining the stderr
#stderrConfig = -e %s/stderr

#  String pattern for defining the stdout
#stdoutConfig = -o %s/stdout

#  String pattern for replay from the submit command
#submit_res_pattern = Your job (?P<id>\d+) (.+)

#  String used to submit job to queue
#submit_str = cd %s; qsub -cwd -V %s %s %s %s

#  Timeout in seconds after which a job is declared killed if it has not touched
#  its heartbeat file. Heartbeat is touched every 30s so do not set this below
#  120 or so.
#timeout = 600


#=======================================================================
#  configuration parameters for internal Shell utility.
[Shell]


#=======================================================================
#  Tasks configuration options
[Tasks]

#  Monitor tasks even if the monitoring loop isn't enabled
#ForceTaskMonitoring = False

#  Frequency of Task Monitoring loop in seconds
#TaskLoopFrequency = 60.0

#  order of preferred backends (LCG, Panda, NG) for AnaTask analysis
#backendPreference = ['LCG', 'Panda', 'NG']

#  list of preferred clouds to choose for AnaTask analysis
#cloudPreference = []

#  OBSOLETE
#merged_files_per_job = 1

#  OBSOLETE
#recon_files_per_job = 10


#=======================================================================
#  IPython shell configuration See IPython manual for more details:
#  http://ipython.scipy.org/doc/manual
[TextShell_IPython]

#  FIXME
#args = ['-colors','LightBG', '-noautocall']


#=======================================================================
#  default attribute values for AMIDataset objects
[defaults_AMIDataset]

#  Accessprotocol to use on worker node, e.g. Xrootd
#accessprotocol = 


#amiStatus = VALID

#  Check md5sum of input files on storage elemenet - very time consuming !
#check_md5sum = False

#  md5sum or adler checksums of input files
#checksums = []

#  Dataset Name(s)
#dataset = []

#  Data type: DATA, MC or MuonCalibStream
#datatype = 


#entity = dataset

#  Logical File Names to exclude from processing
#exclude_names = []

#  Logical file name pattern to exclude from processing
#exclude_pattern = []

#  Use DQ2_COPY automatically if DQ2_LOCAL fails
#failover = False

#  GoodRunList XML file to search on
#goodRunListXML = None

#  GUID of Logical File Names
#guids = []


#logicalDatasetName = 

#  Match complete and incomplete sources of dataset to CE during job submission
#match_ce_all = False

#  Metadata
#metadata = {}

#  Number of minimum files at incomplete dataset location
#min_num_files = 0

#  Logical File Names to use for processing
#names = []

#  Logical file name pattern to use for processing
#names_pattern = []

#  Number of files.
#number_of_files = 0


#processingStep = Atlas_Production


#project = Atlas_Production

#  Dataset provenance chain
#provenance = []

#  scopes of the input files for RUCIO testing
#scopes = []

#  Sizes of input files
#sizes = []

#  Provide the collection ref if not in primary JOs (useful for TRF usage): AOD,
#  ESD, RAW
#tag_coll_ref = 

#  Input TAG/ELSSI files to run over. tag_info structure will get filled on
#  submission
#tag_files = []

#  TAG information used to split the job
#tag_info = {}

#  Tag Dataset Name
#tagdataset = []

#  Dataset access on worker node: DQ2_LOCAL (default), DQ2_COPY, LFC
#type = 

#  Use AOD to ESD Backnavigation
#use_aodesd_backnav = False

#  Use CVMFS to access TAG files
#use_cvmfs_tag = False


#=======================================================================
#  default attribute values for ARC objects
[defaults_ARC]

#  ARC CE endpoint
#CE = 

#  Job type: Normal, MPICH
#jobtype = Normal

#  Requirements for the resource selection
#requirements = None

#  Interface for handling oversized input sandbox
#sandboxcache = None

#  Use verbose options for ARC commands
#verbose = False


#=======================================================================
#  default attribute values for ATLASCastorDataset objects
[defaults_ATLASCastorDataset]

#  The name of the selected dataset
#dataset = 

#  A directory on castor that contains the datasets in directories
#location = 

#  The selected file names
#names = []

#  A regexp filter to select the correct files
#pattern = 


#=======================================================================
#  default attribute values for ATLASDataset objects
[defaults_ATLASDataset]

#  LFC Catalog address
#lfc = 

#  List of input file lfns
#lfn = []


#=======================================================================
#  default attribute values for ATLASLocalDataset objects
[defaults_ATLASLocalDataset]

#  Try to add these files to the PoolFileCatalog
#create_poolfilecatalog = False

#  List of input files with full path
#names = []

#  Use pool_insertFileToCatalog per single file if bulk insert fails
#use_poolfilecatalog_failover = False


#=======================================================================
#  default attribute values for ATLASOutputDataset objects
[defaults_ATLASOutputDataset]

#  Local output path location
#local_location = 

#  SE output path location
#location = 

#  Output files to be returned via SE
#outputdata = []


#=======================================================================
#  default attribute values for ATLASTier3Dataset objects
[defaults_ATLASTier3Dataset]

#  List of input file Physical File Names
#names = []

#  A text file containing a newline-separated list of Physical File Names
#pfnListFile = None


#=======================================================================
#  default attribute values for ATLASTier3Splitter objects
[defaults_ATLASTier3Splitter]

#  Number of files per subjob
#numfiles = 0

#  Number of subjobs
#numjobs = 0


#=======================================================================
#  default attribute values for AfsCommand objects
[defaults_AfsCommand]

#  Command for destroying credential
#destroy = unlog

#  Dictionary of parameter-value pairs to pass to destroy command
#destroy_parameters = {'cell': '-cell'}

#  Command for obtaining information about credential
#info = tokens

#  Dictionary mapping from Ganga credential properties to command-line options
#info_parameters = {}

#  Command for creating/initialising credential
#init = kinit

#  Dictionary of parameter-value pairs to pass to init command
#init_parameters = {'pipe': '-pipe', 'username': '-principal', 'valid': '-l', 'cell': '-cell'}


#=======================================================================
#  default attribute values for AfsToken objects
[defaults_AfsToken]

#  AFS cell with which token is used [empty string implies local cell]
#cell = 

#  Set of commands to be used for credential-related operations
#command = ICommandSet

#  Number of password attempts allowed when creating credential
#maxTry = 1

#  Default minimum validity
#minValidity = 00:15

#  AFS username with which token is used [defaults to login id]
#username = 

#  Default credential validity at creation
#validityAtCreation = 24:00


#=======================================================================
#  default attribute values for AnaTask objects
[defaults_AnaTask]

#  Analysis Transform
#analysis = None

#  comment of the task
#comment = 

#  Number of Jobs run concurrently
#float = 0

#  Name of the Task
#name = NewTask

#  Resubmit only if the number of running jobs is less than "resub_limit" times
#  the float. This makes the job table clearer, since more jobs can be submitted
#  as subjobs.
#resub_limit = 0.9

#  list of transforms
#transforms = []


#=======================================================================
#  default attribute values for AnaTaskSplitterJob objects
[defaults_AnaTaskSplitterJob]

#  Maximum number of events in a file of input dataset
#numevtsperfile = 0

#  Number of events per subjob
#numevtsperjob = 0

#  List of subjobs
#subjobs = []


#=======================================================================
#  default attribute values for AnaTransform objects
[defaults_AnaTransform]

#  Application of the Transform. Must be a Task-Supporting application.
#application = None

#  Backend of the Transform.
#backend = None

#  name of the output dataset
#dataset_name = 

#  files per job
#files_per_job = 5

#  Input dataset
#inputdata = None

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Name of the transform (cosmetic)
#name = Simple Transform

#  Output dataset
#outputdata = <GangaAtlas.Lib.ATLASDataset.DQ2Dataset.DQ2OutputDataset object at 0x2e1ba90>

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []


#=======================================================================
#  default attribute values for ArgSplitter objects
[defaults_ArgSplitter]

#  A list of lists of arguments to pass to script
#args = []


#=======================================================================
#  default attribute values for ArgSplitterTask objects
[defaults_ArgSplitterTask]

#  A list of lists of arguments to pass to script
#args = []

#  task partition numbers.
#task_partitions = []


#=======================================================================
#  default attribute values for Athena objects
[defaults_Athena]

#  Extra files to include in the user area
#append_to_user_area = []

#  Switch if user code should be compiled remotely
#athena_compile = False

#  ATLAS CMTCONFIG environment variable
#atlas_cmtconfig = 

#  ATLAS DBRelease DQ2 dataset and DQ2Release tar file. Use LATEST for most
#  recent.
#atlas_dbrelease = LATEST

#  Extra environment variable to be set
#atlas_environment = []

#  Athena Executable type, e.g. ATHENA, PYARA, ROOT, TRF, EXE
#atlas_exetype = ATHENA

#  ATLAS Production Software Release
#atlas_production = 

#  ATLAS Project Name
#atlas_project = 

#  ATLAS Software Release
#atlas_release = 

#  ATLAS run configuration
#atlas_run_config = {'input': {'noInput': True}, 'other': {}, 'output': {'alloutputs': []}}

#  ATLAS run directory
#atlas_run_dir = ./

#  suppress some output streams. e.g., ['ESD','TAG']
#atlas_supp_stream = []

#  use AIDA
#atlas_use_AIDA = False

#  Switch to collect statistics info and store in stats field
#collect_stats = False

#  Pattern of files to exclude from user area
#exclude_from_user_area = []

#  Packages to exclude from user area requirements file
#exclude_package = []

#  list of glue packages which cannot be found due to empty i686-slc4-gcc34-opt.
#  e.g., ['External/AtlasHepMC','External/Lhapdf']
#glue_packages = []

#  A tar file of the group area
#group_area = None

#  Location of shared resources. Presence of this attribute implies the
#  application has been prepared.
#is_prepared = None

#  Maximum number of events
#max_events = -999

#  list of job options files
#option_file = []

#  Additional Athena options
#options = 

#  Set to RDO, ESD or AOD to enable RecExCommon type jobs of appropriate type
#recex_type = 

#  Run event list
#run_event = []

#  Name of the file containing run/event list for Panda backend
#run_event_file = 

#  Number of events to skip
#skip_events = 0

#  Dictionary of stats info
#stats = {}

#  Parameters for transformations
#trf_parameter = {}

#  Switch to add AthenaPackages to tarball if Athena.exetype=EXE is used. Also
#  used to enable cmt setup of athena packages for Panda when exetype is PYARA.
#useAthenaPackages = False

#  Use Mana
#useMana = False

#  Use the original filename with the attempt number for input in --trf when
#  there is only one input, which follows the globbing scheme of new
#  transformation framework
#useNewTRF = True

#  Use No Athena setup to allow e.g. free ROOT setup
#useNoAthenaSetup = False

#  Use debug print-out in logfiles of Local/Batch/CREAM/LCG backend
#useNoDebugLogs = False

#  Use RootCore
#useRootCore = False

#  Use RootCore with NoBuild
#useRootCoreNoBuild = False

#  A tar file of the user area
#user_area = None

#  Path where user_area tarfile is created
#user_area_path = 

#  User setup script for special setup
#user_setupfile = None


#=======================================================================
#  default attribute values for AthenaMC objects
[defaults_AthenaMC]

#  ATLAS Software Release
#atlas_release = 

#  flag to use kit or cern AFS installation. Set to CERN for the latter, leave
#  unset otherwise.
#cmtsite = 

#  flag to not do stagein/stageout, for testing.
#dryrun = False

#  JobOption filename, or path is modified locally
#evgen_job_option = 

#  Extra arguments for the transformation, fixed value (experts only)
#extraArgs = 

#  Extra integer arguments for the transformation, with value increasing with
#  the subjob number. Please set like this: extraIncArgs="arg1=val1_0
#  arg2=val2_0" with valX_0 the value taken by the argument at the first subjob.
#  On the second subjob, the arguments will have the value valX_0 + 1 and so
#  on...  (experts only)
#extraIncArgs = 

#  evgen: sets first event number to be generated (in first job. The first event
#  number in second job will be firstevent+number_events_job and so on...).
#  simul, recon: decides how many events to be skipped in input files (= skip
#  +1). This is propagated to all subjobs.
#firstevent = 1

#  Geometry tag for simulation and reconstruction
#geometryTag = ATLAS-DC3-05

#  Step in the generation chain (evgen, simul (is simul+digit), recon,
#  template). template is to use any transformation not coverd by any of the
#  three previous steps.
#mode = 

#  Number of events per job
#number_events_job = 1

#  output partition number
#partition_number = None

#  Name of the generated physics process. Now replaced by production_name
#process_name = 

#  Name of the MC production
#production_name = 

#  Random Seed for MC Generator
#random_seed = 1

#  Run number. Now replaced by production_name
#run_number = 

#  Name of prefered SE or DQ2 site (from TierOfAtlas.py) for output
#se_name = none

#  location of experiment software area for non-grid backends.
#siteroot = 

#  optional flags for the transform run, like --ignoreunknown
#transflags = 

#  Name or Web location of a modified ATLAS transform archive.
#transform_archive = 

#  File name of the transformation script to use
#transform_script = 

#  recon, 12.0.5 and beyond: trigger configuration
#triggerConfig = NONE


#userarea = 

#  Verbosity of transformation for log files
#verbosity = ERROR

#  version tag to insert in the output dataset and file names
#version = 


#=======================================================================
#  default attribute values for AthenaMCInputDatasets objects
[defaults_AthenaMCInputDatasets]

#  DQ2 Dataset Name
#DQ2dataset = 

#  LFC path of directory to find inputfiles on the grid, or local directory path
#  for input datasets (datasetType=local). For all non-DQ2 datasets.
#LFCpath = 

#  Name of the dataset to be used for cavern noise (pileup jobs) or extra input
#  dataset (other transforms). This dataset must be a DQ2 dataset
#cavern = 

#  Type of dataset(DQ2,private,unknown or local). DQ2 means the requested
#  dataset is registered in DQ2 catalogs, private is for input datasets
#  registered in a non-DQ2 storage (Tier3) and known to CERN local LFC. local is
#  for local datasets on Local backend only
#datasetType = unknown

#  Name of the dataset to be used for minimum bias (pileup jobs) or extra input
#  dataset (other transforms). This dataset must be a DQ2 dataset
#minbias = 

#  Number of input cavern files processed by one job or subjob. Minimum 1
#n_cavern_files_job = 1

#  Number of input cavern files processed by one job or subjob. Minimum 1
#n_minbias_files_job = 1

#  Number of "events" per input file. This is used together with
#  application.number_events_job to calculate the number of input files per job,
#  or the number of jobs to inputfiles, respectively. This replaces
#  "n_infiles_job".
#number_events_file = 1

#  FOR EXPERTS: Redefine the input partitions. There are three possibilities to
#  specify the new input partitions: 1) String of input file numbers to be used
#  (each block separated by a comma). A block can be a single number or a closed
#  subrange (x-y). Subranges are defined with a dash. 2) List of input file
#  numbers as integers 3) List of LFNs of input files as strings. This replaces
#  the "inputfiles" property. To only process some events, it is recommended not
#  to use "redefine_partitions" but rather use an AthenaMCSplitter and
#  j.splitter.input_partitions.
#redefine_partitions = 

#  Number of events to skip in the first input file (after skip_files). This
#  shifts the numbering of the output files, so the output file number one will
#  contain the first set of events after skip_files and skip_events.
#skip_events = 0

#  File numbers to skip in the input dataset. This shifts the numbering of the
#  output files: If skip_files = 10 the processing starts with output file
#  number one at input file 11.
#skip_files = 0

#  If True, use the partition numbers "_00001" of the input files. This MUST be
#  enabled if you work with input datasets that keep changing, since only then
#  are you guaranteed not to process the same file twice. If your input dataset
#  is frozen or you only process the dataset once, you can leave this on False.
#use_partition_numbers = False


#=======================================================================
#  default attribute values for AthenaMCInputDatasetsMigration12 objects
[defaults_AthenaMCInputDatasetsMigration12]

#  DQ2 Dataset Name
#DQ2dataset = 

#  LFC path of directory to find inputfiles on the grid, or local directory path
#  for input datasets (datasetType=local). For all non-DQ2 datasets.
#LFCpath = 

#  Name of the dataset to be used for cavern noise (pileup jobs) or extra input
#  dataset (other transforms). This dataset must be a DQ2 dataset
#cavern = 

#  Type of dataset(DQ2,private,unknown or local). DQ2 means the requested
#  dataset is registered in DQ2 catalogs, private is for input datasets
#  registered in a non-DQ2 storage (Tier3) and known to CERN local LFC. local is
#  for local datasets on Local backend only
#datasetType = unknown

#  Logical File Names of subset of files to be processed. Must be used in
#  conjunction of either DQ2dataset or LFCpath.
#inputfiles = []

#  String of input file numbers to be used (each block separated by a coma).A
#  block can be a single number or a closed subrange (x-y). Subranges are
#  defined with a dash. Must be used in conjunction of either DQ2dataset or
#  LFCpath. Alternative to inputfiles.
#inputpartitions = 

#  Name of the dataset to be used for minimum bias (pileup jobs) or extra input
#  dataset (other transforms). This dataset must be a DQ2 dataset
#minbias = 

#  Number of input cavern files processed by one job or subjob. Minimum 1
#n_cavern_files_job = 1

#  Number of input files processed by one job or subjob. Minimum 1
#n_infiles_job = 1

#  Number of input cavern files processed by one job or subjob. Minimum 1
#n_minbias_files_job = 1

#  Number of inputfiles to process.
#number_inputfiles = 


#=======================================================================
#  default attribute values for AthenaMCOutputDatasets objects
[defaults_AthenaMCOutputDatasets]

#  file prefix and dataset suffix for logfiles.
#logfile = 

#  path of output directory tree for storage. Used for both LFC and physical
#  file locations.
#outdirectory = 

#  dataset suffix for combined output dataset. If set, it will collect all
#  expected output files for the job. If not set, every output type (histo,
#  HITS, EVGEN...) will have its own output dataset.
#output_dataset = 

#  EXPERT: Number of first output file. The job processing the first partition
#  will generate the file with the number output_firstfile, the second will
#  generate output_firstfile+1, and so on...
#output_firstfile = 1

#  file prefixes and dataset suffixes for other output root files. To set for
#  example the evgen file prefix, type: j.outputdata.outrootfiles["EVNT"] =
#  "file.prefix". The keys used are EVNT, HIST, HITS, RDO, ESD, AOD and NTUP. To
#  reset a value to default, type "del j.outputdata.outrootfiles["EVNT"]. To
#  disable creation of a file, type j.outputdata.outrootfiles["EVNT"] = "NONE"
#outrootfiles = {}


#=======================================================================
#  default attribute values for AthenaMCOutputDatasetsMigration12 objects
[defaults_AthenaMCOutputDatasetsMigration12]

#  file prefix and dataset suffix for logfiles.
#logfile = 

#  file prefix and dataset suffix for AOD files. Placeholder for any type of
#  output file in template mode.
#outaodfile = 

#  path of output directory tree for storage. Used for both LFC and physical
#  file locations.
#outdirectory = 

#  file prefix and dataset suffix for ESD files. Placeholder for any type of
#  output file in template mode.
#outesdfile = 

#  file prefix and dataset suffix for histogram files. Placeholder for any type
#  of output file in template mode.
#outhistfile = 

#  file prefix and dataset suffix for ntuple files. Placeholder for any type of
#  output file in template mode.
#outntuplefile = 

#  dataset suffix for combined output dataset. If set, it will collect all
#  expected output files for the job. If not set, every output type (histo,
#  HITS, EVGEN...) will have its own output dataset.
#output_dataset = 

#  offset for output file partition numbers. First job will generate the
#  partition number output_firstfile, second will generate output_firstfile+1,
#  and so on...
#output_firstfile = 1

#  file prefix and dataset suffix for RDO files. Placeholder for any type of
#  output file in template mode.
#outrdofile = 

#  file prefix and dataset suffix for primary output root file (EVGEN for evgen
#  jobs, HITS for simul jobs). Placeholder for any type of output file in
#  template mode.
#outrootfile = 


#=======================================================================
#  default attribute values for AthenaMCSplitterJob objects
[defaults_AthenaMCSplitterJob]

#  List of input file numbers to be processed, either as a string in the format
#  "1,3,5-10,15-" or as a list of integers. Alternative to output_partitions
#input_partitions = 

#  Limit the number of subjobs. If this is left at 0, all partitions will be
#  processed.
#numsubjobs = 0

#  List of partition numbers to be processed, either as a string in the format
#  "1,3,5-10,15-" or as a list of integers. Alternative to input_partitions
#output_partitions = 

#  List of random seeds to use for the subjobs. Only used if it is a list
#random_seeds = []


#=======================================================================
#  default attribute values for AthenaMCTask objects
[defaults_AthenaMCTask]

#  ATLAS Software Release
#atlas_release = 

#  flag to use kit or cern AFS installation. Set to CERN for the latter, leave
#  unset otherwise.
#cmtsite = 

#  flag to not do stagein/stageout, for testing.
#dryrun = False

#  JobOption filename, or path is modified locally
#evgen_job_option = 

#  Extra arguments for the transformation, fixed value (experts only)
#extraArgs = 

#  Extra integer arguments for the transformation, with value increasing with
#  the subjob number. Please set like this: extraIncArgs="arg1=val1_0
#  arg2=val2_0" with valX_0 the value taken by the argument at the first subjob.
#  On the second subjob, the arguments will have the value valX_0 + 1 and so
#  on...  (experts only)
#extraIncArgs = 

#  evgen: sets first event number to be generated (in first job. The first event
#  number in second job will be firstevent+number_events_job and so on...).
#  simul, recon: decides how many events to be skipped in input files (= skip
#  +1). This is propagated to all subjobs.
#firstevent = 1

#  Geometry tag for simulation and reconstruction
#geometryTag = ATLAS-DC3-05

#  Step in the generation chain (evgen, simul (is simul+digit), recon,
#  template). template is to use any transformation not coverd by any of the
#  three previous steps.
#mode = 

#  Number of events per job
#number_events_job = 1

#  output partition number
#partition_number = None

#  Name of the generated physics process. Now replaced by production_name
#process_name = 

#  Name of the MC production
#production_name = 

#  Random Seed for MC Generator
#random_seed = 1

#  Run number. Now replaced by production_name
#run_number = 

#  Name of prefered SE or DQ2 site (from TierOfAtlas.py) for output
#se_name = none

#  location of experiment software area for non-grid backends.
#siteroot = 

#  optional flags for the transform run, like --ignoreunknown
#transflags = 

#  Name or Web location of a modified ATLAS transform archive.
#transform_archive = 

#  File name of the transformation script to use
#transform_script = 

#  recon, 12.0.5 and beyond: trigger configuration
#triggerConfig = NONE


#userarea = 

#  Verbosity of transformation for log files
#verbosity = ERROR

#  version tag to insert in the output dataset and file names
#version = 


#=======================================================================
#  default attribute values for AthenaMCTaskSplitterJob objects
[defaults_AthenaMCTaskSplitterJob]

#  List of input file numbers to be processed, either as a string in the format
#  "1,3,5-10,15-" or as a list of integers. Alternative to output_partitions
#input_partitions = 

#  Limit the number of subjobs. If this is left at 0, all partitions will be
#  processed.
#numsubjobs = 0

#  List of partition numbers to be processed, either as a string in the format
#  "1,3,5-10,15-" or as a list of integers. Alternative to input_partitions
#output_partitions = 

#  List of random seeds to use for the subjobs. Only used if it is a list
#random_seeds = []

#  task partition numbers.
#task_partitions = []


#=======================================================================
#  default attribute values for AthenaOutputDataset objects
[defaults_AthenaOutputDataset]

#  Files to be returned
#files = []

#  Output location
#location = 


#=======================================================================
#  default attribute values for AthenaOutputMerger objects
[defaults_AthenaOutputMerger]

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  Subjob numbers to be merged
#subjobs = []

#  Output directory of merged files
#sum_outputdir = 


#=======================================================================
#  default attribute values for AthenaSplitterJob objects
[defaults_AthenaSplitterJob]

#  Match the number of subjobs to the number of inputfiles
#match_subjobs_files = False

#  Number of files per subjob
#numfiles_subjob = 0

#  Number of subjobs
#numsubjobs = 0

#  Match the number of subjobs to the number of datasets
#split_per_dataset = False


#=======================================================================
#  default attribute values for AthenaTask objects
[defaults_AthenaTask]

#  Extra files to include in the user area
#append_to_user_area = []

#  Switch if user code should be compiled remotely
#athena_compile = False

#  ATLAS CMTCONFIG environment variable
#atlas_cmtconfig = 

#  ATLAS DBRelease DQ2 dataset and DQ2Release tar file. Use LATEST for most
#  recent.
#atlas_dbrelease = LATEST

#  Extra environment variable to be set
#atlas_environment = []

#  Athena Executable type, e.g. ATHENA, PYARA, ROOT, TRF, EXE
#atlas_exetype = ATHENA

#  ATLAS Production Software Release
#atlas_production = 

#  ATLAS Project Name
#atlas_project = 

#  ATLAS Software Release
#atlas_release = 

#  ATLAS run configuration
#atlas_run_config = {'input': {'noInput': True}, 'other': {}, 'output': {'alloutputs': []}}

#  ATLAS run directory
#atlas_run_dir = ./

#  suppress some output streams. e.g., ['ESD','TAG']
#atlas_supp_stream = []

#  use AIDA
#atlas_use_AIDA = False

#  Switch to collect statistics info and store in stats field
#collect_stats = False

#  Pattern of files to exclude from user area
#exclude_from_user_area = []

#  Packages to exclude from user area requirements file
#exclude_package = []

#  list of glue packages which cannot be found due to empty i686-slc4-gcc34-opt.
#  e.g., ['External/AtlasHepMC','External/Lhapdf']
#glue_packages = []

#  A tar file of the group area
#group_area = None

#  Location of shared resources. Presence of this attribute implies the
#  application has been prepared.
#is_prepared = None

#  Maximum number of events
#max_events = -999

#  list of job options files
#option_file = []

#  Additional Athena options
#options = 

#  Set to RDO, ESD or AOD to enable RecExCommon type jobs of appropriate type
#recex_type = 

#  Run event list
#run_event = []

#  Name of the file containing run/event list for Panda backend
#run_event_file = 

#  Number of events to skip
#skip_events = 0

#  Dictionary of stats info
#stats = {}

#  Parameters for transformations
#trf_parameter = {}

#  Switch to add AthenaPackages to tarball if Athena.exetype=EXE is used. Also
#  used to enable cmt setup of athena packages for Panda when exetype is PYARA.
#useAthenaPackages = False

#  Use Mana
#useMana = False

#  Use the original filename with the attempt number for input in --trf when
#  there is only one input, which follows the globbing scheme of new
#  transformation framework
#useNewTRF = True

#  Use No Athena setup to allow e.g. free ROOT setup
#useNoAthenaSetup = False

#  Use debug print-out in logfiles of Local/Batch/CREAM/LCG backend
#useNoDebugLogs = False

#  Use RootCore
#useRootCore = False

#  Use RootCore with NoBuild
#useRootCoreNoBuild = False

#  A tar file of the user area
#user_area = None

#  Path where user_area tarfile is created
#user_area_path = 

#  User setup script for special setup
#user_setupfile = None


#=======================================================================
#  default attribute values for AtlasCREAMRequirements objects
[defaults_AtlasCREAMRequirements]

#  Set to True to allow cross-cloud submission. Use with the 'ALL' cloud option.
#anyCloud = True

#  ATLAS cloud name: CERN, IT, ES, FR, UK, DE, NL, TW, CA, US, NG
#cloud = ALL

#  Minimum available CPU time (min)
#cputime = 0

#  DQ2 client version on the computing element
#dq2client_version = 

#  ATLAS cloud names to be excluded
#excluded_clouds = []

#  ATLAS site names to be excluded
#excluded_sites = []

#  External connectivity
#ipconnectivity = False

#  Mininum available memory (MB)
#memory = None

#  Number of Nodes for MPICH jobs
#nodenumber = 1

#  Operation Systems
#os = 

#  Other Requirements
#other = []

#  ATLAS site names
#sites = []

#  Software Installations
#software = []

#  Mimimum available total time (min)
#walltime = None


#=======================================================================
#  default attribute values for AtlasLCGRequirements objects
[defaults_AtlasLCGRequirements]

#  Set to True to allow cross-cloud submission. Use with the 'ALL' cloud option.
#anyCloud = True

#  ATLAS cloud name: CERN, IT, ES, FR, UK, DE, NL, TW, CA, US, NG
#cloud = ALL

#  Minimum available CPU time (min)
#cputime = None

#  DQ2 client version on the computing element
#dq2client_version = 0.1.35

#  ATLAS cloud names to be excluded
#excluded_clouds = []

#  ATLAS site names to be excluded
#excluded_sites = []

#  External connectivity
#ipconnectivity = False

#  Mininum available memory (MB)
#memory = None

#  Number of Nodes for MPICH jobs
#nodenumber = 1

#  Operation Systems
#os = 

#  Other Requirements
#other = []

#  ATLAS site names
#sites = []

#  Software Installations
#software = []

#  Mimimum available total time (min)
#walltime = None


#=======================================================================
#  default attribute values for AtlasTask objects
[defaults_AtlasTask]

#  Check all Transforms during each monitoring loop cycle
#check_all_trfs = True

#  comment of the task
#comment = 

#  Number of Jobs run concurrently
#float = 0

#  Name of the Task
#name = NewTask

#  list of transforms
#transforms = []


#=======================================================================
#  default attribute values for AtlasTransform objects
[defaults_AtlasTransform]

#  Split by total input filesize (cf DQ2JobSplitter.filesize)
#MB_per_job = 0

#  Break out of the Task Loop after submissions
#abort_loop_on_submit = True

#  Application of the Transform.
#application = None

#  Backend of the Transform.
#backend = None

#  Minutes delay between a required/chained unit completing and starting this
#  one
#chain_delay = 0

#  The dataset to copy all units output to, e.g. Grid dataset -> Local Dataset
#copy_output = None

#  List of Regular expressions of which files to exclude from copy
#exclude_file_mask = []

#  files per job (cf DQ2JobSplitter.numfiles)
#files_per_job = -1

#  Maximum number of files to assign to each unit from the given local files
#  (i.e. AtlasLocalDataset). If < 1, use all files. At present, does not apply
#  to DQ2Datasets
#files_per_unit = -1

#  List of Regular expressions of which files to include in copy
#include_file_mask = []

#  list of file objects that will act as input files for a job
#inputfiles = []

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Local location to copy output to
#local_location = 

#  Maximum number of Ganga Threads to use. Note that the number of simultaneous
#  threads is controlled by the queue system (default is 5)
#max_active_threads = 10

#  Name of the transform (cosmetic)
#name = Simple Transform

#  Number of DQ2 download threads to run simultaneously (use setNumDQ2Threads to
#  modify after submission)
#num_dq2_threads = 1

#  Output dataset template
#outputdata = None

#  list of OutputFile objects to be copied to all jobs
#outputfiles = []

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  list of postprocessors to run after job has finished
#postprocessors = None

#  Fraction of failed subjobs to complete subjobs above which the job will be
#  rebrokered
#rebroker_fraction = 0.6

#  Rebroker if too many minor resubs
#rebroker_on_job_fail = True

#  IDs of transforms that must complete before this unit will start. NOTE
#  DOESN'T COPY OUTPUT DATA TO INPUT DATA. Use TaskChainInput Dataset for that.
#required_trfs = []

#  Splitter used on each unit of the Transform.
#splitter = None

#  split into this many subjobs per unit master job (cf
#  DQ2JobSplitter.numsubjobs)
#subjobs_per_unit = 0

#  Use Ganga Threads for submission
#submit_with_threads = False

#  The dataset to copy each individual unit output to, e.g. Grid dataset ->
#  Local Dataset
#unit_copy_output = None

#  Merger to be run copied and run on each unit separately.
#unit_merger = None

#  list of units
#units = []


#=======================================================================
#  default attribute values for AtlasUnit objects
[defaults_AtlasUnit]

#  Application of the Transform.
#application = None

#  The dataset to copy the output of this unit to, e.g. Grid dataset -> Local
#  Dataset
#copy_output = None

#  Input dataset
#inputdata = None

#  list of file objects that will act as input files for a job
#inputfiles = []

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Merger to be run after this unit completes.
#merger = None

#  Name of the unit (cosmetic)
#name = Simple Unit

#  Output dataset
#outputdata = None

#  list of OutputFile objects to be copied to all jobs
#outputfiles = []

#  list of postprocessors to run after job has finished
#postprocessors = None

#  Splitter used on each unit of the Transform.
#splitter = None


#=======================================================================
#  default attribute values for CREAM objects
[defaults_CREAM]

#  CREAM CE endpoint
#CE = 

#  Job type: Normal, MPICH
#jobtype = Normal

#  Requirements for the resource selection
#requirements = None

#  Interface for handling oversized input sandbox
#sandboxcache = None


#=======================================================================
#  default attribute values for Condor objects
[defaults_Condor]

#  Environment settings for execution host
#env = {}

#  Flag to pass current envrionment to execution host
#getenv = False

#  Globus RSL settings (for Condor-G submission)
#globus_rsl = 

#  Globus scheduler to be used (required for Condor-G submission)
#globusscheduler = 

#  Ranking scheme to be used when selecting execution host
#rank = Memory

#  Requirements for selecting execution host
#requirements = CondorRequirements

#  Flag indicating if Condor nodes have shared filesystem
#shared_filesystem = True

#  Options passed to Condor at submission time
#submit_options = []

#  Type of execution environment to be used by Condor
#universe = vanilla


#=======================================================================
#  default attribute values for CondorRequirements objects
[defaults_CondorRequirements]

#  System architecture
#arch = INTEL

#  Excluded execution hosts, given as a string of space-separated names:
#  'machine1 machine2 machine3'; or as a list of names: [ 'machine1',
#  'machine2', 'machine3' ]
#excluded_machine = 

#  Requested execution hosts, given as a string of space-separated names:
#  'machine1 machine2 machine3'; or as a list of names: [ 'machine1',
#  'machine2', 'machine3' ]
#machine = 

#  Mininum physical memory
#memory = 400

#  Operating system
#opsys = LINUX

#  Other requirements, given as a list of strings, for example: [ 'OSTYPE ==
#  "SLC4"', '(POOL == "GENERAL" || POOL == "GEN_FARM")' ]; the final requirement
#  is the AND of all elements in the list
#other = []

#  Minimum virtual memory
#virtual_memory = 400


#=======================================================================
#  default attribute values for CoreTask objects
[defaults_CoreTask]

#  Check all Transforms during each monitoring loop cycle
#check_all_trfs = True

#  comment of the task
#comment = 

#  Number of Jobs run concurrently
#float = 0

#  Name of the Task
#name = NewTask

#  list of transforms
#transforms = []


#=======================================================================
#  default attribute values for CoreTransform objects
[defaults_CoreTransform]

#  Break out of the Task Loop after submissions
#abort_loop_on_submit = True

#  Application of the Transform.
#application = None

#  Backend of the Transform.
#backend = None

#  Minutes delay between a required/chained unit completing and starting this
#  one
#chain_delay = 0

#  Treat the inputdata as inputfiles, i.e. copy the inputdata to the WN
#chaindata_as_inputfiles = False

#  The dataset to copy all units output to, e.g. Grid dataset -> Local Dataset
#copy_output = None

#  A list of fields that should be copied when creating units, e.g. application,
#  inputfiles. Empty (default) implies all fields are copied unless the
#  GeenricSplitter is used
#fields_to_copy = []

#  Number of files per unit if possible. Set to -1 to just create a unit per
#  input dataset
#files_per_unit = -1

#  list of file objects that will act as input files for a job
#inputfiles = []

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Maximum number of Ganga Threads to use. Note that the number of simultaneous
#  threads is controlled by the queue system (default is 5)
#max_active_threads = 10

#  Name of the transform (cosmetic)
#name = Simple Transform

#  Output dataset template
#outputdata = None

#  list of OutputFile objects to be copied to all jobs
#outputfiles = []

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  list of postprocessors to run after job has finished
#postprocessors = None

#  Rebroker if too many minor resubs
#rebroker_on_job_fail = True

#  IDs of transforms that must complete before this unit will start. NOTE
#  DOESN'T COPY OUTPUT DATA TO INPUT DATA. Use TaskChainInput Dataset for that.
#required_trfs = []

#  Splitter used on each unit of the Transform.
#splitter = None

#  Use Ganga Threads for submission
#submit_with_threads = False

#  The dataset to copy each individual unit output to, e.g. Grid dataset ->
#  Local Dataset
#unit_copy_output = None

#  Merger to be run copied and run on each unit separately.
#unit_merger = None

#  Splitter to be used to create the units
#unit_splitter = None

#  list of units
#units = []


#=======================================================================
#  default attribute values for CoreUnit objects
[defaults_CoreUnit]

#  Application of the Transform.
#application = None

#  The dataset to copy the output of this unit to, e.g. Grid dataset -> Local
#  Dataset
#copy_output = None

#  Input dataset
#inputdata = None

#  list of file objects that will act as input files for a job
#inputfiles = []

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Merger to be run after this unit completes.
#merger = None

#  Name of the unit (cosmetic)
#name = Simple Unit

#  Output dataset
#outputdata = None

#  list of OutputFile objects to be copied to all jobs
#outputfiles = []

#  list of postprocessors to run after job has finished
#postprocessors = None

#  Splitter used on each unit of the Transform.
#splitter = None


#=======================================================================
#  default attribute values for CustomChecker objects
[defaults_CustomChecker]

#  Run on master
#checkMaster = True

#  Run on subjobs
#checkSubjobs = True

#  Path to a python module to perform the check.
#module = None


#=======================================================================
#  default attribute values for CustomMerger objects
[defaults_CustomMerger]

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  Path to a python module to perform the merge.
#module = None

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for DQ2Dataset objects
[defaults_DQ2Dataset]

#  Accessprotocol to use on worker node, e.g. Xrootd
#accessprotocol = 

#  Check md5sum of input files on storage elemenet - very time consuming !
#check_md5sum = False

#  md5sum or adler checksums of input files
#checksums = []

#  Dataset Name(s)
#dataset = []

#  Data type: DATA, MC or MuonCalibStream
#datatype = 

#  Logical File Names to exclude from processing
#exclude_names = []

#  Logical file name pattern to exclude from processing
#exclude_pattern = []

#  Use DQ2_COPY automatically if DQ2_LOCAL fails
#failover = False

#  GUID of Logical File Names
#guids = []

#  Match complete and incomplete sources of dataset to CE during job submission
#match_ce_all = False

#  Number of minimum files at incomplete dataset location
#min_num_files = 0

#  Logical File Names to use for processing
#names = []

#  Logical file name pattern to use for processing
#names_pattern = []

#  Number of files.
#number_of_files = 0

#  scopes of the input files for RUCIO testing
#scopes = []

#  Sizes of input files
#sizes = []

#  Provide the collection ref if not in primary JOs (useful for TRF usage): AOD,
#  ESD, RAW
#tag_coll_ref = 

#  Input TAG/ELSSI files to run over. tag_info structure will get filled on
#  submission
#tag_files = []

#  TAG information used to split the job
#tag_info = {}

#  Tag Dataset Name
#tagdataset = []

#  Dataset access on worker node: DQ2_LOCAL (default), DQ2_COPY, LFC
#type = 

#  Use AOD to ESD Backnavigation
#use_aodesd_backnav = False

#  Use CVMFS to access TAG files
#use_cvmfs_tag = False


#=======================================================================
#  default attribute values for DQ2FileIndex objects
[defaults_DQ2FileIndex]

#  a key:value pairs of file metadata
#attributes = {}

#  the DQ2 dataset name
#dataset = 

#  the main identity of the file
#id = 

#  the md5sum of the file
#md5sum = 

#  the name of the file
#name = 

#  the DQ2 site id
#site = 


#=======================================================================
#  default attribute values for DQ2JobSplitter objects
[defaults_DQ2JobSplitter]

#  Maximum filesize sum per subjob im MB.
#filesize = 0

#  Maximum number of events in a file of input dataset
#numevtsperfile = 0

#  Number of events per subjob
#numevtsperjob = 0

#  Number of files per subjob
#numfiles = 0

#  Number of subjobs
#numsubjobs = 0

#  Update siteindex during job submission to get the latest file location
#  distribution.
#update_siteindex = True

#  Use black list of sites create by GangaRobot functional tests.
#use_blacklist = True

#  Allow submission to a site although data is not there for FAX usage
#use_fax = False

#  Use LFC catalog instead of default site catalog/tracker service
#use_lfc = False


#=======================================================================
#  default attribute values for DQ2OutputDataset objects
[defaults_DQ2OutputDataset]

#  Name of the DQ2 output dataset automatically filled by the job
#datasetname = 

#  Name of the group to be used if isGroupDS=True
#groupname = 

#  Use group datasetname prefix
#isGroupDS = False

#  Local output path location
#local_location = 

#  SE output path location
#location = 

#  Output files to be returned via SE
#outputdata = []

#  SE output spacetoken
#spacetoken = 

#  Panda only: Specify a comma-separated list of patterns so that only datasets
#  which match the given patterns are transferred when outputdata.location is
#  set. Either \ or "" is required when a wildcard is used. If omitted, all
#  datasets are transferred
#transferredDS = 

#  Use shorter version of filenames and do not prepend users.myname.ganga
#use_shortfilename = False


#=======================================================================
#  default attribute values for DQ2SandboxCache objects
[defaults_DQ2SandboxCache]

#  the DQ2 dataset name
#dataset_name = 

#  the DQ2 local site id
#local_site_id = CERN-PROD_SCRATCHDISK

#  max. number of tries in case of failures
#max_try = 1

#  file transfer protocol
#protocol = 

#  the DQ2 setup script
#setup = /afs/cern.ch/atlas/offline/external/GRID/ddm/DQ2Clients/setup.sh


#=======================================================================
#  default attribute values for DefaultSplitter objects
[defaults_DefaultSplitter]


#=======================================================================
#  default attribute values for EmptyDataset objects
[defaults_EmptyDataset]


#=======================================================================
#  default attribute values for EventPicking objects
[defaults_EventPicking]

#  Accessprotocol to use on worker node, e.g. Xrootd
#accessprotocol = 

#  Check md5sum of input files on storage elemenet - very time consuming !
#check_md5sum = False

#  md5sum or adler checksums of input files
#checksums = []

#  Dataset Name(s)
#dataset = []

#  Data type: DATA, MC or MuonCalibStream
#datatype = 

#  AMI tag used to match TAG collections names. This option is required when you
#  are interested in older data than the latest one. Either \ or "" is required
#  when a wild-card is used. e.g., f2\*.
#event_pick_amitag = 

#  Logical File Names to exclude from processing
#exclude_names = []

#  Logical file name pattern to exclude from processing
#exclude_pattern = []

#  Use DQ2_COPY automatically if DQ2_LOCAL fails
#failover = False

#  GUID of Logical File Names
#guids = []

#  Match complete and incomplete sources of dataset to CE during job submission
#match_ce_all = False

#  Number of minimum files at incomplete dataset location
#min_num_files = 0

#  Logical File Names to use for processing
#names = []

#  Logical file name pattern to use for processing
#names_pattern = []

#  Number of files.
#number_of_files = 0

#  Type of data for event picking. One of AOD, ESD, RAW.
#pick_data_type = 

#  Dataset pattern which matches the selection
#pick_dataset_pattern = 

#  A filename which contains list of runs/events for event picking.
#pick_event_list = None

#  accept/reject the pick event.
#pick_filter_policy = accept

#  Stream name for event picking, e.g. physics_L1Calo
#pick_stream_name = 

#  scopes of the input files for RUCIO testing
#scopes = []

#  Sizes of input files
#sizes = []

#  Provide the collection ref if not in primary JOs (useful for TRF usage): AOD,
#  ESD, RAW
#tag_coll_ref = 

#  Input TAG/ELSSI files to run over. tag_info structure will get filled on
#  submission
#tag_files = []

#  TAG information used to split the job
#tag_info = {}

#  Tag Dataset Name
#tagdataset = []

#  Dataset access on worker node: DQ2_LOCAL (default), DQ2_COPY, LFC
#type = 

#  Use AOD to ESD Backnavigation
#use_aodesd_backnav = False

#  Use CVMFS to access TAG files
#use_cvmfs_tag = False


#=======================================================================
#  default attribute values for EvgenTransform objects
[defaults_EvgenTransform]

#  Application of the Transform. Must be a Task-Supporting application.
#application = None

#  Backend of the Transform.
#backend = None

#  Input dataset
#inputdata = None

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Name of the transform (cosmetic)
#name = Simple Transform

#  Output dataset
#outputdata = None

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  random seeds to be used in the partition
#random_seeds = {}


#=======================================================================
#  default attribute values for Executable objects
[defaults_Executable]

#  List of arguments for the executable. Arguments may be strings, numerics or
#  File objects.
#args = ['Hello World']

#  Dictionary of environment variables that will be replaced in the running
#  environment.
#env = {}

#  A path (string) or a File object specifying an executable.
#exe = echo

#  Location of shared resources. Presence of this attribute implies the
#  application has been prepared.
#is_prepared = None


#=======================================================================
#  default attribute values for ExecutableTask objects
[defaults_ExecutableTask]

#  List of arguments for the executable. Arguments may be strings, numerics or
#  File objects.
#args = ['Hello World']

#  Dictionary of environment variables that will be replaced in the running
#  environment.
#env = {}

#  A path (string) or a File object specifying an executable.
#exe = echo

#  Location of shared resources. Presence of this attribute implies the
#  application has been prepared.
#is_prepared = None


#=======================================================================
#  default attribute values for File objects
[defaults_File]

#  path to the file source
#name = 

#  destination subdirectory (a relative path)
#subdir = .


#=======================================================================
#  default attribute values for FileChecker objects
[defaults_FileChecker]

#  Run on master
#checkMaster = True

#  Run on subjobs
#checkSubjobs = True

#  Toggle whether job fails if string is found or not found.
#failIfFound = True

#  File to search in
#files = []

#  Toggle whether to fail job if a file isnt found.
#filesMustExist = True

#  String to search for
#searchStrings = []


#=======================================================================
#  default attribute values for GangaDataset objects
[defaults_GangaDataset]

#  list of file objects that will be the inputdata for the job
#files = []

#  Treat the inputdata as inputfiles, i.e. copy the inputdata to the WN
#treat_as_inputfiles = False


#=======================================================================
#  default attribute values for GangaDatasetSplitter objects
[defaults_GangaDatasetSplitter]

#  the number of files per subjob
#files_per_subjob = 5


#=======================================================================
#  default attribute values for GangaList objects
[defaults_GangaList]


#=======================================================================
#  default attribute values for GenericSplitter objects
[defaults_GenericSplitter]

#  The attribute on which the job is splitted
#attribute = 

#  Dictionary to specify multiple attributes to split over
#multi_attrs = {}

#  A list of the values corresponding to the attribute of the subjobs
#values = []


#=======================================================================
#  default attribute values for GoogleFile objects
[defaults_GoogleFile]

#  wheather the output file should be compressed before sending somewhere
#compressed = False

#  reason for the upload failure
#failureReason = 

#  local dir where the file is stored, used from get and put methods
#localDir = 

#  pattern of the file name
#namePattern = 


#=======================================================================
#  default attribute values for GridCommand objects
[defaults_GridCommand]

#  Command for destroying credential
#destroy = grid-proxy-destroy

#  Dictionary of parameter-value pairs to pass to destroy command
#destroy_parameters = {}

#  Command for obtaining information about credential
#info = grid-proxy-info

#  Dictionary mapping from Ganga credential properties to command-line options
#info_parameters = {}

#  Command for creating/initialising credential
#init = grid-proxy-init

#  Dictionary of parameter-value pairs to pass to init command
#init_parameters = {'pipe': '-pwstdin', 'valid': '-valid'}


#=======================================================================
#  default attribute values for GridFileIndex objects
[defaults_GridFileIndex]

#  a key:value pairs of file metadata
#attributes = {}

#  the main identity of the file
#id = 

#  the md5sum of the file
#md5sum = 

#  the name of the file
#name = 


#=======================================================================
#  default attribute values for GridProxy objects
[defaults_GridProxy]

#  Set of commands to be used for credential-related operations
#command = ICommandSet

#  Refresh time of proxy info cache
#info_refresh_time = 00:15

#  String of options to be passed to command for proxy creation
#init_opts = 

#  Number of password attempts allowed when creating credential
#maxTry = 5

#  Default minimum validity
#minValidity = 72:00

#  Default credential validity at creation
#validityAtCreation = 24:00

#  Virtual organisation managment system information
#voms = atlas


#=======================================================================
#  default attribute values for GridSandboxCache objects
[defaults_GridSandboxCache]

#  max. number of tries in case of failures
#max_try = 1

#  file transfer protocol
#protocol = 


#=======================================================================
#  default attribute values for GridftpFileIndex objects
[defaults_GridftpFileIndex]

#  a key:value pairs of file metadata
#attributes = {}

#  the main identity of the file
#id = 

#  the md5sum of the file
#md5sum = 

#  the name of the file
#name = 


#=======================================================================
#  default attribute values for GridftpSandboxCache objects
[defaults_GridftpSandboxCache]

#  the base URI for storing cached files
#baseURI = 

#  max. number of tries in case of failures
#max_try = 1

#  file transfer protocol
#protocol = 


#=======================================================================
#  default attribute values for ICommandSet objects
[defaults_ICommandSet]

#  Command for destroying credential
#destroy = 

#  Dictionary of parameter-value pairs to pass to destroy command
#destroy_parameters = {}

#  Command for obtaining information about credential
#info = 

#  Dictionary mapping from Ganga credential properties to command-line options
#info_parameters = {}

#  Command for creating/initialising credential
#init = 

#  Dictionary of parameter-value pairs to pass to init command
#init_parameters = {}


#=======================================================================
#  default attribute values for ITask objects
[defaults_ITask]

#  Check all Transforms during each monitoring loop cycle
#check_all_trfs = True

#  comment of the task
#comment = 

#  Number of Jobs run concurrently
#float = 0

#  Name of the Task
#name = NewTask

#  list of transforms
#transforms = []


#=======================================================================
#  default attribute values for Interactive objects
[defaults_Interactive]


#=======================================================================
#  default attribute values for Jedi objects
[defaults_Jedi]

#  EXPERT ONLY
#accessmode = 

#  String for Executable make command - if filled triggers a build job for the
#  Execuatble
#bexec = 

#  define extra output files, e.g. ['output1.txt','output2.dat']
#extOutFile = []

#  Force staging of input DS
#forcestaged = False

#  Create individual output dataset for each data-type. By default, all output
#  files are added to one output dataset
#individualOutDS = False

#  Existing Library dataset to use (disables buildjob)
#libds = None

#  Boolean if no build job should be sent - use it together with
#  Athena.athena_compile variable
#nobuild = False

#  Requirements for the resource selection
#requirements = None

#  Require the job to run at a specific site
#site = AUTO


#=======================================================================
#  default attribute values for JediPandaJob objects
[defaults_JediPandaJob]

#  Panda Job id
#id = None

#  Panda Job status
#status = None


#=======================================================================
#  default attribute values for JediPandaMergeJob objects
[defaults_JediPandaMergeJob]

#  Panda Job id
#id = None

#  Panda Job status
#status = None


#=======================================================================
#  default attribute values for JediRequirements objects
[defaults_JediRequirements]

#  Set to true to allow jobs to run in all clouds. If False, jobs are limited to
#  run in "requirements.cloud"
#anyCloud = True

#  cloud where jobs are submitted (default:US)
#cloud = US

#  Config string for the Job Execution Monitor.
#configJEM = 

#  Config parameters for output merging jobs.
#configMerge = {'type': '', 'exec': ''}

#  Enable a checker to skip corrupted files
#corCheck = False

#  Required CPU count in seconds. Mainly to extend time limit for looping
#  detection
#cputime = -1

#  disable automatic job retry on the server side
#disableAutoRetry = False

#  Enable the Job Execution Monitor.
#enableJEM = False

#  Enable the output merging jobs.
#enableMerge = False

#  Clouds to be excluded while brokering.
#excluded_clouds = []

#  Panda sites to be excluded while brokering.
#excluded_sites = []

#  Send the job using express quota to have higher priority. The number of
#  express subjobs in the queue and the total execution time used by express
#  subjobs are limited (a few subjobs and several hours per day, respectively).
#  This option is intended to be used for quick tests before bulk submission.
#  Note that buildXYZ is not included in quota calculation. If this option is
#  used when quota has already exceeded, the panda server will ignore the option
#  so that subjobs have normal priorities. Also, if you submit 1 buildXYZ and N
#  runXYZ subjobs when you only have quota of M (M < N),  only the first M
#  runXYZ subjobs will have higher priorities
#express = False

#  Send job to a long queue
#long = False

#  The maximum number of files per job is 200 by default since too many input
#  files result in a too long command-line argument on WN which crashes the job.
#  This option relax the limit.
#maxNFilesPerJob = 200

#  Required memory size
#memory = -1

#  Number of events per file to run over
#nEventsPerFile = 0

#  Number of files on which each sub-job runs.
#nFilesPerJob = 0

#  Instantiate one sub job per NGBPERJOB GB of input files. nGBPerJob=MAX sets
#  the size to the default maximum value
#nGBPerJob = 0

#  Suppress email notification
#noEmail = False

#  If input files are not read from SE, they will be skipped by default. This
#  option disables the functionality
#notSkipMissing = False

#  Expert option: overwriteQueuedata.
#overwriteQueuedata = False

#  Expert option: overwriteQueuedataConfig.
#overwriteQueuedataConfig = 

#  Specify a different root version for non-Athena jobs.
#rootver = 

#  Skip scout jobs
#skipScout = False

#  Number of sub-jobs to which a job is split.
#split = 0

#  Boolean if input.txt contains the input files as comma separated list or
#  separeted by a line break
#usecommainputtxt = False


#=======================================================================
#  default attribute values for Job objects
[defaults_Job]

#  specification of the application to be executed
#application = None

#  specification of the resources to be used (e.g. batch system)
#backend = None

#  comment of the job
#comment = 

#  Automatically resubmit failed subjobs
#do_auto_resubmit = False

#  JobInfo
#info = None

#  dataset definition (typically this is specific either to an application, a
#  site or the virtual organization
#inputdata = None

#  list of file objects that will act as input files for a job
#inputfiles = []

#  list of File objects shipped to the worker node
#inputsandbox = []

#  optional label which may be any combination of ASCII characters
#name = 

#  dataset definition (typically this is specific either to an application, a
#  site or the virtual organization
#outputdata = None

#  list of file objects decorating what have to be done with the output files
#  after job is completed
#outputfiles = []

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  Enable Submission of subjobs in parallel
#parallel_submit = False

#  list of postprocessors to run after job has finished
#postprocessors = []

#  optional splitter
#splitter = None


#=======================================================================
#  default attribute values for JobInfo objects
[defaults_JobInfo]

#  job monitor instance
#monitor = None


#=======================================================================
#  default attribute values for JobTemplate objects
[defaults_JobTemplate]

#  specification of the application to be executed
#application = None

#  specification of the resources to be used (e.g. batch system)
#backend = None

#  comment of the job
#comment = 

#  Automatically resubmit failed subjobs
#do_auto_resubmit = False

#  JobInfo
#info = None

#  dataset definition (typically this is specific either to an application, a
#  site or the virtual organization
#inputdata = None

#  list of file objects that will act as input files for a job
#inputfiles = []

#  list of File objects shipped to the worker node
#inputsandbox = []

#  optional label which may be any combination of ASCII characters
#name = 

#  dataset definition (typically this is specific either to an application, a
#  site or the virtual organization
#outputdata = None

#  list of file objects decorating what have to be done with the output files
#  after job is completed
#outputfiles = []

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  Enable Submission of subjobs in parallel
#parallel_submit = False

#  list of postprocessors to run after job has finished
#postprocessors = []

#  optional splitter
#splitter = None


#=======================================================================
#  default attribute values for JobTime objects
[defaults_JobTime]

#  Dictionary containing timestamps for job
#timestamps = {}


#=======================================================================
#  default attribute values for JobTree objects
[defaults_JobTree]


#name = 


#=======================================================================
#  default attribute values for LCG objects
[defaults_LCG]

#  Request a specific Computing Element
#CE = 

#  Job type: Normal, MPICH
#jobtype = Normal

#  Middleware type
#middleware = GLITE

#  Enable the job perusal feature of GLITE
#perusable = False

#  Requirements for the resource selection
#requirements = None

#  Interface for handling oversized input sandbox
#sandboxcache = None


#=======================================================================
#  default attribute values for LCGFileIndex objects
[defaults_LCGFileIndex]

#  a key:value pairs of file metadata
#attributes = {}

#  the main identity of the file
#id = 

#  the LFC hostname
#lfc_host = 

#  the original file path on local machine
#local_fpath = 

#  the md5sum of the file
#md5sum = 

#  the name of the file
#name = 


#=======================================================================
#  default attribute values for LCGRequirements objects
[defaults_LCGRequirements]

#  allowed CEs in regular expression
#allowedCEs = 

#  Minimum available CPU time (min)
#cputime = 0

#  A list of strings giving the available DataAccessProtocol protocols
#dataaccessprotocol = ['gsiftp']

#  The DataRequirements entry for the JDL. A list of dictionaries, each with
#  "InputData", "DataCatalogType" and optionally "DataCatalog" entries
#datarequirements = []

#  excluded CEs in regular expression
#excludedCEs = 

#  External connectivity
#ipconnectivity = False

#  Mininum available memory (MB)
#memory = 0

#  Number of Nodes for MPICH jobs
#nodenumber = 1

#  Other Requirements
#other = []

#  Software Installations
#software = []

#  Mimimum available total time (min)
#walltime = 0


#=======================================================================
#  default attribute values for LCGSEFile objects
[defaults_LCGSEFile]

#  the LCG SE SURL
#SURL = 

#  wheather the output file should be compressed before sending somewhere
#compressed = False

#  outputdir of the job with which the outputsandbox file object is associated
#joboutputdir = 

#  the LCG LFC hostname
#lfc_host = lfc-dteam.cern.ch

#  local dir where the file is stored, used from get and put methods
#localDir = 

#  list of locations where the outputfiles were uploaded
#locations = []

#  pattern of the file name
#namePattern = 

#  the LCG SE port
#port = 

#  the LCG SE hostname
#se = srm-public.cern.ch

#  the relative path to the file from the VO directory on the SE
#se_rpath = 

#  the LCG SE type
#se_type = 

#  the SRM space token, meaningful only when se_type is set to srmv2
#srm_token = 


#=======================================================================
#  default attribute values for LCGSandboxCache objects
[defaults_LCGSandboxCache]

#  the LCG LFC hostname
#lfc_host = 

#  max. number of tries in case of failures
#max_try = 1

#  file transfer protocol
#protocol = 

#  the LCG SE hostname
#se = 

#  the relative path to the VO directory on the SE
#se_rpath = generated

#  the LCG SE type
#se_type = srmv1

#  the SRM space token, meaningful only when se_type is set to srmv2
#srm_token = 


#=======================================================================
#  default attribute values for LSF objects
[defaults_LSF]

#  extra options for Batch. See help(Batch) for more details
#extraopts = 

#  queue name as defomed in your local Batch installation
#queue = 


#=======================================================================
#  default attribute values for Local objects
[defaults_Local]

#  adjust process priority using nice -n command
#nice = 0


#=======================================================================
#  default attribute values for LocalFile objects
[defaults_LocalFile]

#  wheather the output file should be compressed before sending somewhere
#compressed = False

#  local dir where the file is stored, used from get and put methods
#localDir = 

#  pattern of the file name
#namePattern = 


#=======================================================================
#  default attribute values for MCTask objects
[defaults_MCTask]

#  comment of the task
#comment = 

#  Evgen Transform
#evgen = None

#  Number of Jobs run concurrently
#float = 0

#  Name of the Task
#name = NewTask

#  Recon Transform
#recon = None

#  Resubmit only if the number of running jobs is less than "resub_limit" times
#  the float. This makes the job table clearer, since more jobs can be submitted
#  as subjobs.
#resub_limit = 0.9

#  Simul Transform
#simul = None

#  Total number of events to generate
#total_events = 1000

#  list of transforms
#transforms = []


#=======================================================================
#  default attribute values for MassStorageFile objects
[defaults_MassStorageFile]

#  wheather the output file should be compressed before sending somewhere
#compressed = False

#  Directory on mass storage where the file is stored
#inputremotedirectory = None

#  outputdir of the job with which the outputsandbox file object is associated
#joboutputdir = 

#  local dir where the file is stored, used from get and put methods
#localDir = 

#  list of locations where the outputfiles are uploaded
#locations = []

#  pattern of the file name
#namePattern = 

#  keyword path to where the output should be uploaded, i.e.
#  /some/path/here/{jid}/{sjid}/{fname},
#  if this field is not set, the output will go in {jid}/{sjid}/{fname} or in
#  {jid}/{fname}
#  depending on whether the job is split or not
#outputfilenameformat = None


#=======================================================================
#  default attribute values for MetadataDict objects
[defaults_MetadataDict]


#=======================================================================
#  default attribute values for MultiPostProcessor objects
[defaults_MultiPostProcessor]


#=======================================================================
#  default attribute values for MultiTask objects
[defaults_MultiTask]

#  comment of the task
#comment = 

#  Number of Jobs run concurrently
#float = 0

#  Name of the Task
#name = NewTask

#  Resubmit only if the number of running jobs is less than "resub_limit" times
#  the float. This makes the job table clearer, since more jobs can be submitted
#  as subjobs.
#resub_limit = 0.9

#  list of transforms
#transforms = []


#=======================================================================
#  default attribute values for MultiTransform objects
[defaults_MultiTransform]

#  Split by total input filesize (cf DQ2JobSplitter.filesize)
#MB_per_job = 0

#  Application of the Transform. Must be a Task-Supporting application.
#application = None

#  Backend of the Transform.
#backend = None

#  Trigger automatic dq2 download of related datasets
#do_auto_download = False

#  files per job (cf DQ2JobSplitter.numfiles)
#files_per_job = 5

#  Run the merger per unit rather than on the whole transform
#individual_unit_merger = False

#  Input dataset
#inputdata = None

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Local files downloaded/merged by the completed transform
#local_files = {'merge': [], 'dq2': []}

#  Local merger to be done over all units when complete.
#merger = None

#  Name of the transform (cosmetic)
#name = Simple Transform

#  Output dataset
#outputdata = None

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  If the maximum number of job failures is hit, rebroker the unit
#rebroker_on_job_fail = False

#  ID of Transform that this should follow
#required_trfs = []

#  Reduce to a single unit that runs over the all outputs from all required trfs
#single_unit = False

#  Splitter to be used for this transform. Note that split options specified in
#  the transform WILL NOT overwrite these settings.
#splitter = None

#  split into this many subjobs per unit master job (cf
#  DQ2JobSplitter.numsubjobs)
#subjobs_per_unit = 0


#=======================================================================
#  default attribute values for Notifier objects
[defaults_Notifier]

#  Email address
#address = 

#  Email on subjob completion
#verbose = False


#=======================================================================
#  default attribute values for PBS objects
[defaults_PBS]

#  extra options for Batch. See help(Batch) for more details
#extraopts = 

#  queue name as defomed in your local Batch installation
#queue = 


#=======================================================================
#  default attribute values for Panda objects
[defaults_Panda]

#  EXPERT ONLY
#accessmode = 

#  String for Executable make command - if filled triggers a build job for the
#  Execuatble
#bexec = 

#  define extra output files, e.g. ['output1.txt','output2.dat']
#extOutFile = []

#  Force staging of input DS
#forcestaged = False

#  Create individual output dataset for each data-type. By default, all output
#  files are added to one output dataset
#individualOutDS = False

#  Existing Library dataset to use (disables buildjob)
#libds = None

#  Boolean if no build job should be sent - use it together with
#  Athena.athena_compile variable
#nobuild = False

#  Requirements for the resource selection
#requirements = None

#  Require the job to run at a specific site
#site = AUTO


#=======================================================================
#  default attribute values for PandaBuildJob objects
[defaults_PandaBuildJob]

#  Panda Job id
#id = None

#  Panda Job status
#status = None


#=======================================================================
#  default attribute values for PandaMergeJob objects
[defaults_PandaMergeJob]

#  Panda Job id
#id = None

#  Panda Job status
#status = None


#=======================================================================
#  default attribute values for PandaPilot objects
[defaults_PandaPilot]

#  Pilot argument pattern. Do not modify unless you know what you are doing.
#arg_pattern = -s %%QUEUE%% -h %%QUEUE%% -j false -u self

#  File object containing the pilot startup script. Do not modify unless you
#  know what you are doing.
#exe = File(name='/afs/cern.ch/sw/ganga/install/6.1.9-pre/python/GangaPanda/Lib/Athena/runpilot3-script-stub.sh',subdir='.')

#  The analysis queue to register the pilot on e.g. ANALY_TRIUMF
#queue = 


#=======================================================================
#  default attribute values for PandaRequirements objects
[defaults_PandaRequirements]

#  Set to true to allow jobs to run in all clouds. If False, jobs are limited to
#  run in "requirements.cloud"
#anyCloud = True

#  cloud where jobs are submitted (default:US)
#cloud = US

#  Config string for the Job Execution Monitor.
#configJEM = 

#  Config parameters for output merging jobs.
#configMerge = {'type': '', 'exec': ''}

#  Enable a checker to skip corrupted files
#corCheck = False

#  Required CPU count in seconds. Mainly to extend time limit for looping
#  detection
#cputime = -1

#  Enable the Job Execution Monitor.
#enableJEM = False

#  Enable the output merging jobs.
#enableMerge = False

#  Clouds to be excluded while brokering.
#excluded_clouds = []

#  Panda sites to be excluded while brokering.
#excluded_sites = []

#  Send the job using express quota to have higher priority. The number of
#  express subjobs in the queue and the total execution time used by express
#  subjobs are limited (a few subjobs and several hours per day, respectively).
#  This option is intended to be used for quick tests before bulk submission.
#  Note that buildXYZ is not included in quota calculation. If this option is
#  used when quota has already exceeded, the panda server will ignore the option
#  so that subjobs have normal priorities. Also, if you submit 1 buildXYZ and N
#  runXYZ subjobs when you only have quota of M (M < N),  only the first M
#  runXYZ subjobs will have higher priorities
#express = False

#  Send job to a long queue
#long = False

#  Required memory size
#memory = -1

#  If input files are not read from SE, they will be skipped by default. This
#  option disables the functionality
#notSkipMissing = False

#  Expert option: overwriteQueuedata.
#overwriteQueuedata = False

#  Expert option: overwriteQueuedataConfig.
#overwriteQueuedataConfig = 

#  Specify a different root version for non-Athena jobs.
#rootver = 

#  Expert option: transfertype.
#transfertype = 

#  Boolean if input.txt contains the input files as comma separated list or
#  separeted by a line break
#usecommainputtxt = False


#=======================================================================
#  default attribute values for ProdTrans objects
[defaults_ProdTrans]

#  CMTCONFIG environment variable
#atlas_cmtconfig = 

#  ATLAS Software Release
#atlas_release = 


#core_count = 0

#  ATLAS DB Release. Use LATEST for most recent
#dbrelease = LATEST

#  Dataset name for the DB
#dbrelease_dataset = 

#  Home package
#home_package = 

#  Type of the input file
#input_type = 

#  Location of shared resources. Presence of this attribute implies the
#  application has been prepared.
#is_prepared = None

#  Addtional parameters for the job
#job_parameters = 


#max_events = 0

#  Logical File Names for output
#output_files = []

#  Type of the output file
#output_type = ['NTUP_TOP']

#  Initialial priority for the Job
#priority = 1000

#  Production Source Label
#prod_source_label = 

#  Generate output file names with random suffix
#randomize_lfns = False

#  Transformation
#transformation = 


#=======================================================================
#  default attribute values for ReconTransform objects
[defaults_ReconTransform]

#  Application of the Transform. Must be a Task-Supporting application.
#application = None

#  Backend of the Transform.
#backend = None

#  Input dataset
#inputdata = None

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Name of the transform (cosmetic)
#name = Simple Transform

#  Output dataset
#outputdata = None

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  random seeds to be used in the partition
#random_seeds = {}


#=======================================================================
#  default attribute values for Remote objects
[defaults_Remote]

#  Overides any environment variables set in the job
#environment = {}

#  Command line to start ganga on the remote host
#ganga_cmd = 

#  The directory to use for the remote workspace, repository, etc.
#ganga_dir = 

#  The remote host and port number ('host:port') to use. Default port is 22.
#host = 

#  Set to the type of ssh key to use (if required). Possible values are 'RSA'
#  and 'DSS'.
#key_type = RSA

#  Sequence of commands to execute before running Ganga on the remote site
#pre_script = ['']

#  specification of the resources to be used (e.g. batch system)
#remote_backend = None

#  Set to true to the location of the the ssh key to use for authentication,
#  e.g. /home/mws/.ssh/id_rsa. Note, you should make sure 'key_type' is also set
#  correctly.
#ssh_key = 

#  The username at the remote host
#username = 


#=======================================================================
#  default attribute values for Root objects
[defaults_Root]

#  List of arguments for the script. Accepted types are numerics and strings
#args = []

#  Location of shared resources. Presence of this attribute implies the
#  application has been prepared.
#is_prepared = None

#  A File object specifying the script to execute when Root starts
#script = File(name='/afs/cern.ch/sw/ganga/install/6.1.9-pre/python/Ganga/Lib/Root/defaultRootScript.C',subdir='.')

#  Execute 'script' using Python. The PyRoot libraries are added to the
#  PYTHONPATH.
#usepython = False

#  The version of Root to run
#version = 6.04.02


#=======================================================================
#  default attribute values for RootFileChecker objects
[defaults_RootFileChecker]

#  Run on master
#checkMaster = True

#  Toggle whether to check the merging proceedure
#checkMerge = True

#  Run on subjobs
#checkSubjobs = True

#  File to search in
#files = []

#  Toggle whether to fail job if a file isnt found.
#filesMustExist = True


#=======================================================================
#  default attribute values for RootMerger objects
[defaults_RootMerger]

#  Arguments to be passed to hadd.
#args = None

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for RootMergerAANT objects
[defaults_RootMergerAANT]

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for SGE objects
[defaults_SGE]

#  extra options for Batch. See help(Batch) for more details
#extraopts = 

#  queue name as defomed in your local Batch installation
#queue = 


#=======================================================================
#  default attribute values for SandboxFile objects
[defaults_SandboxFile]

#  wheather the output file should be compressed before sending somewhere
#compressed = False

#  local dir where the file is stored, used from get and put methods
#localDir = 

#  pattern of the file name
#namePattern = 


#=======================================================================
#  default attribute values for ShareDir objects
[defaults_ShareDir]

#  path to the file source
#name = 

#  destination subdirectory (a relative path)
#subdir = .


#=======================================================================
#  default attribute values for ShareRef objects
[defaults_ShareRef]


#=======================================================================
#  default attribute values for SimulTransform objects
[defaults_SimulTransform]

#  Application of the Transform. Must be a Task-Supporting application.
#application = None

#  Backend of the Transform.
#backend = None

#  Input dataset
#inputdata = None

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Name of the transform (cosmetic)
#name = Simple Transform

#  Output dataset
#outputdata = None

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []

#  random seeds to be used in the partition
#random_seeds = {}


#=======================================================================
#  default attribute values for SmartMerger objects
[defaults_SmartMerger]

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for StagerDataset objects
[defaults_StagerDataset]

#  Dataset Name(s) or a root path in which the dataset files are located
#dataset = []

#  Data type: DATA, MC or MuonCalibStream
#datatype = 

#  GUID of Logical File Names
#guids = []

#  Logical File Names to use for processing
#names = []

#  Dataset type: DQ2 (refer to a DQ2 dataset), CASTOR/DPM (refer to a castor/dpm
#  path) or LOCAL (refer to a local directory)
#type = DQ2

#  Sets to True for staging the dataset files via xrootd protocol.
#xrootd_access = False


#=======================================================================
#  default attribute values for StagerJobSplitter objects
[defaults_StagerJobSplitter]

#  Number of files per subjob
#numfiles = 0


#=======================================================================
#  default attribute values for TNTJobSplitter objects
[defaults_TNTJobSplitter]

#  Username from authentication.xml
#auth_name = 

#  Password for username from authentication.xml
#auth_pass = 

#  Logfile name
#logfile = TNT.log-22657

#  Run jobs on CEs with local dataset
#match_ce = False

#  Minimum number of events per sub collection
#minevents = 0

#  Database query to execute
#query = 

#  Source collection name
#src_collection_name = 

#  Source collection type
#src_collection_type = RootCollection

#  Source connection string
#src_connection_string = 


#=======================================================================
#  default attribute values for TagPrepare objects
[defaults_TagPrepare]

#  ATLAS CMTCONFIG environment variable
#atlas_cmtconfig = 

#  ATLAS Software Release
#atlas_release = 

#  Prepare an LCG TAG submission rather than Panda
#lcg_prepare = False

#  The maximum number of references retrieve per TAG file (defaults to 3).
#max_num_refs = 3

#  The stream reference to use (AOD, ESD).
#stream_ref = AOD

#  Returned TAG info
#tag_info = {}


#=======================================================================
#  default attribute values for Task objects
[defaults_Task]

#  comment of the task
#comment = 

#  Number of Jobs run concurrently
#float = 0

#  Name of the Task
#name = NewTask

#  Resubmit only if the number of running jobs is less than "resub_limit" times
#  the float. This makes the job table clearer, since more jobs can be submitted
#  as subjobs.
#resub_limit = 0.9

#  list of transforms
#transforms = []


#=======================================================================
#  default attribute values for TaskChainInput objects
[defaults_TaskChainInput]

#  List of Regular expressions of which files to exclude for input
#exclude_file_mask = []

#  List of Regular expressions of which files to include for input
#include_file_mask = []

#  Input Transform ID
#input_trf_id = -1

#  Create a single unit from all inputs in the transform
#single_unit = False

#  Use the copied output instead of default output (e.g. use local copy instead
#  of grid copy)
#use_copy_output = True


#=======================================================================
#  default attribute values for TaskLocalCopy objects
[defaults_TaskLocalCopy]

#  List of Regular expressions of which files to exclude from copy
#exclude_file_mask = []

#  List of successfully downloaded files
#files = []

#  List of Regular expressions of which files to include in copy
#include_file_mask = []

#  Local location to copy files to
#local_location = 


#=======================================================================
#  default attribute values for TextMerger objects
[defaults_TextMerger]

#  Output should be compressed with gzip.
#compress = False

#  A list of files to merge.
#files = []

#  Jobs that are in the failed or killed states will be excluded from the merge
#  when this flag is set to True.
#ignorefailed = False

#  The default behaviour for this Merger object. Will overwrite output files.
#overwrite = False


#=======================================================================
#  default attribute values for Transform objects
[defaults_Transform]

#  Application of the Transform. Must be a Task-Supporting application.
#application = None

#  Backend of the Transform.
#backend = None

#  Input dataset
#inputdata = None

#  list of File objects shipped to the worker node
#inputsandbox = []

#  Name of the transform (cosmetic)
#name = Simple Transform

#  Output dataset
#outputdata = None

#  list of filenames or patterns shipped from the worker node
#outputsandbox = []


#=======================================================================
#  default attribute values for VomsCommand objects
[defaults_VomsCommand]

#  Command for destroying credential
#destroy = voms-proxy-destroy

#  Dictionary of parameter-value pairs to pass to destroy command
#destroy_parameters = {}

#  Command for obtaining information about credential
#info = voms-proxy-info

#  Dictionary mapping from Ganga credential properties to command-line options
#info_parameters = {'vo': '-vo'}

#  Command for creating/initialising credential
#init = voms-proxy-init

#  Dictionary of parameter-value pairs to pass to init command
#init_parameters = {'pipe': '-pwstdin', 'voms': '-voms', 'valid': '-valid'}


