\documentclass{howto}

\RequirePackage{xspace}

\def\ganga {\textsc{Ganga}\xspace}
\def\python {\textsc{Python}\xspace}
\def\ipython {\textsc{IPython}\xspace}
\def\root {\textsc{ROOT}\xspace}
\def\lhcb {LHC{\em b\/}\xspace}
\def\gaudi {\textsc{Gaudi}\xspace}
\def\davinci {\textsc{DaVinci}\xspace}
\def\dirac {\textsc{Dirac}\xspace}
\def\gauss {\textsc{Gauss}\xspace}
\def\majorv {4}
\def\minorv {3.7}
\def\totalv {\majorv.\minorv}
\def\release{\ganga-\totalv}

\title{\lhcb specific manual for Ganga \totalv}

% At minimum, give your name and an email address.  You can include a
% snail-mail address if you like.
\author{Ulrik Egede}
\authoraddress{U.Egede@imperial.ac.uk}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
\noindent
This manual describes the \lhcb specific plugins for \ganga. It covers the
area of how to create a \ganga job with a \gaudi application, how to submit
jobs to the \dirac production system and how to select data for analysis in
\lhcb.

\begin{seealso}
  The Working with \ganga guide is where you will find an overall introduction
  to \ganga. It should be read before this document.
  \seeurl{http://ganga.web.cern.ch/ganga/user/}{}
\end{seealso}

\begin{seealso}
  An \lhcb specific reference manual is available at 
  \seeurl{http://ganga.web.cern.ch/ganga/release/4.3.7/reports/html/Manuals/GangaLHCbManual.html}{}
  with the exact release you are using substituted into the URL. the text you
  get in the reference manual is identical to the help text you get on the
  command line in \ganga but might be more convenient for reading.
\end{seealso}

\section{Applications}
\label{sec:gaudi}
In \ganga you can specify any \gaudi application directly as the appliaction
for your job. Specific versions of the handler are available for all the
different \gaudi applliactions like \gauss and \davinci. Since they all work
in the same way we will only describe \davinci here.

The example below illustrates the creation of a job where the version of
\davinci is changed and the options file specified.
\begin{verbatim}
In [1]: dv = DaVinci()
In [2]: dv
Out[2]: DaVinci {
 version = 'v14r5' ,
 extraopts = None ,
 package = 'Phys' ,
 cmt_user_path = '/afs/cern.ch/user/u/uegede/cmtuser' ,
 masterpackage = None ,
 optsfile = File {
    name = '' ,
    subdir = '.'
    }
 }

In [3]: dv.version='v12r4'
In [4]: dv.optsfile='~/myopts.opts'
In [5]: j = Job(application=dv)
\end{verbatim}
This can of course all be done in a single line.
\begin{verbatim}
j = Job(application=DaVinci(version='v12r16', optsfile='~/myopts.opts'))
\end{verbatim}

\subsection{The parameters of the \gaudi (\gauss, \davinci, etc.) handlers}
\label{sec:GaudiParameters}
The \gaudi applications each have a set of parameters that here are explained
for reference. The examples in section~\ref{sec:Example} are good for
understanding how they can be used.
\begin{description}
\item[version] This is the version of \davinci (or whatever other \gaudi
  application) that will be used.
\item[optsfile] This is the location of your top level options file. Give the
  absolute path and do not change the value of the \texttt{subdir} field.
  Include statements in the file will be expanded at submission time and a
  full copy made.
\item[extraopts] In this field you can add extra options that will be appended
  to the end of your options file before running. This is very convienient for
  changing a single parameter before running a job.
\begin{verbatim}
j.application.extraopts="""
ApplicationMgr.HistogramPersistency = "ROOT";
HistogramPersistencySvc.OutputFile = "DVHistos_2.root";

ApplicationMgr.EvtMax = 500  ;
EventSelector.FirstEvent = 0 ;
"""
\end{verbatim}
  Notice the use of triple quotes in \python to specify a multiline option.
\item[cmt_user_path] The \texttt{CMT} user path to be used. By default the
  value of the first element in the \texttt{CMTPROJECTPATH} environment
  variable.  After assigning this you can do 
\begin{verbatim}
j.application.getpack('Phys/DaVinci v19r2') 
\end{verbatim}
  to check out into the new location. If you work on several simultaneous
  projects it is advisable that you keep them in separate \texttt{CMT} areas.
\item[masterpackage] The package where your top level requirements file is
  read from. Can be written either as a path \texttt{Tutorial/Analysis/v6r0}
  or in a \texttt{CMT} style notation \texttt{Analysis v6r0 Tutorial}.
\end{description}

\subsection{Job submission and running}
\label{sec:GaudiConfigAndRun}
When a \ganga job with a \gaudi application is submitted several things will
happen.
\begin{itemize}
\item \texttt{CMT} is used to identify the packages in your local \texttt{CMT}
  area that the job depends on. The requirements file in your
  \texttt{masterpackage} will be the one that determines this.
\item All the include statements in the option files are recursively expanded
  and the fully expanded options file is cached with the job.
\item A copy of all shared libraries used in your local \texttt{CMT} area is
  made.
\end{itemize}
The purpose of the above oprations is to make your job independent of your
\texttt{CMT} area. This means that after you have submitted a job (and before
it has finished) you can edit your options files, rebuild the code etc.\
without putting your running job at risk. This is something that is not
possible if running from the command line. If you are curious about the cached
files that are created (or suspect a bug in the implementation) you can find
them in the directory \texttt{j.inputdir} where \texttt{j} is your job. Simply 
unpack the two tar files and see what is there.


\subsection{Helper functions}
\label{sec:gaudiHelpers}
The \gaudi application handlers provide functions to make it easier to check
out and build code when you are working with multiple different versions and
release areas (so no more fiddling with setting \texttt{CMTPATH} by hand and
remembering to do the correct \texttt{DaVinciEnv} before you build the code.
Look at the example below where the CMT area is in my
\texttt{public/cmtTutorial} directory and the package
\texttt{Tutorial/Analysis v5} is checked out and built.
\begin{verbatim}
In [11]:dv = DaVinci(version='v12r15', 
                     cmt_user_path='~/tmp/public/cmtTutorial',
                     masterpackage='Tutorial/Analysis/v5')
 
In [12]:dv.getpack('Tutorial/Analysis v5')
:
getpack v4r0
cvs -d :kserver:isscvs.cern.ch:/local/reps/lhcb co -d gaudi-req-190403 Tutorial/Analysis/cmt/requirements
U gaudi-req-190403/requirements
--------------------------------------------------------------------------------
cvs -d :kserver:isscvs.cern.ch:/local/reps/lhcb co -P -d v5 -r v5 Tutorial/Analysis
:

< edit your code >

In [13]:dv.make()
:
#--------------------------------------------------------------
# Now trying [make] in /afs/cern.ch/user/u/uegede/tmp/public/cmtTutorial/Tutorial/Analysis/v5/cmt (191/192)
#---------------------------------------------------------------
:
------> Analysis : library ok
------> Analysis ok
------> (constituents.make) Analysis done
 all ok.
\end{verbatim}
The full signature of the helper functions are:
\begin{description}
\item[getpack(options)] Execute a getpack command \texttt{getpack
    $<$options$>$} with \texttt{CMT} configured to use the specified CMT user
  area. To see the arguments you can include in the \texttt{getpack}
    command give it the argument \texttt{-h} as in \texttt{dv.getpack('-h')}.
\item[make(argument)] Build the code in the release area the application
  object points to. The actual command executed is \texttt{cmt broadcast make
    $<$argument$>$} after the proper configuration has taken place. An example
  is \texttt{j.application.make()} to do a broadcast make or
  \texttt{j.application.make('clean')} to clean up.
\item[cmt(command)] Execute a general cmt command in the cmt user area pointed
  to by the application. Will execute the command \texttt{cmt $<$command$>$}
  after the proper configuration. Do not include the word \texttt{cmt}
  yourself. An example is \texttt{j.application.cmt('show uses')}.
\end{description}

The CMT commands are related to the CMT area that the application points to.
If you perform a \texttt{getpack}, a \texttt{make} or whatever using these
commands it will affect all jobs where the application points to this area.

\begin{notice}
  If a job has already been submitted shared libraries and options are cached
  and they will not be affected by whatever CMT commands you might use. This
  is the case even if the job is still pending in a batch queue.
\end{notice}

\subsection{\root jobs}
\label{sec:ROOT}
there is a \root application handler in \ganga that make it possible to create
jobs for running within \root. As a user you will specify the \textsc{CINT} or
\textsc{pyRoot} script to run and then the rest is taken care of. The \root
application works with all backends. See the comprehensive documentation in
the reference manual for further details.

\section{Backends}
\label{sec:backends}

\subsection{\dirac}
\label{sec:dirac}
\begin{notice}
  As a pre-requisite you need to have a Grid certificate, have your
  \texttt{.globus} directory correctly confugured and be a member of the
  \lhcb virtual organisation (VO).
\end{notice}
To run jobs on the Grid you need to use the \dirac backend. For input data you
will have to specify it as logical filenames~(LFNs) rather than the physical
filenames~(PFNs) that are used if you run on specific local files. The bookkeeping database
returns LFNs by default so this should be easy.
\begin{seealso}
  In addition to the monitoring within \ganga, you can monitor the progress on
  the \dirac monitoring webpage 
  \seeurl{http://lhcb.pic.es/DIRAC/Monitoring/Analysis/}{}. To get the
  corresponding id look at \texttt{j.backend.id} for your \ganga job.
\end{seealso}

\subsection{LCG}
\label{sec:lcg}
You can submit jobs \gaudi jobs directly to \texttt{LCG} rather than
through the \dirac Workload Management System. The implementation is
very basic at the moment and has no concept of sending the job to a
location where the data is nearby. It simply goes to an LCG site with
the \lhcb software installed and then copy the data to local disk
before starting to run. All output is returned in the output
sandbox. \textbf{The recommended way of doing analysis on the grid is
thorugh \dirac} but comparisons to direct \texttt{LCG} are
acceptable. Inputdata should be specified as LFNs in the same way as
for the \dirac backend. In general there will not be any support
provided within \lhcb for direct submission to LCG.


\section{Input and Output}
\label{sec:InOut}

\subsection{Input sandbox}
\label{sec:Inputsandbox}
The files specified as a list in the \texttt{inputsandbox} attribute of a jobs
describes the files that will be copied to the local execution environment.
Options files and shared libraries are taken care of by \ganga so you should
not specify them.

\subsection{Input data}
\label{sec:datasets}
The data that your \gaudi job will read should be specified as an
\texttt{LHCbDataset} in the \texttt{inputdata} attribute of your job.

The following will create a dataset object with logical filenames from the
stripped DC04v2 dataset:
\begin{verbatim}
In [22]: dataLFN = LHCbDataset(files=[
'LFN:/lhcb/production/DC04/v2/00980000/DST/Presel_00980000_00001212.dst' ,
'LFN:/lhcb/production/DC04/v2/00980000/DST/Presel_00980000_00001245.dst' ,
'LFN:/lhcb/production/DC04/v2/00980000/DST/Presel_00980000_00001249.dst' ,
'LFN:/lhcb/production/DC04/v2/00980000/DST/Presel_00980000_00001215.dst' ,
'LFN:/lhcb/production/DC04/v2/00980000/DST/Presel_00980000_00001252.dst' ,
'LFN:/lhcb/production/DC04/v2/00980000/DST/Presel_00980000_00001246.dst' ,
'LFN:/lhcb/production/DC04/v2/00980000/DST/Presel_00980000_00001248.dst'])
\end{verbatim}
There is no need to enter the same lines into your options file as \ganga will
take care of this at submission time.

If you create a \davinci job in \ganga without specifying an input dataset in
the \texttt{j.inputdata} attribute, the input data will be extracted from the
options file as it will happen if you run a \davinci job outside \ganga. The
specification of inputdata in the options file is left for backwards
compatibility but will eventually disappear. This is to ensure a clear
separation between the configuration of the application and the data it will
process.

\begin{notice}
  The dataset defined in the \texttt{inputdata} field will take precedence
  over what is in the options file. So if you have a specification in both
  places, anything in the options file will be ignored.
\end{notice}

\subsection{OutputSandbox}
\label{sec:OutputSandbox}
When a job has finished it will copy its output back to the local file
workspace (by default \texttt{$\tilde{}$/gangadir/workspace}). the
\texttt{outputdir} attribute will give you the exact location. As an example:
\begin{verbatim}
In [4]:j.outputdir
Out[4]: /afs/cern.ch/user/u/uegede/gangadir/workspace/Remote/143/output/
\end{verbatim}

Normally you should just leave this field empty and all output files from your
\gaudi job will get copied back. For \root jobs you need to specify everything
apart from standard output and standard error.

To look into the output sandbox it is very convenient to use the \texttt{peek}
method on a job \texttt{j}.
\begin{verbatim}
# Look at what is in the output sandbox
j.peek()

# Look in the input sandbox
j.peek( "../input" )

# View ROOT histograms, running root.exe in a new terminal window
j.peek( "histograms.root", "root.exe &&" )
\end{verbatim}

See the reference for the full documentation on what is possible with the
\texttt{peek} method and how it can be configured.

If the size of an output sandbox file with the \dirac backend exceeds 10~Mb it
will automatically be treated as output data instead and copied to a Grid
Storage Element.

\subsection{OutputData}
\label{sec:OutputData}
Rather than returning large files to your local file system you might want to
store them on a mass storage system instead. This is done by default for data
files created by \texttt{GaussTape}, \texttt{DigiWriter} or the
\texttt{DstWriter}. The location of files in the mass storage depends on the
backend used:
\begin{description}
\item[Local, LSF] Files are stored in the location
  \texttt{\$CASTOR_HOME/gangadir/$<$j.id$>$/outputdata/}
\item[Dirac] The files are registered in the LCG file catalogue~(LFC) and can
  be used as input to Grid jobs in exactly the same way as for other input
  data. The name of the file is stored in a structure similar to the home
  directories on afs at CERN:
\begin{verbatim}
      LFN:/lhcb/user/<initial>/<username>/<diracid>/<fname> 
\end{verbatim}
  The \dirac id is obtained as \texttt{j.backend.id}. We are currently working
  on an easy way to obtain a local copy of the output data.
\item[LCG] The direct \texttt{LCG} handler doesn't support the concept of
  output data at the moment.
\end{description}


\section{Job splitting}
\label{sec:splitting}
If you want to analyse a large dataset you can create a single \emph{master
  job} and then specify how much data should be analysed in each of a set of
\emph{sub jobs}. For \lhcb the \texttt{SplitByFiles} splitter is used for
this. The splitter takes a single argument which specifies the number of input
data files that each sub job will process. The following example will create a
\davinci master job with the default 10 data files per subjob.
\begin{verbatim}
In [31]:j = Job(application=dv, splitter=SplitByFiles())
\end{verbatim}

There is also a new splitter called \texttt{DiracSplitter}. This one works as
\texttt{SplitByFiles} but ensures that a given job will only have data that is
located at the same Grid site. This splitter only makes sense to use with the
Dirac backend though.

\section{Example of a \davinci analysis job}
\label{sec:Example}
The example here put together the pieces from above in a non-trivial way. It
will analyse a signal dataset with an algorithm from the Tutorial package of
\davinci. The first analysis will be on the local machine going through just
100 events, and afterwards on the Grid analysing many events with splitting
into many subjobs.

First we define a \davinci application. We place the code in a non-default
location to avoid interfering with other development work. We use a supplied
options file but just add a single line which limits the number of events
analysed.
\begin{verbatim}
topdir='~/public/cmtTutorial'
master='Tutorial/Analysis/v5'
tutdir=topdir+'/'+master+'/solution5'
dv = DaVinci(version='v12r15',
             cmt_user_path=topdir, 
             masterpackage=master)
dv.optsfile=tutdir+'/solution5/DVTutorial_5.opts'
dv.extraopts='ApplicationMgr.EvtMax = 100;'
t = JobTemplate(name='TutorialAnalysis',
                application=dv, 
                backend=Local())
\end{verbatim}
We then check out code from CVS and compile the package. Cheat by copying some
code from the solutions directory before we compile.
\begin{verbatim}
dv.getpack('Tutorial/Analysis v5')
!cp $tutdir/solution2/*.cpp $tutdir/solution2/*.h $tutdir/src
!cp $tutdir/solution5/*.cpp $tutdir/solution5/*.h $tutdir/src
dv.make()
\end{verbatim}
We now go on to define the dataset for the local analysis. This is simply
chosen from the bookkeeping database as $B_s \rightarrow J/\Psi \phi$, $\phi
\rightarrow \mu^+ \mu^-$ data located at CERN.
\begin{verbatim}
dataPFN = LHCbDataset(files=[
  'PFN:rfio:/castor/cern.ch/lhcb/DC04/00000543_00000001_5.dst',
  'PFN:rfio:/castor/cern.ch/lhcb/DC04/00000543_00000002_5.dst'])
\end{verbatim}
With this we create our local test job with the template, only adding our
dataset. Notice that if the files you specify are registered in the file
catalogue you can also use logical file names for local and batch submission.
\begin{verbatim}
j = Job(t, inputdata=dataPFN)
\end{verbatim}
While developing the code we might then go around in loops for the following
lines of submission, checking, editing and rebuilding.
\begin{verbatim}
j.submit()

< wait for job to finish >

j.peek('stdout')

< edit the code >

j.application.make()

< create new job >

j = Job(t, inputdata=dataLFN)
\end{verbatim}
When happy we can then create our analysis job for the Grid. We assing a
dataset with logical filenames instead (see above), change the number of
events to analyse and tell the job to split into subjobs with 1 dataset per
job.
\begin{verbatim}
dataLFN = LHCbDataset(files=[
  'LFN:/lhcb/production/DC04/v1/DST/00000671_00003129_5.dst',
  'LFN:/lhcb/production/DC04/v1/DST/00000671_00003228_5.dst',
  'LFN:/lhcb/production/DC04/v1/DST/00000671_00003216_5.dst',
  'LFN:/lhcb/production/DC04/v1/DST/00000671_00003222_5.dst',
  'LFN:/lhcb/production/DC04/v1/DST/00000671_00003207_5.dst',
  'LFN:/lhcb/production/DC04/v1/DST/00000671_00003090_5.dst',
  'LFN:/lhcb/production/DC04/v1/DST/00000671_00003220_5.dst'])
j = Job(t,inputdata=dataLFN, backend=Dirac())
j.application.extraopts='ApplicationMgr.EvtMax = 10000000;'
j.splitter=SplitByFiles(filesPerJob = 3)
j.submit()
\end{verbatim}
As the dataset we used here has 7 datafiles this will cause the job to split
into 7 subjobs on submission. We recommend to use 10 datasets per job as the
default but break it up further here to illustrate the functionality. When the
master job \texttt{j} is completed all its subjobs are completed.
\begin{verbatim}
for js in j.subjobs:
   js.peek('DVTuple.root','ls -sh')

 11K /afs/cern.ch/user/u/uegede/gangadir/workspace/Remote/14300001/output/DVTuple.root
 10K /afs/cern.ch/user/u/uegede/gangadir/workspace/Remote/14300002/output/DVTuple.root
 10K /afs/cern.ch/user/u/uegede/gangadir/workspace/Remote/14300003/output/DVTuple.root
 11K /afs/cern.ch/user/u/uegede/gangadir/workspace/Remote/14300004/output/DVTuple.root
 10K /afs/cern.ch/user/u/uegede/gangadir/workspace/Remote/14300005/output/DVTuple.root
 10K /afs/cern.ch/user/u/uegede/gangadir/workspace/Remote/14300006/output/DVTuple.root
 11K /afs/cern.ch/user/u/uegede/gangadir/workspace/Remote/14300007/output/DVTuple.root
\end{verbatim}

In the end lets store the job definition for our job on the Grid as a template
for easy use in a later \ganga session:
\begin{verbatim}
t = JobTemplate(j)
t.name='MyFirstGridAnalysis'
templates
\end{verbatim}

At the moment DIRAC is not supporting slc4 binaries, so any analysis job sent to DIRAC has to have its shared libraries built in slc3 mode. The instructions here deal with how to do that even if you are logged into a slc4 machine at CERN or elsewhere.

\section{Submitting DC06 analysis jobs to DIRAC from slc4 machines}
At the moment DIRAC is not supporting slc4 binaries, so any analysis
job sent to DIRAC has to have its shared libraries built in slc3
mode. See the transcript below for how to do this.

\begin{verbatim}
# Login to lxplus or some other slc4 or rhel4 machine.
ssh lxplus.cern.ch

# fake the slc3 architecture
lbcmt gcc323

# Define Ganga release as usual. GangaEnv now takes care to set the correct
# version of python needed.

GangaEnv 4.4.2

# Start Ganga
ganga

# Define a job
j=Job(application=DaVinci(version='v19r3'), backend=Dirac(CPUTime=3600))

# Check out some packages
j.application.getpack('Phys/DaVinci v19r3')
j.application.getpack('Tutorial/Analysis v6r4')

# Modify your code as you like

# Clean build area
j.application.make('clean')

# Build application. The cpp=g++ option is special. If you forget it
# (and get horrible build errors) you have to do a make clean as above
# before you repeat with the option included. If you analyse DC04 data
# you can omit this special option.
j.application.make('cpp=g++') 

# Submit the job
j.submit()

# This slc3 mode also works locally so you can do
j=j.copy()
j.backend=LSF()
j.submit()
\end{verbatim}


\section{Getting help and reporting bugs}
The \texttt{lhcb-distributed-analysis@cern.ch} mailing list should be used for
all questions in \lhcb related to \ganga, \dirac and data access issues.
Questions might be help with debugging, advice on optimal use and reporting of
suspected bugs. Please subscribe to this list so you can stay informed and
help other users as well.

As with all other software reporting of bugs is an essential task any user can
help with. To report a bug to \ganga go to the page in Savannah.
\begin{seealso}
\seeurl{https://savannah.cern.ch/bugs/?group_id=195}{}
\end{seealso}
Check if the bug is already reported, and if not submit a new one. Please
include maximal information about how to reproduce the bug and if you include
output from the command line turn debug information on first (just start
\ganga with the \texttt{--debug} option).

Please report errors in the documentation as well!
\end{document}
