#!/bin/bash

"exec" "python" "$0" "$@"


def _usage():
    print \
"""
NAME
	dq2_get - provide access to DQ2 datasets 

SYNOPSIS

	dq2_get [ -h | --help]
                [ -v | --verbose ]
                [ -n | --nocopy ]
                [ -r | --remote ]
                [ -c | --choose ]
                [ -l | --lcg ]
                [ -p | --parallel n ]                
                [ -t | --timeout n ]                
                [ -d | --destination destination ]
                [ -s | --source sourceSite ]
                datasetname
                [lfn1 [lfn2 [...]]]
DESCRIPTION

	For datasets already present on a local storage element (SE) data is
        copied to the local directory or to another directory in the SE.

OPTIONS

        -h | --help		Print this message

	-v | --verbose		Verbosity

	-n | --nocopy           Do not copy any files

	-r | --remote           Copy files over the grid if files are not found in the local SE

	-c | --choose           Choose appropriate site automatically when copy files over the grid

	-p | --parallel         Number of copy threads (default:3)

	-t | --timeout          Timeout limit in second for each file transfer (default:1800)

	-l | --lcg              Access LCG datasets. This option will be removed once LCG prodsys
                                is integrated with DQ2

	-d | --destination      Directory in the storage element where files will be put.
                                Files will be copied to local directory, if omitted.

  	-s | --source           Specify source site from which files get copied
                              
EXAMPLES

        * Access a dataset by copying the data to the workernode

	dq2_get mc11.004205.pythia_W_tau_tauola.digit.RDO.v11000201_8569


	The data corresponding to the dataset are copied to the worker node


	* Access two files in a dataset using dcap

	dq2_get -p dcap mc11.004205.pythia_W_tau_tauola.digit.RDO.v11000201_8569 \\
            mc11.004205.pythia_W_tau_tauola.digit.RDO.v11000201._00195.pool.root.3 \\
            mc11.004205.pythia_W_tau_tauola.digit.RDO.v11000201._00187.pool.root.2


	* Access a dataset over grid (e.g., copy BNL data from CERN). Note that you need
          a grid proxy in this case

	dq2_get -r mc11.004205.pythia_W_tau_tauola.digit.RDO.v11000201_8569

"""

import re
import os
import sys
import time
import urllib
import Queue
import commands
import random
import threading
import xml.dom.minidom

# import lfc api
try:
    # ignore Python C API version mismatch
    sys.stderr = open("/dev/null", "w")
    # import
    import lfc
except:
    pass
# repair stderr
sys.stderr = sys.__stderr__


# error codes
EC_Configuration = 20
EC_VUID          = 30
EC_QueryFiles    = 40
EC_Location      = 50
EC_Copy          = 60
EC_Main          = 70
EC_PFNfromLFC    = 80


# configuration
try:
    # DQ2 server
    baseURLDQ2 = os.environ['DQ2_URL_SERVER']
except:
    print "ERROR : DQ2_URL_SERVER is not defined"
    sys.exit(EC_Configuration)
try:
    # local site ID
    DQ2LOCALSITEID = os.environ['DQ2_LOCAL_ID']
except:
    print "ERROR : DQ2_LOCAL_ID is not defined"
    sys.exit(EC_Configuration)
try:
    # local access protocol
    configLOCALPROTOCOL = os.environ['DQ2_LOCAL_PROTOCOL']
except:
    configLOCALPROTOCOL = 'rfio'
try:
    # local SRM host
    configSRMHOST = os.environ['DQ2_SRM_HOST']
except:
    configSRMHOST = ''
try:
    # local GSIFTP host
    configGSIFTPHOST = os.environ['DQ2_GSIFTP_HOST']
except:
    configGSIFTPHOST = ''    
try:
    # use SRM for all data transfer
    configUSESRM = False
    if re.match('True',os.environ['DQ2_USE_SRM'],re.I) != None:
        configUSESRM = True
except:
    pass
try:
    # root directory of storage
    configSTORAGEROOT = os.environ['DQ2_STORAGE_ROOT']
except:
    if configLOCALPROTOCOL == 'rfio':
        configSTORAGEROOT = '/castor'
    elif configLOCALPROTOCOL == 'dcap':
        configSTORAGEROOT = '/pnfs'
    else:
        configSTORAGEROOT = ''
try:
    # prefix for local access
    configLOCALPREFIX = os.environ['DQ2_LOCAL_PREFIX']
except:
    configLOCALPREFIX = ''
try:
    # remote copy command
    configCOPYCOMMAND = os.environ['DQ2_COPY_COMMAND']
except:
    configCOPYCOMMAND = ''


# global flags
globalVerbose = False
globalNocopy  = False
globalChoose  = False


# LFC map
lfcMap = {

    'IFAE'       : 'lfc01.pic.es',
    'QMUL'       : 'lfc0448.gridpp.rl.ac.uk',
    'DESY-HH'    : 'lfc-fzk.gridka.de',
    'TRIUMF'     : 'lfc.triumf.ca',
    'SINP'       : 'mu11.matrix.sara.nl',
    'CAM'        : 'lfc0448.gridpp.rl.ac.uk',
    'ALBERTA'    : 'lfc.triumf.ca',
    'IFIC'       : 'lfc01.pic.es',
    'EDINBURGH'  : 'lfc0448.gridpp.rl.ac.uk',
    'ASGC'       : 'lfc.grid.sinica.edu.tw',
    'NAPOLI'     : 'lfc-sc.cr.cnaf.infn.it',
    'MILANO'     : 'lfc-sc.cr.cnaf.infn.it',
    'RAL'        : 'lfc0448.gridpp.rl.ac.uk',
    'PIC'        : 'lfc01.pic.es',
    'SACLAY'     : 'lfc-atlas.in2p3.fr',
    'MANC'       : 'lfc0448.gridpp.rl.ac.uk',
    'UAM'        : 'lfc01.pic.es',
    'AU-UNIMELB' : 'lfc.grid.sinica.edu.tw',
    'LANCS'      : 'lfc0448.gridpp.rl.ac.uk',
    'DESY-ZN'    : 'lfc-fzk.gridka.de',
    'LNF'        : 'lfc-sc.cr.cnaf.infn.it',
    'CNAF'       : 'lfc-sc.cr.cnaf.infn.it',
    'CYF'        : 'lfc-fzk.gridka.de',
    'TORON'      : 'lfc.triumf.ca',
    'TOKYO'      : 'lfc-atlas.in2p3.fr',
    'OXF'        : 'lfc0448.gridpp.rl.ac.uk',
    'FZU'        : 'lfc-fzk.gridka.de',
    'FZK'        : 'lfc-fzk.gridka.de',
    'UVIC'       : 'lfc.triumf.ca',
    'TW-IPAS-T2' : 'lfc.grid.sinica.edu.tw',
    'CPPM'       : 'lfc-atlas.in2p3.fr',
    'AST2'       : 'lfc.grid.sinica.edu.tw',
    'SARA'       : 'mu11.matrix.sara.nl',
    'LPC'        : 'lfc-atlas.in2p3.fr',
    'GLASGOW'    : 'lfc0448.gridpp.rl.ac.uk',
    'LAL'        : 'lfc-atlas.in2p3.fr',
    'BEIJING'    : 'lfc-atlas.in2p3.fr',
    'LYON'       : 'lfc-atlas.in2p3.fr',
    'WUP'        : 'lfc-fzk.gridka.de',
    'CERN'       : 'prod-lfc-atlas-local.cern.ch',
    'LAPP'       : 'lfc-atlas.in2p3.fr',
    'SFU'        : 'lfc.triumf.ca',
    'ITEP'       : 'mu11.matrix.sara.nl',
    'ROMA1'      : 'lfc-sc.cr.cnaf.infn.it',
    'CSCS'       : 'lfc-fzk.gridka.de',
    'LPNHE'      : 'lfc-atlas.in2p3.fr',

    }


# URL map
urlMap = {
    'NDGF'   : 'http://cpt.uio.no:8000/dq2/',
    'BNL'    : 'http://dms02.usatlas.bnl.gov:8000/dq2/',
    'BU'     : 'http://atlas001.bu.edu:8000/dq2/',
    'UC'     : 'http://tier2-01.uchicago.edu:8000/dq2/',
    'UC_TP'  : 'http://tp-grid2.uchicago.edu:8000/dq2/',
    'UC_VOB' : 'http://tier2-05.uchicago.edu:8000/dq2/',
    'UTA'    : 'http://osg-itb2.dpcc.uta.edu:8000/dq2/',
    'IU'     : 'http://g3aux.avidd.iu.edu:8000/dq2/',
    'OU'     : 'http://ouhep00.nhn.ou.edu:8000/dq2/',
    'SLAC'   : 'http://yakut04.slac.stanford.edu:8000/dq2/',
    'AGLT2'  : 'http://umfs02.grid.umich.edu:8000/dq2/',
    }


# curl class
class _Curl:
    # constructor
    def __init__(self):
        # path to curl
        self.path = 'curl --user-agent "dqcurl"'
        # verification of the host certificate
        self.verifyHost = False


    # GET method
    def get(self,url,data={}):
        # make command
        com = '%s --silent --get' % self.path
        if not self.verifyHost:
            com += ' --insecure'
        # data
        for key in data.keys():
            com += ' --data "%s"' % urllib.urlencode({key:data[key]})
        com += ' %s' % url
        # execute
	if globalVerbose:
	    print com
        s,o = commands.getstatusoutput(com)
	if o != '\x00':
            try:
                tmpout = urllib.unquote_plus(o)
                o = eval(tmpout)
            except:
                pass
	ret = (s,o)
	if globalVerbose:
	    print ret
        return ret


# get VUID of a dataset
def _getVUID(name):
    # instantiate curl
    curl = _Curl()
    # get VUID
    url = baseURLDQ2 + 'ws_repository/dataset'
    data = {'dsn':name,'version':0}    
    status,out = curl.get(url,data)
    if status != 0:
        print status,out
        print "ERROR : could not retrieve VUID for %s from DQ2 server" % name
        sys.exit(EC_VUID)
    if out == '\x00' or (not out.has_key(name)):
        print status,out
        print "ERROR : %s was not found in the DQ2 catalog" % name
        sys.exit(EC_VUID)            
    return out[name]['vuids'][0]


# query files in dataset
def _queryFilesInDataset(vuid):
    # instantiate curl
    curl = _Curl()
    # get files
    url = baseURLDQ2 + 'ws_content/files'
    data = {'vuid':vuid}
    status,out =  curl.get(url,data)
    if status != 0:
        print status,out
        print "ERROR : could not get files for VUID=%s" % vuid
        sys.exit(EC_QueryFiles)
    # parse
    if out == '\x00':
        print "ERROR : no constituent files"
        sys.exit(EC_QueryFiles)
    ret = {}
    for guid,lfn in out.iteritems():
        ret[lfn] = guid
    return ret            


# query files in dataset for LCG
def _queryFilesInDataset_lcg(name,lfnPat):
    # check grid-proxy
    status,output = commands.getstatusoutput('grid-proxy-info -e')
    if status != 0:
        print "ERROR : No valid grid-proxy. Do 'grid-proxy-init'"
        sys.exit(1)
    # set LFC HOST
    os.environ['LFC_HOST'] = 'lfc-atlas-test.cern.ch'
    lfcRE = re.compile("((([a-z0-9]*)\.\d*\.\w*\.([_a-zA-Z0-9]*)\.?(\w*)\.\w*))")
    match = lfcRE.match( name )
    if match != None:
	toks=match.groups()
	path = os.sep.join([toks[2],toks[3],toks[1]])
	if len(toks)==5 and toks[4]=='log':
	    vuid="/grid/atlas/logfiles/"+path
	else:
	    vuid="/grid/atlas/datafiles/"+path
    else:
        print "could not find %s in prodsys LFC" % name
        print "This dataset may be in DQ2. Try again without '-l'"
        sys.exit(1)
    print vuid
    buff = 1000
    print "In _queryFilesInDataset_lcg"
    res=lfc.lfc_chdir( vuid )
    if res == 0:
	print "The " + vuid
    else:
	err_num = lfc.cvar.serrno
	err_string = lfc.sstrerror(err_num)
	print "There was an error while cding to " + vuid + ": Error " +\
              str(err_num) + " (" + err_string + ")"
        print "This dataset may be in DQ2. Try again without '-l'"
	sys.exit(1)
        
    lfc_dir = lfc.lfc_opendir( vuid )
    lfc_direntry = lfc.lfc_readdirxc( lfc_dir )
    files = {}
    pfns  = {}
    fsizeMap = {}
    file_stats = []
    dir=[ lfc_direntry ]
    # lfc_list structure
    stat  = lfc.lfc_filestatg()
    listp = lfc.lfc_list()
    # No idea what the arguments are but it`s faster
    """
    try: lfc.lfc_startsess('','')
    except NameError: pass
    while not lfc_direntry == None:
        # Only look at linked(validated) files
        if lfc_direntry.filemode==41471:
            name=lfc_direntry.d_name
            files[name]='UNKNOWN'
            partnrRE = re.compile("[_a-zA-Z0-9\.]*\._([0-9]*)[\w\.]*")
            match = partnrRE.match( name )
            if match != None:
                partnr=int(match.groups()[0])
                nfound+=1
            sys.stdout.write('.')
            sys.stdout.flush()
        lfc_direntry = lfc.lfc_readdirxc( lfc_dir )
    try: lfc.lfc_endsess()
    except NameError: pass
    print "\nFound %d files"%nfound
    print "Looking up guids"
    """
    nfound=0
    nmatch=0
    lfc.lfc_rewinddir(lfc_dir)
    try: lfc.lfc_startsess('','')
    except NameError: pass
    lfc_direntry = lfc.lfc_readdirxc( lfc_dir )
    while not lfc_direntry == None:
        if lfc_direntry.filemode==41471 or lfc_direntry.filemode==33204:
            path = os.sep.join([vuid,lfc_direntry.d_name])
            # pattern matching
	    matchPattern = False
            for tmpPat in lfnPat:
                pattern = tmpPat.replace('*','.*')
		if pattern == lfc_direntry.d_name:
#                if (pattern == tmpPat and pattern != lfc_direntry.d_name) or \
#                   (pattern != tmpPat and re.search(pattern,lfc_direntry.d_name) == None):
                    matchPattern = True
                    break
            sys.stdout.write('.')
            sys.stdout.flush()
            if not matchPattern:
                lfc_direntry = lfc.lfc_readdirxc( lfc_dir )
                dir += [ lfc_direntry ]
                continue
            res = lfc.lfc_statg(lfc_direntry.d_name,"",stat)
            if res == 0:
                guid = stat.guid
            else:
                err_num = lfc.cvar.serrno
                err_string = lfc.sstrerror(err_num)
                print "\nThere was an error while looking for " + path + ": Error " +\
                      str(err_num) + " (" + err_string + ")"
                #sys.exit(1)
                lfc_direntry = lfc.lfc_readdirxc( lfc_dir )
                dir += [ lfc_direntry ]
                continue
            md5sum = None
            try: md5sum = lfc_direntry.MD
            except: pass
            file_stats += [ (lfc_direntry.d_name, lfc_direntry.filesize,
                             lfc_direntry.ctime, md5sum,guid) ]
            # GUID
            files[lfc_direntry.d_name]=guid
            # size
            fsizeMap[lfc_direntry.d_name] = int(stat.filesize)
            # get replica    
            res = lfc.lfc_listreplica('',guid,lfc.CNS_LIST_BEGIN,listp)
            if res != None:
                # get SURL
		sfn = res.sfn
		# replace sfn:// with gsiftp://
                sfn = re.sub('^sfn://','gsiftp://',sfn)
                pfns[lfc_direntry.d_name]=sfn
            else:
                print "Repica not found for %s" % lfc_direntry.d_name
                sys.exit(1)
            # release buffer    
            lfc.lfc_listreplica('',guid,lfc.CNS_LIST_END,listp)
            nmatch+=1            
        lfc_direntry = lfc.lfc_readdirxc( lfc_dir )
        dir += [ lfc_direntry ]
    lfc.lfc_closedir( lfc_dir )
    try: lfc.lfc_endsess()
    except NameError: pass
    print "\nFound %d files"% nmatch
    return files,pfns,fsizeMap


# get PoolFileCatalog
def _getPoolFileCatalog(siteID,lfns):
    # instantiate curl
    curl = _Curl()
    # get PoolFileCatalog
    iLFN = 0
    outXML =''    
    strLFNs = ''
    # if no site service
    if not urlMap.has_key(siteID):
        return outXML
    url = urlMap[siteID] + 'lrc/PoolFileCatalog'
    for lfn in lfns:
        iLFN += 1
        # make argument
        strLFNs += '%s ' % lfn
        if iLFN % 40 == 0 or iLFN == len(lfns):
            # get PoolFileCatalog
            strLFNs = strLFNs.rstrip()
            data = {'lfns':strLFNs}
            # avoid too long argument
            strLFNs = ''
	    # execute
            status,out = curl.get(url,data)
            if status != 0 or out.startswith('Error'):
                continue
            if not out.startswith('<?xml'):
                continue
            # append
            outXML += out
    # remove redundant trailer and header
    th = \
"""
</POOLFILECATALOG><\?xml version="1.0" encoding="UTF-8" standalone="no" \?>
<!-- Edited By POOL -->
<!DOCTYPE POOLFILECATALOG SYSTEM "InMemory">
<POOLFILECATALOG>
"""
    outXML = re.sub(th,'',outXML)
    outXML = re.sub("""\s*<META name="fsize" type="string"/>""",'',outXML)
    outXML = re.sub("""\s*<META name="md5sum" type="string"/>""",'',outXML)
    outXML = re.sub("""\s*<META name="lastmodified" type="string"/>""",'',outXML)
    outXML = re.sub("""\s*<META name="archival" type="string"/>""",'',outXML)
    outXML = re.sub("""\s*<META name="permanent" type="string"/>""",'',outXML)
    # return XML
    return outXML


# extract PFN
def _getPFNs(xmlStr):
    pfnMap   = {}
    fsizeMap = {}
    # instantiate parser
    try:
        root  = xml.dom.minidom.parseString(xmlStr)
        files = root.getElementsByTagName('File')
        for file in files:
            # get PFN and LFN nodes
            physical = file.getElementsByTagName('physical')[0]
            pfnNode  = physical.getElementsByTagName('pfn')[0]
            logical  = file.getElementsByTagName('logical')[0]
            lfnNode  = logical.getElementsByTagName('lfn')[0]
            # convert UTF8 to Raw
            pfn = str(pfnNode.getAttribute('name'))
            lfn = str(lfnNode.getAttribute('name'))
            # append
            pfnMap[lfn] = pfn
            # get metadata
            fsizeMap[lfn] = 0
            for meta in file.getElementsByTagName('metadata'):
                # get fsize
                name = str(meta.getAttribute('att_name'))
                if name == 'fsize':
                    try:
                        fsizeMap[lfn] = int(meta.getAttribute('att_value'))
                    except:
                        # size is incorrect
                        print "WARNING : size of %s is incorrect in LRC" % lfn
                        fsizeMap[lfn] = -1
    except:
        pass
    return pfnMap,fsizeMap


# get PFN from LFC
def _getPFNsLFC(siteID,guidMap,remoteFlag=True):
    pfnMap   = {}
    fsizeMap = {}
    # check grid-proxy
    status,output = commands.getstatusoutput('grid-proxy-info -e')
    if status != 0:
        print "ERROR : No valid grid-proxy. Do 'grid-proxy-init'"
        sys.exit(1)
    # URL is not given
    if siteID == '':
        return pfnMap
    # set LFC HOST
    os.environ['LFC_HOST'] = lfcMap[siteID]
    # lfc_list structure
    stat  = lfc.lfc_filestatg()
    listp = lfc.lfc_list()
    # start LFC session
    try: lfc.lfc_startsess('','')
    except NameError: pass
    if globalVerbose:
        print "get SURLs from %s" % lfcMap[siteID]
    # loop over all GUIDs
    for lfn,guid in guidMap.iteritems():
        if globalVerbose:
            sys.stdout.write('.')
            sys.stdout.flush()
        # get replica    
        res = lfc.lfc_listreplica('',guid,lfc.CNS_LIST_BEGIN,listp)
        if res != None:
            # get SURL
            sfn = res.sfn
            # replace sfn:// with gsiftp://
            sfn = re.sub('^sfn://','gsiftp://',sfn)
            # append
            pfnMap[lfn] = sfn
            # get metadata
            res = lfc.lfc_statg("",guid,stat)
            fsizeMap[lfn] = int(stat.filesize)
        # release buffer    
        lfc.lfc_listreplica('',guid,lfc.CNS_LIST_END,listp)
    # end session
    try: lfc.lfc_endsess()
    except NameError: pass
    if globalVerbose:
        print
    return pfnMap,fsizeMap


# select appropriate location
def _getLocation(vuid,localFlag=False):
    # instantiate curl
    curl = _Curl()
    # get location
    url = baseURLDQ2 + 'ws_location/dataset'
    data = {'vuids':[vuid],'dsns':[]}
    status,out = curl.get(url,data)
    if status != 0:
        print status,out
        print "ERROR : could not query location for %s" % vuid
        sys.exit(EC_Location)
    # look for site
    sites = []
    if out != '\x00' and out != {}:
        tmpMap = out[out.keys()[0]]
        for tmpSite in (tmpMap[0] + tmpMap[1]):
            # change CERNCAF to CERN
            tmpSite = re.sub('^CERNCAF$','CERN',tmpSite)            
            # change TIER0 to CERN
            tmpSite = re.sub('^TIER0','CERN',tmpSite)
            # remove TAPE
            tmpSite = re.sub('TAPE$','',tmpSite)
            # remove DISK
            tmpSite = re.sub('DISK$','',tmpSite)
            # remove PANDA
            tmpSite = re.sub('PANDA$','',tmpSite)
            # append
            if (not tmpSite in sites) and (tmpSite in lfcMap.keys() or
                                           tmpSite in urlMap.keys()):
                sites.append(tmpSite)
    # check localID
    if localFlag:
        return (DQ2LOCALSITEID in sites)
    # remove local ID
    if sites == [DQ2LOCALSITEID]:
        pass
    elif DQ2LOCALSITEID in sites:
        sites.remove(DQ2LOCALSITEID)
    # no site
    if len(sites)==0:
        print "ERROR : No replica location for the dataset. You may be able to copy the dataset"
	print "        using 'dq2_get -l', Or dataset registration may be incomplete."
        print "        Submit a report to https://savannah.cern.ch/projects/dq2-ddm-ops/"
        sys.exit(EC_Location)
    # only one site
    if len(sites) == 1:
        return sites[0]
    # many sites
    print "Some files are missing in the local storage"
    print "They are in the following sites;"
    for i in range(len(sites)):
        print "[%d] : %s" % (i+1,sites[i])
    # automatic
    if globalChoose:
        index = random.randint(1,len(sites))
        print "%s is chosen automatically" % sites[int(index)-1]
    else:
        # select site        
        index = raw_input('Which site to retrieve them from ? [1-%d] : ' % len(sites))
    return sites[int(index)-1]


# worker thread for copy
class _copyWorker (threading.Thread):
    # const
    _TimeOutToken = 'TimeOut'
    # constructor
    def __init__(self,threadPool,resultBuffer,timeout):
        threading.Thread.__init__(self)
        # thread pool
        self.threadPool = threadPool
        # result buffer
        self.resultBuffer = resultBuffer
        # new thread
        self.newThread = True
        # timeout
        self.timeout = timeout
    # set parameter
    def setParam(self,pfn,com):
        # PFN
        self.pfn = pfn
        # command
        self.com = com
    # run
    def run(self):
        # result
        res = (self.pfn,False)
        # number of retry
        nTry = 3
        for iTry in range(nTry):
            # copy
            thr = threading.Thread(target=self.copy)
            thr.start()
            thr.join(self.timeout)
            # timeout
            if self.result == _copyWorker._TimeOutToken:
                status,output = (255,'Timeout')
                # get process list
                out = commands.getoutput("ps axjfww | grep %s" % os.getpgrp())
                pid = -1
                findParent = False
                for line in out.split('\n'):
                    # look for original command process
                    if re.search(self.com,line) != None:
                        items = line.split()
                        # get PGID
                        pgid = items[2]
                        # proper PGID
                        if pgid != str(os.getpgrp()):
                            continue
			# set PPID and PID
			ppid = items[0]	
			pid  = items[1]	
			# set flag
                        findParent = True
                        continue
		    # look for real execution process	
                    if findParent:
			# get PPID
                        items = line.split()                        
			myPPID = items[0]
                        # check if this process belongs to the original process 
			if myPPID != pid:
                            # kill
                            break
                        else:
                            # set PPID and PID
                            ppid = items[0]	
                            pid  = items[1]
                # kill
                if pid > 0:
                    commands.getoutput("kill -9 %s" % pid)
                    print "kill %s" % pid
            else:
                status,output = self.result
            # succeeded
            if status == 0:
                res = (self.pfn,True)
                break
            # failed
            print output
            print "could not finish '%s' with %s" % (self.com,status)
            if iTry+1 == nTry:
                break
            print "retry %s" % iTry
            # sleep
            time.sleep(30)
        # set flag
        self.newThread = False
        # put result
        self.resultBuffer.put(res)
        # release self
        self.threadPool.put(self)
    # copy file
    def copy(self):
        # set default result
        self.result = _copyWorker._TimeOutToken
        # execute
        self.result = commands.getstatusoutput(self.com)        
        

# parallel copy
def _parallelCopy(nPara,copyList,timeout):
    # instantiate thread pool
    threadPool = Queue.Queue(nPara)
    # instantiate result buffer
    resultBuffer = Queue.Queue()
    # instantiate workers
    for i in range(nPara):
        thr = _copyWorker(threadPool,resultBuffer,timeout)
        # put 
        threadPool.put(thr)
    # start Handler for SIGINT
    _SIGINTHandler.start() 
    # loop over all files
    for pfn in copyList.keys():
        # get thread
        thr = threadPool.get()
        # check if it is new thread
        if not thr.newThread:
            # instantiate new workers
            thr = _copyWorker(threadPool,resultBuffer,timeout)
        # command
        com = copyList[pfn]
        if globalVerbose:
            print com
        # set parameter
        thr.setParam(pfn,com)
        # start
        thr.start()
    # wait until all thread finish
    for i in range(nPara):
        # get thread
        thr = threadPool.get()
        # check if it is alive
        if thr.isAlive():
            # wait
            thr.join()
    # get result
    res = {}
    for i in copyList:
        item = resultBuffer.get()
        res[item[0]] = item[1]
    # return
    return res
    

# copy files and return a map to new PFNs
def _copyFiles(pfnMap,fsizeMap,remote_pfnMap,remote_fsizeMap,protocol,destination,srm_host,
               gsiftp_host,use_srm,parallel,se_root,timeout,local_prefix,copy_command):
    newPFNmap={}
    # define command for copy
    if protocol=='dcap':
        prefix       = 'dcap'
        lsCommand    = 'ls'
        copyCommand  = 'dccp'
        mkdirCommand = 'mkdir -p'
    elif protocol=='rfio':
        prefix       = 'castor'
        lsCommand    = 'nsls'
        copyCommand  = 'rfcp'
        mkdirCommand = 'nsmkdir -p -m 775'        
    elif protocol=='unix':
        prefix       = ''
        lsCommand    = 'ls'
        copyCommand  = 'cp'
        mkdirCommand = 'mkdir -p -m 775'        
    else:
        print "ERROR : protocol %s is not supported" % protocol
        sys.exit(EC_Copy)
    # overwrite ls and mkdir if dest is not in SE
    if (not destination.startswith(se_root)) or se_root=='':
        prefix       = ''
        lsCommand    = 'ls'
        mkdirCommand = 'mkdir -p -m 775'
    # check destination
    status,output = commands.getstatusoutput('%s %s' % (lsCommand,destination))
    if status != 0:
        # make dir
        status,output = commands.getstatusoutput('%s %s' % (mkdirCommand,destination))
        if status != 0:
            print output
            print "ERROR : could not make %s" % destination
            sys.exit(EC_Copy)        
    # copy
    for lfn,pfn in pfnMap.iteritems():
        # remove protocol::/host
        newpfn = re.sub('^[^:]+://[^/]+','',pfn)
        # remove protocol:
        newpfn = re.sub('^[^:]+:/','/',newpfn)
        # remove /srm/managerv1?SFN=
        newpfn = re.sub('/srm/managerv1\?SFN=','',newpfn)
        # remove redundant /
        newpfn = re.sub('^//','/',newpfn)
        # basename
        basename = lfn
        # execute
        if not globalNocopy:
            skipCopy = False
            # check if the files is already there
            status,output = commands.getstatusoutput('%s -l %s/%s' % (lsCommand,destination,basename))
            if status == 0:
                # size check
                fsize = int(output.split()[4])
                if fsize == fsizeMap[lfn]:
                    skipCopy = True
                    if globalVerbose:
                        print '%s already exists in %s : skipped' % (basename,destination)
            # copy
            if not skipCopy:
                com = '%s %s%s %s/%s' % (copyCommand,local_prefix,newpfn,destination,basename)
                if globalVerbose:
                    print com
                status,output = commands.getstatusoutput(com)
                if status != 0:
                    print output
                    print "ERROR : could not finish '%s' with %s" % (com,status)
                    sys.exit(EC_Copy)
            # new PFN
            if destination == '.':
                newPFN = basename
            else:
                # append protocol
                newPFN = "%s:%s/%s" % (prefix,destination,basename)
        else:
            # protocol + origianl PFN
            newPFN = '%s:%s' % (prefix,newpfn)
        # append
        newPFNmap[pfn] = newPFN
    # remote copy
    if not globalNocopy:
        copyList   = {}
        copiedFile = {}
        for lfn,pfn in remote_pfnMap.iteritems():
            # patches for SRM
            # 1) put port number
            newpfn = re.sub('(?P<srm>^srm://[^:/]+)/','\g<srm>:8443/',pfn)
            # 2) remove /srm/managerv1?SFN=
            newpfn = re.sub('/srm/managerv1\?SFN=','',newpfn)
            # basename
            basename = lfn
            # check if the files is already there
            status,output = commands.getstatusoutput('%s -l %s/%s' % (lsCommand,destination,basename))
            if status == 0:
                # size check
                fsize = int(output.split()[4])
                if fsize == remote_fsizeMap[lfn]:
                    copiedFile[pfn] = True
                    if globalVerbose:
                        print '%s already exists in %s : skipped' % (basename,destination)
                    continue
            # two party transfer
            if (not destination.startswith(se_root)) or se_root=='' or (srm_host=='' and gsiftp_host==''):
                # SRM or GSIFTP not supported 
                if newpfn.startswith('srm:/') or (newpfn.startswith('gsiftp:/') and use_srm):
                    remoteCopyCommand = 'srmcp -retry_num=1 -streams_num=10'
                    if os.environ.has_key('X509_CERT_DIR'):
                        remoteCopyCommand += ' -x509_user_trusted_certificates=%s' % os.environ['X509_CERT_DIR']
                    # patches for SRM, put :2811/
                    newpfn = re.sub('(?P<gsiftp>^gsiftp://[^:/]+)/','\g<gsiftp>:2811//',newpfn)
                elif newpfn.startswith('gsiftp:/'):
                    remoteCopyCommand = 'globus-url-copy'
                else:
                    print "ERROR : copy protocol is not supported for %s" % newpfn
                    sys.exit(EC_Copy)
                # destination filename
                if destination.startswith('/'):
                    dest_filename = 'file:///%s/%s' % (destination,basename)
                else:
                    dest_filename = 'file:///%s/%s/%s' % (os.getcwd(),destination,basename)
		# user defined copy command
                if copy_command != '':
                    remoteCopyCommand = copy_command
                    # patch for BNL
                    newpfn = re.sub('srm://dcsrm.usatlas.bnl.gov:8443/',
                                    'gsiftp://dcgftp.usatlas.bnl.gov:2811/',newpfn)
            # third party transfer
            else:
                # SRM or GSIFTP not supported 
                if newpfn.startswith('srm:/') or (newpfn.startswith('gsiftp:/') and use_srm):
                    if re.search('^[^:]+://[^/]+/*/dpm/',newpfn) != None:
                        remoteCopyCommand = 'srmcp -pushmode=false -retry_num=1 -streams_num=10'
                    else:
                        remoteCopyCommand = 'srmcp -pushmode=true -retry_num=1 -streams_num=10'
                    if os.environ.has_key('X509_CERT_DIR'):
                        remoteCopyCommand += ' -x509_user_trusted_certificates=%s' % os.environ['X509_CERT_DIR']
                    # patches for SRM, put :2811/
                    newpfn = re.sub('(?P<gsiftp>^gsiftp://[^:/]+)/','\g<gsiftp>:2811//',newpfn)
                    # destination filename
                    if srm_host != '':
                        dest_filename = '%s%s/%s' % (srm_host,destination,basename)
                    elif gsiftp_host != '':
                        dest_filename = '%s%s/%s' % (gsiftp_host,destination,basename)
                    else:
                        print "ERROR : DQ2_SRM_HOST or DQ2_GSIFTP_HOST should be defined"
                        sys.exit(EC_Copy)
                # GSIFTP
                elif newpfn.startswith('gsiftp:/'):
                    remoteCopyCommand = 'globus-url-copy'
                    # destination filename
                    if gsiftp_host != '':
                        dest_filename = '%s%s/%s' % (gsiftp_host,destination,basename)
                    else:
                        print "ERROR : DQ2_GSIFTP_HOST is not defined"
                        sys.exit(EC_Copy)
                else:
                    print "ERROR : copy protocol is not supported for %s" % newpfn
                    sys.exit(EC_Copy)
            # copy from remote
            com = '%s %s %s' % (remoteCopyCommand,newpfn,dest_filename)
            # append
            copyList[pfn] = com
        # copy
        copyResult = _parallelCopy(parallel,copyList,timeout)
	# join copyResult and copiedFile 
	for pfn in copiedFile:
            copyResult[pfn] = copiedFile[pfn]
        # new PFN
        for pfn in copyResult:
            # failed
            if not copyResult[pfn]:
                continue
            # basename
            basename = pfn.split('/')[-1]
            # new name
            if not destination.startswith(se_root):
                if destination.startswith('/'):
                    newPFN = '%s/%s' % (destination,basename)
                else:
                    newPFN = '%s/%s/%s' % (os.getcwd(),destination,basename)        
            else:
                # append protocol
                newPFN = "%s:%s/%s" % (prefix,destination,basename)
            # append
            newPFNmap[pfn] = newPFN
    # return
    return newPFNmap


####################################################################
# SIGINT handler : Python blocks SIGINT when running threads. However,
#                  forked processes launched by commands ('sleep' in
#                  this case) can receive the signal. A CTL-C mechanism
#                  is implemented using this feature, 
class SIGINTHandler (threading.Thread):
    def __init__(self,switch):
        threading.Thread.__init__(self)
        self.switch = switch
        
    def run(self):
        # start a sleep process to receive CTL-C
        status,out = commands.getstatusoutput('sleep 168h')
        # when CTL-C, status != 0
        if status != 0:
            # terminate whole process
            if self.switch.getFlag():
                commands.getoutput('kill -- -%s' % os.getpgrp())


class SIGINTSwitch:
    def __init__(self):
        self.flag = True
    def getFlag(self):
        return self.flag
    def disable(self):
        self.flag = False

    
# singleton
_SIGINTSwitch = SIGINTSwitch()
_SIGINTHandler = SIGINTHandler(_SIGINTSwitch)
del SIGINTSwitch
del SIGINTHandler


####################################################################
# main
def main():
    import sys
    import getopt
    returnvalue = 0

    # option class
    class _options:
        def __init__(self):
            pass
    options = _options()
    del _options
    # set default values
    options.verbose     = False
    options.nocopy      = False
    options.remote      = False
    options.choose      = False
    options.lcg         = False
    options.source      = ''
    options.destination = '.'
    options.parallel    = 3    
    options.timeout     = 1800    
    # get command-line parameters
    try:
        opts, args = getopt.getopt(sys.argv[1:],"hvnrcld:p:t:s:",
                                   ["help","verbose","nocopy","remote","choose",
                                    "lcg","destination=","parallel=","timeout=",
                                    "source="])
    except:
        _usage()
        print "ERROR : Invalid options"
        sys.exit(EC_Main)    
    # set options
    for o, a in opts:
        if o in ("-h","--help"):
            _usage()
            sys.exit()
        if o in ("-v","--verbose"):
            options.verbose = True
        if o in ("-l","--lcg"):
            options.lcg = True
        if o in ("-n","--nocopy"):
            options.nocopy = True
        if o in ("-r","--remote"):
            options.remote = True
        if o in ("-c","--choose"):
            options.choose = True
        if o in ("-s","--source"):
            options.source = a
        if o in ("-d","--destination"):
            options.destination = a
        if o in ("-p","--parallel"):
            options.parallel = int(a)
        if o in ("-t","--timeout"):
            options.timeout = int(a)
    # global flags
    global globalVerbose
    globalVerbose = options.verbose
    global globalNocopy
    globalNocopy = options.nocopy
    global globalChoose
    globalChoose = options.choose
    # datasetname
    if len(args) == 0:
        print "no datasetname"
        sys.exit(EC_Main)    
    datasetname = args[0]
    # LFNs
    args.pop(0)
    lfns = args
    if options.lcg:
        # removed attempt numbers
        if len(lfns) != 0:
            tmplfns = []
            for lfn in lfns:
                tmplfns.append(re.sub('\.\d+$','',lfn))
            lfns = tmplfns    
        print lfns    
        # get file list from prodSys LFC
        tmpFileMap,lcgPfnMap,lcgFsizeMap = _queryFilesInDataset_lcg(datasetname,lfns)
    else:
        # get VUID
        vuid = _getVUID(datasetname)
        # get file list
        tmpFileMap = _queryFilesInDataset(vuid)
    # check if LFNs are in the dataset
    fileMap = {}
    if len(lfns) != 0:
        for lfn in lfns:
            pat = lfn.replace('*','.*')
            if pat == lfn:
                # normal matching
                if not lfn in tmpFileMap.keys():
                    print "ERROR : %s is not in %s" % (lfn,datasetname)
                    sys.exit(EC_Main)
                fileMap[lfn] = tmpFileMap[lfn]
            else:
                # wild card matching
                for tmpLFN in tmpFileMap.keys():
                    if re.search(pat,tmpLFN) != None:
                        fileMap[tmpLFN] = tmpFileMap[tmpLFN]
    else:
        # use all files in the dataset
        fileMap = tmpFileMap
    # get PFN    
    pfnMap   = {}
    fsizeMap = {}        
    if not options.lcg:
        if _getLocation(vuid,True):
            # check LRC
            if DQ2LOCALSITEID in lfcMap.keys():
                # get PFN from LFC
                pfnMap,fsizeMap = _getPFNsLFC(DQ2LOCALSITEID,fileMap,False)
            else:
                # get PoolFileCatalog
                xmlStr = _getPoolFileCatalog(DQ2LOCALSITEID,fileMap)
                # get PFN
                pfnMap,fsizeMap = _getPFNs(xmlStr)
    # check if LFNs are in the PFN map
    missFiles = {}
    for lfn in fileMap.keys():
        if not lfn in pfnMap.keys():
            missFiles[lfn] = fileMap[lfn]
    # error message
    if len(missFiles) and not options.remote:
        print "The following files are not found at %s. " % DQ2LOCALSITEID
        print "Use '-r' if you want to copy them over the grid"
        for lfn in missFiles:
            print "  %s" % lfn
    # preparation for remote copy
    missPfnMap   = {}
    missFsizeMap = {}    
    if options.remote:
        if len(missFiles):
            # check grid-proxy
            status,output = commands.getstatusoutput('grid-proxy-info -e')
            if status != 0:
                print "ERROR : No valid grid-proxy. Do 'grid-proxy-init'"
                sys.exit(EC_Main)
            if options.lcg:
                for lfn in missFiles.keys():
                    missPfnMap[lfn]   = lcgPfnMap[lfn]
                    missFsizeMap[lfn] = lcgFsizeMap[lfn]                    
            else:
                # get remote site which holds the dataset
                if options.source != '':
                    remoteSite = options.source
                else:
                    remoteSite = _getLocation(vuid)
                if remoteSite != DQ2LOCALSITEID:
                    # check remote LRC
                    if remoteSite in lfcMap.keys():
                        # LFC
                        missPfnMap,missFsizeMap = _getPFNsLFC(remoteSite,missFiles)
                    else:
                        # get PoolFileCatalog
                        missXmlStr = _getPoolFileCatalog(remoteSite,missFiles)
                        # get PFN
                        missPfnMap,missFsizeMap = _getPFNs(missXmlStr)
	    # check if LFNs are in the PFN map
            flag_first = True
	    for missFile in missFiles:
                if missFile not in missPfnMap.keys():
                    if flag_first:
                        flag_first = False
                        print "WARNING : Replica at %s is incomplete" % remoteSite
                    print "%s is not found" % missFile
    # copy files
    copyResult = _copyFiles(pfnMap,fsizeMap,missPfnMap,missFsizeMap,
                            configLOCALPROTOCOL,options.destination,configSRMHOST,configGSIFTPHOST,
                            configUSESRM,options.parallel,configSTORAGEROOT,options.timeout,
                            configLOCALPREFIX,configCOPYCOMMAND)
    # result
    failedFiles = []
    for lfn in pfnMap.keys():
        pfn = pfnMap[lfn]
        if not pfn in copyResult.keys():
            failedFiles.append(lfn)
    for lfn in missPfnMap.keys():
        pfn = missPfnMap[lfn]
        if not pfn in copyResult.keys():
            failedFiles.append(lfn)
    print "Done"        
    print "Total:%s - Failed:%s" % (len(pfnMap)+len(missPfnMap),len(failedFiles))
    if len(failedFiles) != 0:
	returnvalue = EC_Main
        failedFiles.sort()
        for lfn in failedFiles:
            print "   %s" % lfn

    if (len(pfnMap)+len(missPfnMap)==0):
	returnvalue = EC_Main
    # disable SIGINT
    _SIGINTSwitch.disable()
    # look for SIGINTHandler and kill it
    out = commands.getoutput("ps axjfww | grep %s" % os.getpgrp())
    for line in out.split('\n'):
        # get PID and PGID
        items = line.split()
        pid  = items[1]
        pgid = items[2]
        # correct PGID
        if pgid != str(os.getpgrp()):
            continue
        # look for sleep process
        if re.search(' sleep ',line) != None:
            # kill
            commands.getoutput("kill -9 %s" % pid)
    # return
    return returnvalue


if __name__ == "__main__":
    returnvalue = main()
    sys.exit(returnvalue)
    
